{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cf2ffb8",
   "metadata": {},
   "source": [
    "### Import Necessary Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eec1e2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install spacy\n",
    "#!pip install TextBlob\n",
    "#!pip install tqdm\n",
    "#!python -m spacy download en_core_web_sm\n",
    "#!pip install yake\n",
    "#!pip install transformers torch\n",
    "#!python -m spacy download en_core_web_lg\n",
    "#!pip install bertopic --user\n",
    "#!pip install hdbscan\n",
    "#!pip install contextualized-topic-models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8edd06e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\ashly\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ashly\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ashly\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ashly\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\ashly\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ashly\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ashly\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\ashly\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package conll2000 to\n",
      "[nltk_data]     C:\\Users\\ashly\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package conll2000 is already up-to-date!\n",
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\ashly\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from textblob import download_corpora\n",
    "download_corpora.main()\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from tqdm.notebook import tqdm\n",
    "import yake\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "from sklearn.decomposition import NMF\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from tqdm.auto import tqdm\n",
    "import logging\n",
    "\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from contextualized_topic_models.models.ctm import CombinedTM\n",
    "from contextualized_topic_models.models.ctm import CTM\n",
    "from contextualized_topic_models.utils.data_preparation import TopicModelDataPreparation\n",
    "from contextualized_topic_models.datasets.dataset import CTMDataset\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050751da",
   "metadata": {},
   "source": [
    "### TedTalk Data Loading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddfdbef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (4005, 19)\n"
     ]
    }
   ],
   "source": [
    "# Function to load the data from the csv file to a dataframe and print the shape.\n",
    "def load_data_and_print_shape(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(f'Shape: {df.shape}')\n",
    "    return df\n",
    "\n",
    "tt_df = load_data_and_print_shape('ted_talks.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b537863",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['talk_id', 'title', 'speaker_1', 'all_speakers', 'occupations',\n",
       "       'about_speakers', 'views', 'recorded_date', 'published_date', 'event',\n",
       "       'native_lang', 'available_lang', 'comments', 'duration', 'topics',\n",
       "       'related_talks', 'url', 'description', 'transcript'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the columns present in the dataframe.\n",
    "tt_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "579cbaf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>talk_id</th>\n",
       "      <th>title</th>\n",
       "      <th>speaker_1</th>\n",
       "      <th>all_speakers</th>\n",
       "      <th>occupations</th>\n",
       "      <th>about_speakers</th>\n",
       "      <th>views</th>\n",
       "      <th>recorded_date</th>\n",
       "      <th>published_date</th>\n",
       "      <th>event</th>\n",
       "      <th>native_lang</th>\n",
       "      <th>available_lang</th>\n",
       "      <th>comments</th>\n",
       "      <th>duration</th>\n",
       "      <th>topics</th>\n",
       "      <th>related_talks</th>\n",
       "      <th>url</th>\n",
       "      <th>description</th>\n",
       "      <th>transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Averting the climate crisis</td>\n",
       "      <td>Al Gore</td>\n",
       "      <td>{0: 'Al Gore'}</td>\n",
       "      <td>{0: ['climate advocate']}</td>\n",
       "      <td>{0: 'Nobel Laureate Al Gore focused the world’...</td>\n",
       "      <td>3523392</td>\n",
       "      <td>2006-02-25</td>\n",
       "      <td>2006-06-27</td>\n",
       "      <td>TED2006</td>\n",
       "      <td>en</td>\n",
       "      <td>['ar', 'bg', 'cs', 'de', 'el', 'en', 'es', 'fa...</td>\n",
       "      <td>272.0</td>\n",
       "      <td>977</td>\n",
       "      <td>['alternative energy', 'cars', 'climate change...</td>\n",
       "      <td>{243: 'New thinking on the climate crisis', 54...</td>\n",
       "      <td>https://www.ted.com/talks/al_gore_averting_the...</td>\n",
       "      <td>With the same humor and humanity he exuded in ...</td>\n",
       "      <td>Thank you so much, Chris. And it's truly a gre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>92</td>\n",
       "      <td>The best stats you've ever seen</td>\n",
       "      <td>Hans Rosling</td>\n",
       "      <td>{0: 'Hans Rosling'}</td>\n",
       "      <td>{0: ['global health expert; data visionary']}</td>\n",
       "      <td>{0: 'In Hans Rosling’s hands, data sings. Glob...</td>\n",
       "      <td>14501685</td>\n",
       "      <td>2006-02-22</td>\n",
       "      <td>2006-06-27</td>\n",
       "      <td>TED2006</td>\n",
       "      <td>en</td>\n",
       "      <td>['ar', 'az', 'bg', 'bn', 'bs', 'cs', 'da', 'de...</td>\n",
       "      <td>628.0</td>\n",
       "      <td>1190</td>\n",
       "      <td>['Africa', 'Asia', 'Google', 'demo', 'economic...</td>\n",
       "      <td>{2056: \"Own your body's data\", 2296: 'A visual...</td>\n",
       "      <td>https://www.ted.com/talks/hans_rosling_the_bes...</td>\n",
       "      <td>You've never seen data presented like this. Wi...</td>\n",
       "      <td>About 10 years ago, I took on the task to teac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>Simplicity sells</td>\n",
       "      <td>David Pogue</td>\n",
       "      <td>{0: 'David Pogue'}</td>\n",
       "      <td>{0: ['technology columnist']}</td>\n",
       "      <td>{0: 'David Pogue is the personal technology co...</td>\n",
       "      <td>1920832</td>\n",
       "      <td>2006-02-24</td>\n",
       "      <td>2006-06-27</td>\n",
       "      <td>TED2006</td>\n",
       "      <td>en</td>\n",
       "      <td>['ar', 'bg', 'de', 'el', 'en', 'es', 'fa', 'fr...</td>\n",
       "      <td>124.0</td>\n",
       "      <td>1286</td>\n",
       "      <td>['computers', 'entertainment', 'interface desi...</td>\n",
       "      <td>{1725: '10 top time-saving tech tips', 2274: '...</td>\n",
       "      <td>https://www.ted.com/talks/david_pogue_simplici...</td>\n",
       "      <td>New York Times columnist David Pogue takes aim...</td>\n",
       "      <td>(Music: \"The Sound of Silence,\" Simon &amp; Garfun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>Greening the ghetto</td>\n",
       "      <td>Majora Carter</td>\n",
       "      <td>{0: 'Majora Carter'}</td>\n",
       "      <td>{0: ['activist for environmental justice']}</td>\n",
       "      <td>{0: 'Majora Carter redefined the field of envi...</td>\n",
       "      <td>2664069</td>\n",
       "      <td>2006-02-26</td>\n",
       "      <td>2006-06-27</td>\n",
       "      <td>TED2006</td>\n",
       "      <td>en</td>\n",
       "      <td>['ar', 'bg', 'bn', 'ca', 'cs', 'de', 'en', 'es...</td>\n",
       "      <td>219.0</td>\n",
       "      <td>1116</td>\n",
       "      <td>['MacArthur grant', 'activism', 'business', 'c...</td>\n",
       "      <td>{1041: '3 stories of local eco-entrepreneurshi...</td>\n",
       "      <td>https://www.ted.com/talks/majora_carter_greeni...</td>\n",
       "      <td>In an emotionally charged talk, MacArthur-winn...</td>\n",
       "      <td>If you're here today — and I'm very happy that...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>66</td>\n",
       "      <td>Do schools kill creativity?</td>\n",
       "      <td>Sir Ken Robinson</td>\n",
       "      <td>{0: 'Sir Ken Robinson'}</td>\n",
       "      <td>{0: ['author', 'educator']}</td>\n",
       "      <td>{0: \"Creativity expert Sir Ken Robinson challe...</td>\n",
       "      <td>65051954</td>\n",
       "      <td>2006-02-25</td>\n",
       "      <td>2006-06-27</td>\n",
       "      <td>TED2006</td>\n",
       "      <td>en</td>\n",
       "      <td>['af', 'ar', 'az', 'be', 'bg', 'bn', 'ca', 'cs...</td>\n",
       "      <td>4931.0</td>\n",
       "      <td>1164</td>\n",
       "      <td>['children', 'creativity', 'culture', 'dance',...</td>\n",
       "      <td>{865: 'Bring on the learning revolution!', 173...</td>\n",
       "      <td>https://www.ted.com/talks/sir_ken_robinson_do_...</td>\n",
       "      <td>Sir Ken Robinson makes an entertaining and pro...</td>\n",
       "      <td>Good morning. How are you? (Audience) Good. It...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   talk_id                            title         speaker_1  \\\n",
       "0        1      Averting the climate crisis           Al Gore   \n",
       "1       92  The best stats you've ever seen      Hans Rosling   \n",
       "2        7                 Simplicity sells       David Pogue   \n",
       "3       53              Greening the ghetto     Majora Carter   \n",
       "4       66      Do schools kill creativity?  Sir Ken Robinson   \n",
       "\n",
       "              all_speakers                                    occupations  \\\n",
       "0           {0: 'Al Gore'}                      {0: ['climate advocate']}   \n",
       "1      {0: 'Hans Rosling'}  {0: ['global health expert; data visionary']}   \n",
       "2       {0: 'David Pogue'}                  {0: ['technology columnist']}   \n",
       "3     {0: 'Majora Carter'}    {0: ['activist for environmental justice']}   \n",
       "4  {0: 'Sir Ken Robinson'}                    {0: ['author', 'educator']}   \n",
       "\n",
       "                                      about_speakers     views recorded_date  \\\n",
       "0  {0: 'Nobel Laureate Al Gore focused the world’...   3523392    2006-02-25   \n",
       "1  {0: 'In Hans Rosling’s hands, data sings. Glob...  14501685    2006-02-22   \n",
       "2  {0: 'David Pogue is the personal technology co...   1920832    2006-02-24   \n",
       "3  {0: 'Majora Carter redefined the field of envi...   2664069    2006-02-26   \n",
       "4  {0: \"Creativity expert Sir Ken Robinson challe...  65051954    2006-02-25   \n",
       "\n",
       "  published_date    event native_lang  \\\n",
       "0     2006-06-27  TED2006          en   \n",
       "1     2006-06-27  TED2006          en   \n",
       "2     2006-06-27  TED2006          en   \n",
       "3     2006-06-27  TED2006          en   \n",
       "4     2006-06-27  TED2006          en   \n",
       "\n",
       "                                      available_lang  comments  duration  \\\n",
       "0  ['ar', 'bg', 'cs', 'de', 'el', 'en', 'es', 'fa...     272.0       977   \n",
       "1  ['ar', 'az', 'bg', 'bn', 'bs', 'cs', 'da', 'de...     628.0      1190   \n",
       "2  ['ar', 'bg', 'de', 'el', 'en', 'es', 'fa', 'fr...     124.0      1286   \n",
       "3  ['ar', 'bg', 'bn', 'ca', 'cs', 'de', 'en', 'es...     219.0      1116   \n",
       "4  ['af', 'ar', 'az', 'be', 'bg', 'bn', 'ca', 'cs...    4931.0      1164   \n",
       "\n",
       "                                              topics  \\\n",
       "0  ['alternative energy', 'cars', 'climate change...   \n",
       "1  ['Africa', 'Asia', 'Google', 'demo', 'economic...   \n",
       "2  ['computers', 'entertainment', 'interface desi...   \n",
       "3  ['MacArthur grant', 'activism', 'business', 'c...   \n",
       "4  ['children', 'creativity', 'culture', 'dance',...   \n",
       "\n",
       "                                       related_talks  \\\n",
       "0  {243: 'New thinking on the climate crisis', 54...   \n",
       "1  {2056: \"Own your body's data\", 2296: 'A visual...   \n",
       "2  {1725: '10 top time-saving tech tips', 2274: '...   \n",
       "3  {1041: '3 stories of local eco-entrepreneurshi...   \n",
       "4  {865: 'Bring on the learning revolution!', 173...   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://www.ted.com/talks/al_gore_averting_the...   \n",
       "1  https://www.ted.com/talks/hans_rosling_the_bes...   \n",
       "2  https://www.ted.com/talks/david_pogue_simplici...   \n",
       "3  https://www.ted.com/talks/majora_carter_greeni...   \n",
       "4  https://www.ted.com/talks/sir_ken_robinson_do_...   \n",
       "\n",
       "                                         description  \\\n",
       "0  With the same humor and humanity he exuded in ...   \n",
       "1  You've never seen data presented like this. Wi...   \n",
       "2  New York Times columnist David Pogue takes aim...   \n",
       "3  In an emotionally charged talk, MacArthur-winn...   \n",
       "4  Sir Ken Robinson makes an entertaining and pro...   \n",
       "\n",
       "                                          transcript  \n",
       "0  Thank you so much, Chris. And it's truly a gre...  \n",
       "1  About 10 years ago, I took on the task to teac...  \n",
       "2  (Music: \"The Sound of Silence,\" Simon & Garfun...  \n",
       "3  If you're here today — and I'm very happy that...  \n",
       "4  Good morning. How are you? (Audience) Good. It...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the first five rows  of the dataframe.\n",
    "tt_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea877b1",
   "metadata": {},
   "source": [
    "### User Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2da25f8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (1656, 5)\n"
     ]
    }
   ],
   "source": [
    "# Function to load the data from the csv file to a dataframe and print the shape.\n",
    "def load_user_data(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(f'Shape: {df.shape}')\n",
    "    return df\n",
    "\n",
    "user_df = load_user_data('user_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc8bf28b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['User ID', 'Talk ID', 'Rating', 'View Count', 'Comments'], dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the columns present in the dataframe.\n",
    "user_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4a1125f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User ID</th>\n",
       "      <th>Talk ID</th>\n",
       "      <th>Rating</th>\n",
       "      <th>View Count</th>\n",
       "      <th>Comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1840</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>Moved to tears by the emotional rollercoaster ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2282</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>\"A symphony of thought-provoking ideas. Superb!\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2370</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>\"Somewhat confusing and hard to follow, to be ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>8227</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>\"A true gem! This talk was both informative an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>1556</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>\"The talk's transitions between topics were a ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   User ID  Talk ID  Rating  View Count  \\\n",
       "0        1     1840       4           5   \n",
       "1        1     2282       5           8   \n",
       "2        1     2370       2           2   \n",
       "3        2     8227       4           6   \n",
       "4        2     1556       2           1   \n",
       "\n",
       "                                            Comments  \n",
       "0  Moved to tears by the emotional rollercoaster ...  \n",
       "1   \"A symphony of thought-provoking ideas. Superb!\"  \n",
       "2  \"Somewhat confusing and hard to follow, to be ...  \n",
       "3  \"A true gem! This talk was both informative an...  \n",
       "4  \"The talk's transitions between topics were a ...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the first five rows of the dataframe.\n",
    "user_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644a5e21",
   "metadata": {},
   "source": [
    "### TedTalk Data Pre-processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6ee5eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract only the columns required for analysis.\n",
    "def extract_required_columns(df):\n",
    "    required_columns = ['talk_id', 'title', 'speaker_1', 'occupations', 'event', \n",
    "                        'published_date', 'duration', 'topics', 'description', \n",
    "                        'views', 'comments', 'transcript']\n",
    "    return df[required_columns]\n",
    "tt_df_sub = extract_required_columns(tt_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf4e6a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null values per column:\n",
      "talk_id             0\n",
      "title               0\n",
      "speaker_1           0\n",
      "occupations       522\n",
      "event               0\n",
      "published_date      0\n",
      "duration            0\n",
      "topics              0\n",
      "description         0\n",
      "views               0\n",
      "comments          655\n",
      "transcript          0\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "Number of duplicate rows: 0\n",
      "\n",
      "No duplicated rows found.\n"
     ]
    }
   ],
   "source": [
    "# Function to check for null values and duplicates in the dataframe.\n",
    "def check_data_quality(df):\n",
    "    # Check for null values in the DataFrame.\n",
    "    null_values = df.isnull().sum()\n",
    "    print(\"Null values per column:\")\n",
    "    print(null_values)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # Check for duplicate values.\n",
    "    duplicate_rows = df[df.duplicated()]\n",
    "\n",
    "    # To see the number of duplicate rows.\n",
    "    num_duplicate_rows = duplicate_rows.shape[0]\n",
    "    print(f\"Number of duplicate rows: {num_duplicate_rows}\\n\")\n",
    "\n",
    "    # View the duplicated rows if any.\n",
    "    if num_duplicate_rows > 0:\n",
    "        print(\"Duplicated rows:\")\n",
    "        print(duplicate_rows)\n",
    "    else:\n",
    "        print(\"No duplicated rows found.\")\n",
    "\n",
    "# Use the function:\n",
    "check_data_quality(tt_df_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e59d3d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "talk_id             int64\n",
      "title              object\n",
      "speaker_1          object\n",
      "occupations        object\n",
      "event              object\n",
      "published_date     object\n",
      "duration            int64\n",
      "topics             object\n",
      "description        object\n",
      "views               int64\n",
      "comments          float64\n",
      "transcript         object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Check for data types of columns.\n",
    "print(tt_df_sub.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047a1644",
   "metadata": {},
   "source": [
    "#### Basic Pre-Processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5cbfd229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for basic data pre-processing.\n",
    "\n",
    "def preprocess_dataframe(df):\n",
    "    def fill_missing_values(df):\n",
    "        \"\"\"Replace null values with default values.\"\"\"\n",
    "        df['occupations'].fillna('Unknown', inplace=True)\n",
    "        df['comments'].fillna(0, inplace=True)\n",
    "        return df\n",
    "    \n",
    "    def clean_strings(df, columns, chars_to_remove):\n",
    "        \"\"\"Remove specified characters from given columns.\"\"\"\n",
    "        remove_dict = {ord(char): None for char in chars_to_remove}\n",
    "        for col in columns:\n",
    "            df[col] = df[col].str.translate(remove_dict)\n",
    "        return df\n",
    "\n",
    "    def remove_numbers_from_column(df, column):\n",
    "        \"\"\"Remove numbers from specified column.\"\"\"\n",
    "        df[column] = df[column].str.replace(r'\\d+', '', regex=True)\n",
    "        return df\n",
    "\n",
    "    def convert_duration_to_minutes(df, column):\n",
    "        \"\"\"Convert duration from seconds to minutes.\"\"\"\n",
    "        df[column] = (df[column] / 60).round(2)\n",
    "        return df\n",
    "\n",
    "    def convert_to_datetime(df, column):\n",
    "        \"\"\"Convert specified column to datetime.\"\"\"\n",
    "        df[column] = pd.to_datetime(df[column])\n",
    "        return df\n",
    "\n",
    "    def extract_year(df, source_column, new_column):\n",
    "        \"\"\"Extract year from source column and create new column.\"\"\"\n",
    "        df[new_column] = pd.to_datetime(df[source_column]).dt.year\n",
    "        return df\n",
    "\n",
    "    def convert_column_to_int64(df, column):\n",
    "        \"\"\"Convert specified column to int64.\"\"\"\n",
    "        df[column] = df[column].astype('int64')\n",
    "        return df\n",
    "\n",
    "    def rearrange_columns(df, columns_order):\n",
    "        \"\"\"Rearrange dataframe columns based on given order.\"\"\"\n",
    "        return df[columns_order]\n",
    "\n",
    "    def rename_columns(df, mapping):\n",
    "        \"\"\"Rename dataframe columns based on given mapping.\"\"\"\n",
    "        return df.rename(columns=mapping)\n",
    "\n",
    "    # Make a copy to avoid SettingWithCopyWarning.\n",
    "    df = df.copy()\n",
    "\n",
    "    df = fill_missing_values(df)\n",
    "    df = clean_strings(df, ['occupations', 'topics'], ['{', '#', ':', '[', '\\'', ';', ',', ']', '}'])\n",
    "    df = remove_numbers_from_column(df, 'occupations')\n",
    "    df = convert_duration_to_minutes(df, 'duration')\n",
    "    df = convert_to_datetime(df, 'published_date')\n",
    "    df = extract_year(df, 'published_date', 'year')\n",
    "    df = convert_column_to_int64(df, 'comments')\n",
    "    df = rearrange_columns(df, [df.columns[-1]] + df.columns[:-1].tolist())\n",
    "    df = rename_columns(df, {'speaker_1': 'speaker'})\n",
    "\n",
    "    return df\n",
    "\n",
    "# Main preprocessing function.\n",
    "tt_df_sub = preprocess_dataframe(tt_df_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c0dabb5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "year              0\n",
      "talk_id           0\n",
      "title             0\n",
      "speaker           0\n",
      "occupations       0\n",
      "event             0\n",
      "published_date    0\n",
      "duration          0\n",
      "topics            0\n",
      "description       0\n",
      "views             0\n",
      "comments          0\n",
      "transcript        0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Verify if the null values are handled.\n",
    "null_values = tt_df_sub.isnull().sum()\n",
    "print(null_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "38ddf643",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>talk_id</th>\n",
       "      <th>title</th>\n",
       "      <th>speaker</th>\n",
       "      <th>occupations</th>\n",
       "      <th>event</th>\n",
       "      <th>published_date</th>\n",
       "      <th>duration</th>\n",
       "      <th>topics</th>\n",
       "      <th>description</th>\n",
       "      <th>views</th>\n",
       "      <th>comments</th>\n",
       "      <th>transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2006</td>\n",
       "      <td>1</td>\n",
       "      <td>Averting the climate crisis</td>\n",
       "      <td>Al Gore</td>\n",
       "      <td>climate advocate</td>\n",
       "      <td>TED2006</td>\n",
       "      <td>2006-06-27</td>\n",
       "      <td>16.28</td>\n",
       "      <td>alternative energy cars climate change culture...</td>\n",
       "      <td>With the same humor and humanity he exuded in ...</td>\n",
       "      <td>3523392</td>\n",
       "      <td>272</td>\n",
       "      <td>Thank you so much, Chris. And it's truly a gre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2006</td>\n",
       "      <td>92</td>\n",
       "      <td>The best stats you've ever seen</td>\n",
       "      <td>Hans Rosling</td>\n",
       "      <td>global health expert data visionary</td>\n",
       "      <td>TED2006</td>\n",
       "      <td>2006-06-27</td>\n",
       "      <td>19.83</td>\n",
       "      <td>Africa Asia Google demo economics global issue...</td>\n",
       "      <td>You've never seen data presented like this. Wi...</td>\n",
       "      <td>14501685</td>\n",
       "      <td>628</td>\n",
       "      <td>About 10 years ago, I took on the task to teac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2006</td>\n",
       "      <td>7</td>\n",
       "      <td>Simplicity sells</td>\n",
       "      <td>David Pogue</td>\n",
       "      <td>technology columnist</td>\n",
       "      <td>TED2006</td>\n",
       "      <td>2006-06-27</td>\n",
       "      <td>21.43</td>\n",
       "      <td>computers entertainment interface design media...</td>\n",
       "      <td>New York Times columnist David Pogue takes aim...</td>\n",
       "      <td>1920832</td>\n",
       "      <td>124</td>\n",
       "      <td>(Music: \"The Sound of Silence,\" Simon &amp; Garfun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2006</td>\n",
       "      <td>53</td>\n",
       "      <td>Greening the ghetto</td>\n",
       "      <td>Majora Carter</td>\n",
       "      <td>activist for environmental justice</td>\n",
       "      <td>TED2006</td>\n",
       "      <td>2006-06-27</td>\n",
       "      <td>18.60</td>\n",
       "      <td>MacArthur grant activism business cities envir...</td>\n",
       "      <td>In an emotionally charged talk, MacArthur-winn...</td>\n",
       "      <td>2664069</td>\n",
       "      <td>219</td>\n",
       "      <td>If you're here today — and I'm very happy that...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2006</td>\n",
       "      <td>66</td>\n",
       "      <td>Do schools kill creativity?</td>\n",
       "      <td>Sir Ken Robinson</td>\n",
       "      <td>author educator</td>\n",
       "      <td>TED2006</td>\n",
       "      <td>2006-06-27</td>\n",
       "      <td>19.40</td>\n",
       "      <td>children creativity culture dance education pa...</td>\n",
       "      <td>Sir Ken Robinson makes an entertaining and pro...</td>\n",
       "      <td>65051954</td>\n",
       "      <td>4931</td>\n",
       "      <td>Good morning. How are you? (Audience) Good. It...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year  talk_id                            title           speaker  \\\n",
       "0  2006        1      Averting the climate crisis           Al Gore   \n",
       "1  2006       92  The best stats you've ever seen      Hans Rosling   \n",
       "2  2006        7                 Simplicity sells       David Pogue   \n",
       "3  2006       53              Greening the ghetto     Majora Carter   \n",
       "4  2006       66      Do schools kill creativity?  Sir Ken Robinson   \n",
       "\n",
       "                            occupations    event published_date  duration  \\\n",
       "0                      climate advocate  TED2006     2006-06-27     16.28   \n",
       "1   global health expert data visionary  TED2006     2006-06-27     19.83   \n",
       "2                  technology columnist  TED2006     2006-06-27     21.43   \n",
       "3    activist for environmental justice  TED2006     2006-06-27     18.60   \n",
       "4                       author educator  TED2006     2006-06-27     19.40   \n",
       "\n",
       "                                              topics  \\\n",
       "0  alternative energy cars climate change culture...   \n",
       "1  Africa Asia Google demo economics global issue...   \n",
       "2  computers entertainment interface design media...   \n",
       "3  MacArthur grant activism business cities envir...   \n",
       "4  children creativity culture dance education pa...   \n",
       "\n",
       "                                         description     views  comments  \\\n",
       "0  With the same humor and humanity he exuded in ...   3523392       272   \n",
       "1  You've never seen data presented like this. Wi...  14501685       628   \n",
       "2  New York Times columnist David Pogue takes aim...   1920832       124   \n",
       "3  In an emotionally charged talk, MacArthur-winn...   2664069       219   \n",
       "4  Sir Ken Robinson makes an entertaining and pro...  65051954      4931   \n",
       "\n",
       "                                          transcript  \n",
       "0  Thank you so much, Chris. And it's truly a gre...  \n",
       "1  About 10 years ago, I took on the task to teac...  \n",
       "2  (Music: \"The Sound of Silence,\" Simon & Garfun...  \n",
       "3  If you're here today — and I'm very happy that...  \n",
       "4  Good morning. How are you? (Audience) Good. It...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display first five rows of the cleaned dataframe.\n",
    "tt_df_sub.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361c8f84",
   "metadata": {},
   "source": [
    "#### Advanced Text Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "24a0d5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# More Pre-processing.\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Convert text to lowercase.\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove text in square brackets & parenthesis.\n",
    "    text = re.sub(r'\\[.*?\\]', '', text)\n",
    "    text = re.sub(r'\\(.*?\\)', '', text)\n",
    "\n",
    "    # Remove punctuation.\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "    # Tokenize.\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove stopwords.\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "    # Lemmatize.\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply preprocessing to the dataframe columns.\n",
    "tt_df_sub['description'] = tt_df_sub['description'].apply(preprocess_text)\n",
    "tt_df_sub['transcript'] = tt_df_sub['transcript'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2083a147",
   "metadata": {},
   "source": [
    "### User Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dc3a2e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to pre-process the comments column.\n",
    "\n",
    "def preprocess_comments(comments):\n",
    "    # Convert to lowercase.\n",
    "    comments = comments.lower()\n",
    "    \n",
    "    # Remove punctuation and numbers.\n",
    "    comments = comments.translate(str.maketrans('', '', string.punctuation + string.digits))\n",
    "    \n",
    "    # Tokenization.\n",
    "    tokens = word_tokenize(comments)\n",
    "    \n",
    "    # Stop words removal.\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # Lemmatization.\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
    "    \n",
    "    return ' '.join(lemmatized_tokens)\n",
    "\n",
    "# Apply the function to each comment.\n",
    "user_df['Comments'] = user_df['Comments'].apply(preprocess_comments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7c9b7a",
   "metadata": {},
   "source": [
    "### Feature Engineering "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c47186b",
   "metadata": {},
   "source": [
    "#### Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b813760d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to determine the word count of description and transcript columns.\n",
    "def compute_word_counts(df, column_name):\n",
    "    return df[column_name].apply(lambda x: len(x.split()))\n",
    "\n",
    "def add_word_counts(df):\n",
    "    df['description_word_count'] = compute_word_counts(df, 'description')\n",
    "    df['transcript_word_count'] = compute_word_counts(df, 'transcript')\n",
    "    return df\n",
    "\n",
    "tt_df_sub = add_word_counts(tt_df_sub)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703f4dd4",
   "metadata": {},
   "source": [
    "#### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "df30c597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to compute TF-IDF with sublinear TF scaling.\n",
    "def compute_tfidf_with_sublinear_scaling(df, column_name, max_features=5000):\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        max_df=0.30,\n",
    "        max_features=max_features,\n",
    "        stop_words='english',\n",
    "        sublinear_tf=True \n",
    "    )\n",
    "    tfidf_matrix = vectorizer.fit_transform(df[column_name])\n",
    "    return vectorizer, tfidf_matrix\n",
    "\n",
    "# Compute TF-IDF with sublinear TF scaling for description and transcript columns.\n",
    "description_vectorizer, description_tfidf_matrix = compute_tfidf_with_sublinear_scaling(tt_df_sub, 'description')\n",
    "transcript_vectorizer, transcript_tfidf_matrix = compute_tfidf_with_sublinear_scaling(tt_df_sub, 'transcript') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2971b970",
   "metadata": {},
   "source": [
    "#### N-Grams "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e3c2f480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute n-grams.\n",
    "def compute_tfidf_with_ngrams(df, column_name, max_features=5000, ngram_range=(1,2)):\n",
    "    vectorizer = TfidfVectorizer(max_df=0.85, max_features=max_features, stop_words='english', ngram_range=ngram_range)\n",
    "    tfidf_matrix = vectorizer.fit_transform(df[column_name])\n",
    "    return vectorizer, tfidf_matrix\n",
    "\n",
    "vectorizer, tfidf_matrix = compute_tfidf_with_ngrams(tt_df_sub, 'transcript')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e9b57f",
   "metadata": {},
   "source": [
    "#### Named Entity Recognition "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac08f5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#Function to perform Named Entity Recognition.\n",
    "\n",
    "# Load the large English model.\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "def extract_named_entities(text):\n",
    "    # Defining the entity labels we are interested in\n",
    "    relevant_entities = ['PERSON', 'ORG', 'GPE', 'DATE', 'EVENT', 'PRODUCT', 'WORK_OF_ART', 'LAW']\n",
    "    doc = nlp(text)\n",
    "    return [ent.text for ent in doc.ents if ent.label_ in relevant_entities]\n",
    "\n",
    "def clean_entities(entities):\n",
    "    return [ent for ent in entities if len(ent) > 2 and ent.lower() not in STOP_WORDS]\n",
    "\n",
    "def add_named_entities(df, column_name):\n",
    "    new_column = f\"{column_name}_named_entities\"\n",
    "    tqdm.pandas(desc=f\"Processing {column_name}\")\n",
    "    df[new_column] = df[column_name].progress_apply(extract_named_entities)\n",
    "    df[new_column] = df[new_column].apply(clean_entities)\n",
    "    return df\n",
    "\n",
    "tt_df_sub = add_named_entities(tt_df_sub, 'description')\n",
    "tt_df_sub = add_named_entities(tt_df_sub, 'transcript') '''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9ffd80",
   "metadata": {},
   "source": [
    "#### Keyword Extraction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8c20bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Function to perform keyword extraction.\n",
    "def extract_keywords(text):\n",
    "    kw_extractor = yake.KeywordExtractor()\n",
    "    keywords = kw_extractor.extract_keywords(text)\n",
    "    return [kw[0] for kw in keywords]\n",
    "\n",
    "# Apply the keyword extraction function to the 'transcript' column with tqdm progress.\n",
    "tt_df_sub['transcript_keywords'] = tqdm(tt_df_sub['transcript'].apply(extract_keywords), desc='Extracting Keywords')'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1951b61e",
   "metadata": {},
   "source": [
    "#### Document Embedding using BERT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23221c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Load pre-trained BERT model and tokenizer.\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9e9d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Function to perform BERT embedding.\n",
    "def get_bert_embedding(text):\n",
    "    # Tokenize input text\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "    \n",
    "    # Get BERT embeddings.\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Use the embedding of the [CLS] token as representation.\n",
    "    embedding = outputs.last_hidden_state[:, 0, :].numpy()\n",
    "\n",
    "    return embedding\n",
    "\n",
    "# Apply the function and see progress with tqdm.\n",
    "tt_df_sub['bert_embedding'] = [get_bert_embedding(x) for x in tqdm(tt_df_sub['transcript'])]'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179ed147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert DataFrame to CSV and Json for verification.\n",
    "#tt_df_sub.to_csv('tt_df_sub.csv', index=False)\n",
    "#tt_df_sub.to_json('tt_df_sub.json', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1067fdfd",
   "metadata": {},
   "source": [
    "### TedTalk Exploratory Data Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d5a69c",
   "metadata": {},
   "source": [
    "#### Descriptive Statistics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd83759",
   "metadata": {},
   "outputs": [],
   "source": [
    "tt_df_sub.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa5e20b",
   "metadata": {},
   "source": [
    "#### Word Count Distribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68606ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram to plot the distribution of description and transcript word count.\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(tt_df_sub['description_word_count'], kde=True, bins=50)\n",
    "plt.title('Distribution of Description Word Counts')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(tt_df_sub['transcript_word_count'], kde=True, bins=50)\n",
    "plt.title('Distribution of Transcript Word Counts')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53321d1",
   "metadata": {},
   "source": [
    "#### Word Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b86f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word cloud for transcript.\n",
    "def generate_wordcloud(text):\n",
    "    wordcloud = WordCloud(background_color='white', width=800, height=800).generate(text)\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "generate_wordcloud(' '.join(tt_df_sub['transcript'].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0176bd",
   "metadata": {},
   "source": [
    "### Model Building: Topic Modeling "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48322d44",
   "metadata": {},
   "source": [
    "#### 1) LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86585293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split each transcript into words.\n",
    "texts = tt_df_sub['transcript'].apply(lambda x: x.split()).tolist()\n",
    "\n",
    "# Build the bigram and trigram models.\n",
    "bigram = Phrases(texts, min_count=5, threshold=100)  \n",
    "trigram = Phrases(bigram[texts], threshold=100)\n",
    "\n",
    "# Construct bigram and trigram.\n",
    "bigram_mod = Phraser(bigram)\n",
    "trigram_mod = Phraser(trigram)\n",
    "\n",
    "# Define functions for creating bigrams and trigrams with tqdm progress.\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in tqdm(texts, desc='Making bigrams')]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in tqdm(texts, desc='Making trigrams')]\n",
    "\n",
    "# Form bigrams and trigrams with tqdm progress.\n",
    "texts_with_bigrams = make_bigrams(texts)\n",
    "texts_with_trigrams = make_trigrams(texts_with_bigrams)\n",
    "\n",
    "# Recreate the dictionary to include bigrams and trigrams.\n",
    "dictionary = corpora.Dictionary(texts_with_trigrams)\n",
    "\n",
    "# Filter out extremes to remove tokens that appear too frequently or infrequently.\n",
    "dictionary.filter_extremes(no_below=20, no_above=0.5)\n",
    "\n",
    "# Convert the documents to a bag-of-words representation with tqdm progress.\n",
    "corpus = [dictionary.doc2bow(text) for text in tqdm(texts_with_trigrams, desc='Creating corpus')]\n",
    "\n",
    "# Set up logging for progress tracking.\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "# Train LDA model with progress updates and additional hyperparameters.\n",
    "NUM_TOPICS = 15\n",
    "ldamodel = gensim.models.LdaMulticore(\n",
    "    corpus,\n",
    "    num_topics=NUM_TOPICS,\n",
    "    id2word=dictionary,\n",
    "    passes=35,\n",
    "    workers=3,\n",
    "    alpha='asymmetric',  \n",
    "    eta='auto',        \n",
    "    random_state=100,   \n",
    "    per_word_topics=True, \n",
    "    minimum_probability=0.01\n",
    ")\n",
    "\n",
    "# Print topics.\n",
    "ldatopics = ldamodel.print_topics(num_words=10)\n",
    "for topic in ldatopics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ece8542",
   "metadata": {},
   "source": [
    "#### Evaluation of LDA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8d81a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coherence Score.\n",
    "coherence_model_lda = CoherenceModel(model=ldamodel, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460c3cf9",
   "metadata": {},
   "source": [
    "#### 2) LSA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65dc62f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# Define the number of topics\n",
    "NUM_TOPICS = 15\n",
    "\n",
    "# Step 1: Apply LSA\n",
    "lsa_model = TruncatedSVD(n_components=NUM_TOPICS, random_state=42)\n",
    "lsa_model.fit(transcript_tfidf_matrix)\n",
    "\n",
    "# Step 2: Extract and display topics\n",
    "feature_names = transcript_vectorizer.get_feature_names_out()\n",
    "for i, topic in enumerate(lsa_model.components_):\n",
    "    top_features_ind = topic.argsort()[:-10 - 1:-1]\n",
    "    top_features = [feature_names[index] for index in top_features_ind]\n",
    "    weights = topic[top_features_ind]\n",
    "\n",
    "    print(f\"Topic {i+1}:\")\n",
    "    print(\", \".join(top_features))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbcc513",
   "metadata": {},
   "source": [
    "#### Evaluation of LSA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356bd0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the list of top words for each topic from LSA.\n",
    "top_n = 10  # Number of top words to consider for each topic.\n",
    "topics = [[feature_names[i] for i in topic.argsort()[:-top_n - 1:-1]] for topic in lsa_model.components_]\n",
    "\n",
    "# Prepare the Gensim dictionary and corpus.\n",
    "dictionary = Dictionary(texts_with_trigrams)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts_with_trigrams]\n",
    "\n",
    "# Create the Coherence Model.\n",
    "coherence_model = CoherenceModel(topics=topics, texts=texts_with_trigrams, dictionary=dictionary, coherence='c_v')\n",
    "\n",
    "# Compute the Coherence Score.\n",
    "coherence_score = coherence_model.get_coherence()\n",
    "print(f\"Coherence Score: {coherence_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b828e5de",
   "metadata": {},
   "source": [
    "#### 3) Non-Negative Matrix Factorization (NMF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0dc4a76a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashly\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1422: FutureWarning: `alpha` was deprecated in version 1.0 and will be removed in 1.2. Use `alpha_W` and `alpha_H` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "violation: 1.0\n",
      "violation: 0.31919591865528496\n",
      "violation: 0.15128732919873691\n",
      "violation: 0.08856207734618604\n",
      "violation: 0.06177094804089835\n",
      "violation: 0.04790379465885834\n",
      "violation: 0.038413986696961916\n",
      "violation: 0.03103121305108189\n",
      "violation: 0.025104880946314877\n",
      "violation: 0.020390443160571224\n",
      "violation: 0.01676954078338039\n",
      "violation: 0.01383124040562342\n",
      "violation: 0.011537328903220472\n",
      "violation: 0.009703092688836026\n",
      "violation: 0.008293561550704544\n",
      "violation: 0.007170082145667308\n",
      "violation: 0.0062766434413592375\n",
      "violation: 0.005560448889492952\n",
      "violation: 0.004978807466155058\n",
      "violation: 0.004504132603744751\n",
      "violation: 0.004117634664718943\n",
      "violation: 0.0038051863169598155\n",
      "violation: 0.003546878279343285\n",
      "violation: 0.0033341345321302005\n",
      "violation: 0.0031523735336994704\n",
      "violation: 0.00298877632589311\n",
      "violation: 0.0028427033207806835\n",
      "violation: 0.0027058606370142396\n",
      "violation: 0.0025780799665862356\n",
      "violation: 0.002456316625225052\n",
      "violation: 0.002332966492131374\n",
      "violation: 0.002207536439232598\n",
      "violation: 0.002080195013176415\n",
      "violation: 0.001954541512356884\n",
      "violation: 0.001828805395694046\n",
      "violation: 0.0017077095816350116\n",
      "violation: 0.0015894373203254244\n",
      "violation: 0.0014740900269962882\n",
      "violation: 0.0013657515942465193\n",
      "violation: 0.0012639863168949215\n",
      "violation: 0.001165358690707583\n",
      "violation: 0.001072466133846523\n",
      "violation: 0.0009828732041728802\n",
      "violation: 0.0008991307844745208\n",
      "violation: 0.0008214405686273132\n",
      "violation: 0.0007510892786129143\n",
      "violation: 0.0006871412930080952\n",
      "violation: 0.000628067734126628\n",
      "violation: 0.0005734054192242521\n",
      "violation: 0.0005232122482533313\n",
      "violation: 0.0004772633978969811\n",
      "violation: 0.00043511645930646134\n",
      "violation: 0.0003966242159864226\n",
      "violation: 0.0003616045978366775\n",
      "violation: 0.0003291917240158183\n",
      "violation: 0.0002994387190445168\n",
      "violation: 0.00027209818964275877\n",
      "violation: 0.0002471743383308602\n",
      "violation: 0.0002244787144243676\n",
      "violation: 0.00020417583337217394\n",
      "violation: 0.00018591220803588628\n",
      "violation: 0.00016949681771610424\n",
      "violation: 0.00015463890977089135\n",
      "violation: 0.00014118923572843014\n",
      "violation: 0.0001289838554309231\n",
      "violation: 0.00011791804546280596\n",
      "violation: 0.00010788645812059386\n",
      "violation: 9.868579181992908e-05\n",
      "Converged at iteration 69\n",
      "Topic #1: guy, man, kid, book, young, wasnt, face, community, knew, hour\n",
      "Topic #2: plant, water, carbon, climate, energy, planet, earth, specie, food, atmosphere\n",
      "Topic #3: economy, dollar, economic, market, business, money, government, company, income, billion\n",
      "Topic #4: patient, cell, disease, cancer, drug, treatment, blood, doctor, medical, medicine\n",
      "Topic #5: ocean, fish, sea, coral, marine, reef, fishing, shark, underwater, whale\n",
      "Topic #6: election, vote, democracy, political, voting, voter, politics, democratic, candidate, citizen\n",
      "Topic #7: universe, galaxy, star, particle, earth, telescope, space, sun, planet, atom\n",
      "Topic #8: city, building, architecture, design, architect, urban, space, built, project, construction\n",
      "Topic #9: computer, data, digital, internet, information, device, online, web, software, phone\n",
      "Topic #10: brain, neuron, signal, cortex, memory, electrical, activity, neural, cell, neuroscience\n",
      "Topic #11: teacher, student, classroom, education, class, teaching, learning, kid, teach, grade\n",
      "Topic #12: africa, african, continent, nigeria, south, ghana, kenya, nigerian, aid, west\n",
      "Topic #13: robot, robotics, ai, machine, robotic, artificial, intelligence, autonomous, task, sensor\n",
      "Topic #14: music, musical, song, musician, sound, instrument, play, composer, melody, piano\n",
      "Topic #15: color, blue, red, green, light, yellow, purple, frequency, eye, retina\n"
     ]
    }
   ],
   "source": [
    "num_topics = 15\n",
    "\n",
    "# Create an NMF model.\n",
    "nmf_model = NMF(\n",
    "    n_components=num_topics,        # Number of topics.\n",
    "    init='nndsvd',                 # Initialization method .\n",
    "    random_state=42,               # Random seed for reproducibility.\n",
    "    alpha=0.1,                   # Regularization parameters.\n",
    "    l1_ratio=0.8,                  # Sparsity parameter. \n",
    "    max_iter=200,                  # Maximum number of iterations.\n",
    "    verbose=1                      # Verbosity level (1 for progress updates).\n",
    ")\n",
    "\n",
    "# Fit the model to the 'transcript' TF-IDF matrix.\n",
    "nmf_model.fit(transcript_tfidf_matrix)\n",
    "\n",
    "# Get the topics and the topic-word distributions.\n",
    "nmftopics = nmf_model.components_\n",
    "\n",
    "n_top_words = 10  \n",
    "feature_names = transcript_vectorizer.get_feature_names_out()\n",
    "for topic_idx, topic in enumerate(nmftopics):\n",
    "    top_words_idx = topic.argsort()[:-n_top_words - 1:-1]\n",
    "    top_words = [feature_names[i] for i in top_words_idx]\n",
    "    print(f\"Topic #{topic_idx + 1}: {', '.join(top_words)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e5e565",
   "metadata": {},
   "source": [
    "#### Evaluation of Non-Negative Matrix Factorization (NMF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c75f92db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence Score: 0.7340553160984783\n"
     ]
    }
   ],
   "source": [
    "# Create a Gensim Dictionary.\n",
    "transcript_dict = Dictionary(tt_df_sub['transcript'].apply(str.split))\n",
    "\n",
    "# Convert the NMF topics into a format suitable for coherence calculation.\n",
    "nmf_topics = [[feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]] for topic in nmftopics]\n",
    "\n",
    "# Calculate the coherence score using the 'c_v' measure.\n",
    "coherence_model = CoherenceModel(topics=nmf_topics, texts=tt_df_sub['transcript'].apply(str.split), dictionary=transcript_dict, coherence='c_v')\n",
    "coherence_score = coherence_model.get_coherence()\n",
    "\n",
    "# Print the coherence score.\n",
    "print(f\"Coherence Score: {coherence_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba196929",
   "metadata": {},
   "source": [
    "#### 4) CTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b63c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data for CTM.\n",
    "tp = TopicModelDataPreparation(\"bert-base-nli-mean-tokens\")\n",
    "\n",
    "training_dataset = tp.fit(text_for_contextual=tt_df['transcript'].tolist(), \n",
    "                          text_for_bow=tt_df_sub['transcript'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6516acbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the size of the vocabulary and contextual embeddings.\n",
    "bow_size = len(tp.vocab)  \n",
    "contextual_size = 768  \n",
    "\n",
    "num_topics = 10  \n",
    "num_epochs = 10  \n",
    "\n",
    "# Initialize the CombinedTM model.\n",
    "ctm = CombinedTM(bow_size=bow_size, contextual_size=contextual_size, n_components=num_topics, num_epochs=num_epochs)\n",
    "\n",
    "# Train the model.\n",
    "ctm.fit(training_dataset)\n",
    "\n",
    "# Extract and display the topics.\n",
    "topics = ctm.get_topic_lists(10)\n",
    "for topic_idx, topic in enumerate(topics):\n",
    "    print(f\"Topic {topic_idx + 1}: {', '.join(topic)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f915da96",
   "metadata": {},
   "source": [
    "#### Evaluation of CTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7ba53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the CTM topics to a format suitable for coherence calculation.\n",
    "ctm_topics = ctm.get_topic_lists(10)\n",
    "\n",
    "# Create a dictionary and corpus for the topics.\n",
    "dictionary = corpora.Dictionary(ctm_topics)\n",
    "corpus = [dictionary.doc2bow(topic) for topic in ctm_topics]\n",
    "\n",
    "# Calculate the coherence score using UMass coherence (you can also use C_v coherence).\n",
    "coherence_model = CoherenceModel(model=None, corpus=corpus, dictionary=dictionary, coherence='u_mass', topics=ctm_topics)\n",
    "coherence_score = coherence_model.get_coherence()\n",
    "\n",
    "print(f\"Coherence Score: {coherence_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2bf51a",
   "metadata": {},
   "source": [
    "#### Topic Distribution for NMF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14db4acb",
   "metadata": {},
   "source": [
    "This model performs better as compared to LDA, LSA and CMT. Hence it is selected for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5bc05c02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "violation: 1.0\n",
      "violation: 0.10164830173445182\n",
      "violation: 0.0051536844945916606\n",
      "violation: 0.000539188364636062\n",
      "violation: 5.325244079046646e-05\n",
      "Converged at iteration 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashly\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1422: FutureWarning: `alpha` was deprecated in version 1.0 and will be removed in 1.2. Use `alpha_W` and `alpha_H` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Get the topic distributions for each document from NMF.\n",
    "nmf_topic_distributions = nmf_model.transform(transcript_tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "06f43a6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Topic_1   Topic_2   Topic_3  Topic_4  Topic_5   Topic_6  Topic_7  \\\n",
      "0  0.059413  0.040164  0.029419      0.0      0.0  0.005039      0.0   \n",
      "1  0.043325  0.000000  0.094778      0.0      0.0  0.000000      0.0   \n",
      "2  0.079526  0.000000  0.000000      0.0      0.0  0.000000      0.0   \n",
      "3  0.068771  0.022616  0.055075      0.0      0.0  0.000000      0.0   \n",
      "4  0.084162  0.000000  0.000000      0.0      0.0  0.000000      0.0   \n",
      "\n",
      "    Topic_8   Topic_9  Topic_10  Topic_11  Topic_12  Topic_13  Topic_14  \\\n",
      "0  0.000000  0.000000       0.0  0.000000  0.000000       0.0  0.000000   \n",
      "1  0.000000  0.020898       0.0  0.001317  0.038808       0.0  0.000000   \n",
      "2  0.000000  0.044134       0.0  0.000000  0.000000       0.0  0.000000   \n",
      "3  0.072825  0.000000       0.0  0.000000  0.000000       0.0  0.000000   \n",
      "4  0.000000  0.000000       0.0  0.034352  0.000000       0.0  0.015967   \n",
      "\n",
      "   Topic_15  \n",
      "0       0.0  \n",
      "1       0.0  \n",
      "2       0.0  \n",
      "3       0.0  \n",
      "4       0.0  \n"
     ]
    }
   ],
   "source": [
    "# Creating a DataFrame from the NMF topic distributions.\n",
    "topic_dist_df = pd.DataFrame(nmf_topic_distributions, columns=[f\"Topic_{i+1}\" for i in range(nmf_topic_distributions.shape[1])])\n",
    "\n",
    "# Display the first few rows of the DataFrame to verify.\n",
    "print(topic_dist_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdeadbb",
   "metadata": {},
   "source": [
    "#### Clustering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "213bac75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashly\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:623: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n"
     ]
    }
   ],
   "source": [
    "# Choose the number of clusters.\n",
    "num_clusters = 12  \n",
    "\n",
    "# Perform K-Means clustering.\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "clusters = kmeans.fit_predict(topic_dist_df)\n",
    "\n",
    "# Add the cluster information to the DataFrame.\n",
    "topic_dist_df['Cluster'] = clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "294cd2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the cleaned TED Talks DataFrame with the topic distribution DataFrame and clusters.\n",
    "cluster_topic_df = pd.concat([tt_df_sub, topic_dist_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6a11083a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert it to a csv file.\n",
    "cluster_topic_df.to_csv('cluster_topic_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0149adf",
   "metadata": {},
   "source": [
    "#### Evaluation of Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9ec44639",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashly\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:623: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette Score: 0.9510161374347714\n",
      "Calinski-Harabasz Index: 2272525.173119256\n",
      "Davies-Bouldin Index: 0.0829712276575479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashly\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:623: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "C:\\Users\\ashly\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:623: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n"
     ]
    }
   ],
   "source": [
    "# Silhouette Score\n",
    "silhouette = silhouette_score(topic_dist_df, clusters)\n",
    "\n",
    "# Calinski-Harabasz Index\n",
    "calinski_harabasz = calinski_harabasz_score(topic_dist_df, clusters)\n",
    "\n",
    "# Davies-Bouldin Index\n",
    "davies_bouldin = davies_bouldin_score(topic_dist_df, clusters)\n",
    "\n",
    "print(f\"Silhouette Score: {silhouette}\")\n",
    "print(f\"Calinski-Harabasz Index: {calinski_harabasz}\")\n",
    "print(f\"Davies-Bouldin Index: {davies_bouldin}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec8f11f",
   "metadata": {},
   "source": [
    "### Model Building: Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f05e7ba",
   "metadata": {},
   "source": [
    "#### 1) DistilBert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6283287b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          Comments  \\\n",
      "0          moved tear emotional rollercoaster talk   \n",
      "1            symphony thoughtprovoking idea superb   \n",
      "2            somewhat confusing hard follow honest   \n",
      "3           true gem talk informative entertaining   \n",
      "4                  talk transition topic bit rough   \n",
      "...                                            ...   \n",
      "1651           conclusion drawn weak unfortunately   \n",
      "1652                   lecture talk wasnt engaging   \n",
      "1653              speaker explanation unclear felt   \n",
      "1654              lacked wow factor expected sadly   \n",
      "1655  left wanting substance expecting deeper dive   \n",
      "\n",
      "                                     Sentiment_Analysis  \n",
      "0     {'label': 'POSITIVE', 'score': 0.9970806241035...  \n",
      "1     {'label': 'POSITIVE', 'score': 0.999852180480957}  \n",
      "2     {'label': 'POSITIVE', 'score': 0.7286676168441...  \n",
      "3     {'label': 'POSITIVE', 'score': 0.9998641014099...  \n",
      "4     {'label': 'NEGATIVE', 'score': 0.998521625995636}  \n",
      "...                                                 ...  \n",
      "1651  {'label': 'NEGATIVE', 'score': 0.9990381002426...  \n",
      "1652  {'label': 'NEGATIVE', 'score': 0.9993945360183...  \n",
      "1653  {'label': 'NEGATIVE', 'score': 0.998503565788269}  \n",
      "1654  {'label': 'NEGATIVE', 'score': 0.9941835999488...  \n",
      "1655  {'label': 'NEGATIVE', 'score': 0.9661938548088...  \n",
      "\n",
      "[1656 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Load DistilBERT tokenizer and model.\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased-finetuned-sst-2-english')\n",
    "\n",
    "# Create a pipeline using the loaded model and tokenizer.\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "def analyze_sentiment(comment):\n",
    "    result = sentiment_pipeline(comment)\n",
    "    return result[0]\n",
    "\n",
    "# Apply the function to the 'Comments' column.\n",
    "user_df['Sentiment_Analysis'] = user_df['Comments'].apply(lambda x: analyze_sentiment(x))\n",
    "\n",
    "# Display the results.\n",
    "print(user_df[['Comments', 'Sentiment_Analysis']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b9d6a8",
   "metadata": {},
   "source": [
    "#### 2) RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "119199f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  Comments Sentiment_RoBERTa\n",
      "0  moved tear emotional rollercoaster talk          positive\n",
      "1    symphony thoughtprovoking idea superb          positive\n",
      "2    somewhat confusing hard follow honest          negative\n",
      "3   true gem talk informative entertaining          positive\n",
      "4          talk transition topic bit rough          negative\n"
     ]
    }
   ],
   "source": [
    "# Load the sentiment analysis pipeline with the specific RoBERTa model.\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\", model=\"cardiffnlp/twitter-roberta-base-sentiment\")\n",
    "\n",
    "# Function to analyze sentiment using the pipeline.\n",
    "def analyze_sentiment_roberta(comment):\n",
    "    try:\n",
    "        result = sentiment_pipeline(comment)\n",
    "        label = result[0]['label']\n",
    "\n",
    "        # Map numerical labels to text\n",
    "        label_mapping = {\n",
    "            'LABEL_0': 'negative',\n",
    "            'LABEL_1': 'neutral',\n",
    "            'LABEL_2': 'positive'\n",
    "        }\n",
    "\n",
    "        return label_mapping.get(label, \"unknown\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing comment: {comment}. Error: {e}\")\n",
    "        return \"error\"\n",
    "\n",
    "# Apply the function to the DataFrame.\n",
    "user_df['Sentiment_RoBERTa'] = user_df['Comments'].apply(analyze_sentiment_roberta)\n",
    "\n",
    "# Display the results.\n",
    "print(user_df[['Comments', 'Sentiment_RoBERTa']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada8aeec",
   "metadata": {},
   "source": [
    "RoBERTa will be chosen for sentiment analysis as it performs better as compared to DistilBert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3acb3ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert it to a csv file.\n",
    "user_df.to_csv('usersenti_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4bf457c",
   "metadata": {},
   "source": [
    "### Recommendation System "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fafdc6",
   "metadata": {},
   "source": [
    "#### 1) Content-Based Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fba9507",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0ee522",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1ea7104f",
   "metadata": {},
   "source": [
    "#### 2) Collaborative Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2996c978",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbecba2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f2f2f265",
   "metadata": {},
   "source": [
    "#### 3) Hybrid Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c07ed17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c2bc53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
