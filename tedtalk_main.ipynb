{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cf2ffb8",
   "metadata": {},
   "source": [
    "### Import Necessary Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec1e2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install spacy\n",
    "#!pip install TextBlob\n",
    "#!pip install tqdm\n",
    "#!python -m spacy download en_core_web_sm\n",
    "#!pip install yake\n",
    "#!pip install transformers torch\n",
    "#!python -m spacy download en_core_web_lg\n",
    "#!pip install bertopic --user\n",
    "#!pip install hdbscan\n",
    "#!pip install contextualized-topic-models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8edd06e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\ashly\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ashly\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ashly\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ashly\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\ashly\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ashly\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ashly\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\ashly\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package conll2000 to\n",
      "[nltk_data]     C:\\Users\\ashly\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package conll2000 is already up-to-date!\n",
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\ashly\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from textblob import download_corpora\n",
    "download_corpora.main()\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from textblob import TextBlob\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from tqdm.notebook import tqdm\n",
    "import yake\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "from sklearn.decomposition import NMF\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from tqdm.auto import tqdm\n",
    "import logging\n",
    "\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from contextualized_topic_models.models.ctm import CombinedTM\n",
    "from contextualized_topic_models.models.ctm import CTM\n",
    "from contextualized_topic_models.utils.data_preparation import TopicModelDataPreparation\n",
    "from contextualized_topic_models.datasets.dataset import CTMDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050751da",
   "metadata": {},
   "source": [
    "### Data Loading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddfdbef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (4005, 19)\n"
     ]
    }
   ],
   "source": [
    "# Function to load the data from the csv file to a dataframe and print the shape.\n",
    "def load_data_and_print_shape(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(f'Shape: {df.shape}')\n",
    "    return df\n",
    "\n",
    "tt_df = load_data_and_print_shape('ted_talks.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b537863",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['talk_id', 'title', 'speaker_1', 'all_speakers', 'occupations',\n",
       "       'about_speakers', 'views', 'recorded_date', 'published_date', 'event',\n",
       "       'native_lang', 'available_lang', 'comments', 'duration', 'topics',\n",
       "       'related_talks', 'url', 'description', 'transcript'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the columns present in the dataframe.\n",
    "tt_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "579cbaf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>talk_id</th>\n",
       "      <th>title</th>\n",
       "      <th>speaker_1</th>\n",
       "      <th>all_speakers</th>\n",
       "      <th>occupations</th>\n",
       "      <th>about_speakers</th>\n",
       "      <th>views</th>\n",
       "      <th>recorded_date</th>\n",
       "      <th>published_date</th>\n",
       "      <th>event</th>\n",
       "      <th>native_lang</th>\n",
       "      <th>available_lang</th>\n",
       "      <th>comments</th>\n",
       "      <th>duration</th>\n",
       "      <th>topics</th>\n",
       "      <th>related_talks</th>\n",
       "      <th>url</th>\n",
       "      <th>description</th>\n",
       "      <th>transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Averting the climate crisis</td>\n",
       "      <td>Al Gore</td>\n",
       "      <td>{0: 'Al Gore'}</td>\n",
       "      <td>{0: ['climate advocate']}</td>\n",
       "      <td>{0: 'Nobel Laureate Al Gore focused the world’...</td>\n",
       "      <td>3523392</td>\n",
       "      <td>2006-02-25</td>\n",
       "      <td>2006-06-27</td>\n",
       "      <td>TED2006</td>\n",
       "      <td>en</td>\n",
       "      <td>['ar', 'bg', 'cs', 'de', 'el', 'en', 'es', 'fa...</td>\n",
       "      <td>272.0</td>\n",
       "      <td>977</td>\n",
       "      <td>['alternative energy', 'cars', 'climate change...</td>\n",
       "      <td>{243: 'New thinking on the climate crisis', 54...</td>\n",
       "      <td>https://www.ted.com/talks/al_gore_averting_the...</td>\n",
       "      <td>With the same humor and humanity he exuded in ...</td>\n",
       "      <td>Thank you so much, Chris. And it's truly a gre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>92</td>\n",
       "      <td>The best stats you've ever seen</td>\n",
       "      <td>Hans Rosling</td>\n",
       "      <td>{0: 'Hans Rosling'}</td>\n",
       "      <td>{0: ['global health expert; data visionary']}</td>\n",
       "      <td>{0: 'In Hans Rosling’s hands, data sings. Glob...</td>\n",
       "      <td>14501685</td>\n",
       "      <td>2006-02-22</td>\n",
       "      <td>2006-06-27</td>\n",
       "      <td>TED2006</td>\n",
       "      <td>en</td>\n",
       "      <td>['ar', 'az', 'bg', 'bn', 'bs', 'cs', 'da', 'de...</td>\n",
       "      <td>628.0</td>\n",
       "      <td>1190</td>\n",
       "      <td>['Africa', 'Asia', 'Google', 'demo', 'economic...</td>\n",
       "      <td>{2056: \"Own your body's data\", 2296: 'A visual...</td>\n",
       "      <td>https://www.ted.com/talks/hans_rosling_the_bes...</td>\n",
       "      <td>You've never seen data presented like this. Wi...</td>\n",
       "      <td>About 10 years ago, I took on the task to teac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>Simplicity sells</td>\n",
       "      <td>David Pogue</td>\n",
       "      <td>{0: 'David Pogue'}</td>\n",
       "      <td>{0: ['technology columnist']}</td>\n",
       "      <td>{0: 'David Pogue is the personal technology co...</td>\n",
       "      <td>1920832</td>\n",
       "      <td>2006-02-24</td>\n",
       "      <td>2006-06-27</td>\n",
       "      <td>TED2006</td>\n",
       "      <td>en</td>\n",
       "      <td>['ar', 'bg', 'de', 'el', 'en', 'es', 'fa', 'fr...</td>\n",
       "      <td>124.0</td>\n",
       "      <td>1286</td>\n",
       "      <td>['computers', 'entertainment', 'interface desi...</td>\n",
       "      <td>{1725: '10 top time-saving tech tips', 2274: '...</td>\n",
       "      <td>https://www.ted.com/talks/david_pogue_simplici...</td>\n",
       "      <td>New York Times columnist David Pogue takes aim...</td>\n",
       "      <td>(Music: \"The Sound of Silence,\" Simon &amp; Garfun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>Greening the ghetto</td>\n",
       "      <td>Majora Carter</td>\n",
       "      <td>{0: 'Majora Carter'}</td>\n",
       "      <td>{0: ['activist for environmental justice']}</td>\n",
       "      <td>{0: 'Majora Carter redefined the field of envi...</td>\n",
       "      <td>2664069</td>\n",
       "      <td>2006-02-26</td>\n",
       "      <td>2006-06-27</td>\n",
       "      <td>TED2006</td>\n",
       "      <td>en</td>\n",
       "      <td>['ar', 'bg', 'bn', 'ca', 'cs', 'de', 'en', 'es...</td>\n",
       "      <td>219.0</td>\n",
       "      <td>1116</td>\n",
       "      <td>['MacArthur grant', 'activism', 'business', 'c...</td>\n",
       "      <td>{1041: '3 stories of local eco-entrepreneurshi...</td>\n",
       "      <td>https://www.ted.com/talks/majora_carter_greeni...</td>\n",
       "      <td>In an emotionally charged talk, MacArthur-winn...</td>\n",
       "      <td>If you're here today — and I'm very happy that...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>66</td>\n",
       "      <td>Do schools kill creativity?</td>\n",
       "      <td>Sir Ken Robinson</td>\n",
       "      <td>{0: 'Sir Ken Robinson'}</td>\n",
       "      <td>{0: ['author', 'educator']}</td>\n",
       "      <td>{0: \"Creativity expert Sir Ken Robinson challe...</td>\n",
       "      <td>65051954</td>\n",
       "      <td>2006-02-25</td>\n",
       "      <td>2006-06-27</td>\n",
       "      <td>TED2006</td>\n",
       "      <td>en</td>\n",
       "      <td>['af', 'ar', 'az', 'be', 'bg', 'bn', 'ca', 'cs...</td>\n",
       "      <td>4931.0</td>\n",
       "      <td>1164</td>\n",
       "      <td>['children', 'creativity', 'culture', 'dance',...</td>\n",
       "      <td>{865: 'Bring on the learning revolution!', 173...</td>\n",
       "      <td>https://www.ted.com/talks/sir_ken_robinson_do_...</td>\n",
       "      <td>Sir Ken Robinson makes an entertaining and pro...</td>\n",
       "      <td>Good morning. How are you? (Audience) Good. It...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   talk_id                            title         speaker_1  \\\n",
       "0        1      Averting the climate crisis           Al Gore   \n",
       "1       92  The best stats you've ever seen      Hans Rosling   \n",
       "2        7                 Simplicity sells       David Pogue   \n",
       "3       53              Greening the ghetto     Majora Carter   \n",
       "4       66      Do schools kill creativity?  Sir Ken Robinson   \n",
       "\n",
       "              all_speakers                                    occupations  \\\n",
       "0           {0: 'Al Gore'}                      {0: ['climate advocate']}   \n",
       "1      {0: 'Hans Rosling'}  {0: ['global health expert; data visionary']}   \n",
       "2       {0: 'David Pogue'}                  {0: ['technology columnist']}   \n",
       "3     {0: 'Majora Carter'}    {0: ['activist for environmental justice']}   \n",
       "4  {0: 'Sir Ken Robinson'}                    {0: ['author', 'educator']}   \n",
       "\n",
       "                                      about_speakers     views recorded_date  \\\n",
       "0  {0: 'Nobel Laureate Al Gore focused the world’...   3523392    2006-02-25   \n",
       "1  {0: 'In Hans Rosling’s hands, data sings. Glob...  14501685    2006-02-22   \n",
       "2  {0: 'David Pogue is the personal technology co...   1920832    2006-02-24   \n",
       "3  {0: 'Majora Carter redefined the field of envi...   2664069    2006-02-26   \n",
       "4  {0: \"Creativity expert Sir Ken Robinson challe...  65051954    2006-02-25   \n",
       "\n",
       "  published_date    event native_lang  \\\n",
       "0     2006-06-27  TED2006          en   \n",
       "1     2006-06-27  TED2006          en   \n",
       "2     2006-06-27  TED2006          en   \n",
       "3     2006-06-27  TED2006          en   \n",
       "4     2006-06-27  TED2006          en   \n",
       "\n",
       "                                      available_lang  comments  duration  \\\n",
       "0  ['ar', 'bg', 'cs', 'de', 'el', 'en', 'es', 'fa...     272.0       977   \n",
       "1  ['ar', 'az', 'bg', 'bn', 'bs', 'cs', 'da', 'de...     628.0      1190   \n",
       "2  ['ar', 'bg', 'de', 'el', 'en', 'es', 'fa', 'fr...     124.0      1286   \n",
       "3  ['ar', 'bg', 'bn', 'ca', 'cs', 'de', 'en', 'es...     219.0      1116   \n",
       "4  ['af', 'ar', 'az', 'be', 'bg', 'bn', 'ca', 'cs...    4931.0      1164   \n",
       "\n",
       "                                              topics  \\\n",
       "0  ['alternative energy', 'cars', 'climate change...   \n",
       "1  ['Africa', 'Asia', 'Google', 'demo', 'economic...   \n",
       "2  ['computers', 'entertainment', 'interface desi...   \n",
       "3  ['MacArthur grant', 'activism', 'business', 'c...   \n",
       "4  ['children', 'creativity', 'culture', 'dance',...   \n",
       "\n",
       "                                       related_talks  \\\n",
       "0  {243: 'New thinking on the climate crisis', 54...   \n",
       "1  {2056: \"Own your body's data\", 2296: 'A visual...   \n",
       "2  {1725: '10 top time-saving tech tips', 2274: '...   \n",
       "3  {1041: '3 stories of local eco-entrepreneurshi...   \n",
       "4  {865: 'Bring on the learning revolution!', 173...   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://www.ted.com/talks/al_gore_averting_the...   \n",
       "1  https://www.ted.com/talks/hans_rosling_the_bes...   \n",
       "2  https://www.ted.com/talks/david_pogue_simplici...   \n",
       "3  https://www.ted.com/talks/majora_carter_greeni...   \n",
       "4  https://www.ted.com/talks/sir_ken_robinson_do_...   \n",
       "\n",
       "                                         description  \\\n",
       "0  With the same humor and humanity he exuded in ...   \n",
       "1  You've never seen data presented like this. Wi...   \n",
       "2  New York Times columnist David Pogue takes aim...   \n",
       "3  In an emotionally charged talk, MacArthur-winn...   \n",
       "4  Sir Ken Robinson makes an entertaining and pro...   \n",
       "\n",
       "                                          transcript  \n",
       "0  Thank you so much, Chris. And it's truly a gre...  \n",
       "1  About 10 years ago, I took on the task to teac...  \n",
       "2  (Music: \"The Sound of Silence,\" Simon & Garfun...  \n",
       "3  If you're here today — and I'm very happy that...  \n",
       "4  Good morning. How are you? (Audience) Good. It...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the first five rows  of the dataframe.\n",
    "tt_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644a5e21",
   "metadata": {},
   "source": [
    "### Data Pre-Processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6ee5eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract only the columns required for analysis.\n",
    "def extract_required_columns(df):\n",
    "    required_columns = ['talk_id', 'title', 'speaker_1', 'occupations', 'event', \n",
    "                        'published_date', 'duration', 'topics', 'description', \n",
    "                        'views', 'comments', 'transcript']\n",
    "    return df[required_columns]\n",
    "tt_df_sub = extract_required_columns(tt_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf4e6a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null values per column:\n",
      "talk_id             0\n",
      "title               0\n",
      "speaker_1           0\n",
      "occupations       522\n",
      "event               0\n",
      "published_date      0\n",
      "duration            0\n",
      "topics              0\n",
      "description         0\n",
      "views               0\n",
      "comments          655\n",
      "transcript          0\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "Number of duplicate rows: 0\n",
      "\n",
      "No duplicated rows found.\n"
     ]
    }
   ],
   "source": [
    "# Function to check for null values and duplicates in the dataframe.\n",
    "def check_data_quality(df):\n",
    "    # Check for null values in the DataFrame.\n",
    "    null_values = df.isnull().sum()\n",
    "    print(\"Null values per column:\")\n",
    "    print(null_values)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # Check for duplicate values.\n",
    "    duplicate_rows = df[df.duplicated()]\n",
    "\n",
    "    # To see the number of duplicate rows.\n",
    "    num_duplicate_rows = duplicate_rows.shape[0]\n",
    "    print(f\"Number of duplicate rows: {num_duplicate_rows}\\n\")\n",
    "\n",
    "    # View the duplicated rows if any.\n",
    "    if num_duplicate_rows > 0:\n",
    "        print(\"Duplicated rows:\")\n",
    "        print(duplicate_rows)\n",
    "    else:\n",
    "        print(\"No duplicated rows found.\")\n",
    "\n",
    "# Use the function:\n",
    "check_data_quality(tt_df_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e59d3d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "talk_id             int64\n",
      "title              object\n",
      "speaker_1          object\n",
      "occupations        object\n",
      "event              object\n",
      "published_date     object\n",
      "duration            int64\n",
      "topics             object\n",
      "description        object\n",
      "views               int64\n",
      "comments          float64\n",
      "transcript         object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Check for data types of columns.\n",
    "print(tt_df_sub.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047a1644",
   "metadata": {},
   "source": [
    "#### Basic Pre-Processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5cbfd229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for basic data pre-processing.\n",
    "\n",
    "def preprocess_dataframe(df):\n",
    "    def fill_missing_values(df):\n",
    "        \"\"\"Replace null values with default values.\"\"\"\n",
    "        df['occupations'].fillna('Unknown', inplace=True)\n",
    "        df['comments'].fillna(0, inplace=True)\n",
    "        return df\n",
    "    \n",
    "    def clean_strings(df, columns, chars_to_remove):\n",
    "        \"\"\"Remove specified characters from given columns.\"\"\"\n",
    "        remove_dict = {ord(char): None for char in chars_to_remove}\n",
    "        for col in columns:\n",
    "            df[col] = df[col].str.translate(remove_dict)\n",
    "        return df\n",
    "\n",
    "    def remove_numbers_from_column(df, column):\n",
    "        \"\"\"Remove numbers from specified column.\"\"\"\n",
    "        df[column] = df[column].str.replace(r'\\d+', '', regex=True)\n",
    "        return df\n",
    "\n",
    "    def convert_duration_to_minutes(df, column):\n",
    "        \"\"\"Convert duration from seconds to minutes.\"\"\"\n",
    "        df[column] = (df[column] / 60).round(2)\n",
    "        return df\n",
    "\n",
    "    def convert_to_datetime(df, column):\n",
    "        \"\"\"Convert specified column to datetime.\"\"\"\n",
    "        df[column] = pd.to_datetime(df[column])\n",
    "        return df\n",
    "\n",
    "    def extract_year(df, source_column, new_column):\n",
    "        \"\"\"Extract year from source column and create new column.\"\"\"\n",
    "        df[new_column] = pd.to_datetime(df[source_column]).dt.year\n",
    "        return df\n",
    "\n",
    "    def convert_column_to_int64(df, column):\n",
    "        \"\"\"Convert specified column to int64.\"\"\"\n",
    "        df[column] = df[column].astype('int64')\n",
    "        return df\n",
    "\n",
    "    def rearrange_columns(df, columns_order):\n",
    "        \"\"\"Rearrange dataframe columns based on given order.\"\"\"\n",
    "        return df[columns_order]\n",
    "\n",
    "    def rename_columns(df, mapping):\n",
    "        \"\"\"Rename dataframe columns based on given mapping.\"\"\"\n",
    "        return df.rename(columns=mapping)\n",
    "\n",
    "    # Make a copy to avoid SettingWithCopyWarning.\n",
    "    df = df.copy()\n",
    "\n",
    "    df = fill_missing_values(df)\n",
    "    df = clean_strings(df, ['occupations', 'topics'], ['{', '#', ':', '[', '\\'', ';', ',', ']', '}'])\n",
    "    df = remove_numbers_from_column(df, 'occupations')\n",
    "    df = convert_duration_to_minutes(df, 'duration')\n",
    "    df = convert_to_datetime(df, 'published_date')\n",
    "    df = extract_year(df, 'published_date', 'year')\n",
    "    df = convert_column_to_int64(df, 'comments')\n",
    "    df = rearrange_columns(df, [df.columns[-1]] + df.columns[:-1].tolist())\n",
    "    df = rename_columns(df, {'speaker_1': 'speaker'})\n",
    "\n",
    "    return df\n",
    "\n",
    "# Main preprocessing function.\n",
    "tt_df_sub = preprocess_dataframe(tt_df_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0dabb5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "year              0\n",
      "talk_id           0\n",
      "title             0\n",
      "speaker           0\n",
      "occupations       0\n",
      "event             0\n",
      "published_date    0\n",
      "duration          0\n",
      "topics            0\n",
      "description       0\n",
      "views             0\n",
      "comments          0\n",
      "transcript        0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Verify if the null values are handled.\n",
    "null_values = tt_df_sub.isnull().sum()\n",
    "print(null_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38ddf643",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>talk_id</th>\n",
       "      <th>title</th>\n",
       "      <th>speaker</th>\n",
       "      <th>occupations</th>\n",
       "      <th>event</th>\n",
       "      <th>published_date</th>\n",
       "      <th>duration</th>\n",
       "      <th>topics</th>\n",
       "      <th>description</th>\n",
       "      <th>views</th>\n",
       "      <th>comments</th>\n",
       "      <th>transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2006</td>\n",
       "      <td>1</td>\n",
       "      <td>Averting the climate crisis</td>\n",
       "      <td>Al Gore</td>\n",
       "      <td>climate advocate</td>\n",
       "      <td>TED2006</td>\n",
       "      <td>2006-06-27</td>\n",
       "      <td>16.28</td>\n",
       "      <td>alternative energy cars climate change culture...</td>\n",
       "      <td>With the same humor and humanity he exuded in ...</td>\n",
       "      <td>3523392</td>\n",
       "      <td>272</td>\n",
       "      <td>Thank you so much, Chris. And it's truly a gre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2006</td>\n",
       "      <td>92</td>\n",
       "      <td>The best stats you've ever seen</td>\n",
       "      <td>Hans Rosling</td>\n",
       "      <td>global health expert data visionary</td>\n",
       "      <td>TED2006</td>\n",
       "      <td>2006-06-27</td>\n",
       "      <td>19.83</td>\n",
       "      <td>Africa Asia Google demo economics global issue...</td>\n",
       "      <td>You've never seen data presented like this. Wi...</td>\n",
       "      <td>14501685</td>\n",
       "      <td>628</td>\n",
       "      <td>About 10 years ago, I took on the task to teac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2006</td>\n",
       "      <td>7</td>\n",
       "      <td>Simplicity sells</td>\n",
       "      <td>David Pogue</td>\n",
       "      <td>technology columnist</td>\n",
       "      <td>TED2006</td>\n",
       "      <td>2006-06-27</td>\n",
       "      <td>21.43</td>\n",
       "      <td>computers entertainment interface design media...</td>\n",
       "      <td>New York Times columnist David Pogue takes aim...</td>\n",
       "      <td>1920832</td>\n",
       "      <td>124</td>\n",
       "      <td>(Music: \"The Sound of Silence,\" Simon &amp; Garfun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2006</td>\n",
       "      <td>53</td>\n",
       "      <td>Greening the ghetto</td>\n",
       "      <td>Majora Carter</td>\n",
       "      <td>activist for environmental justice</td>\n",
       "      <td>TED2006</td>\n",
       "      <td>2006-06-27</td>\n",
       "      <td>18.60</td>\n",
       "      <td>MacArthur grant activism business cities envir...</td>\n",
       "      <td>In an emotionally charged talk, MacArthur-winn...</td>\n",
       "      <td>2664069</td>\n",
       "      <td>219</td>\n",
       "      <td>If you're here today — and I'm very happy that...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2006</td>\n",
       "      <td>66</td>\n",
       "      <td>Do schools kill creativity?</td>\n",
       "      <td>Sir Ken Robinson</td>\n",
       "      <td>author educator</td>\n",
       "      <td>TED2006</td>\n",
       "      <td>2006-06-27</td>\n",
       "      <td>19.40</td>\n",
       "      <td>children creativity culture dance education pa...</td>\n",
       "      <td>Sir Ken Robinson makes an entertaining and pro...</td>\n",
       "      <td>65051954</td>\n",
       "      <td>4931</td>\n",
       "      <td>Good morning. How are you? (Audience) Good. It...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year  talk_id                            title           speaker  \\\n",
       "0  2006        1      Averting the climate crisis           Al Gore   \n",
       "1  2006       92  The best stats you've ever seen      Hans Rosling   \n",
       "2  2006        7                 Simplicity sells       David Pogue   \n",
       "3  2006       53              Greening the ghetto     Majora Carter   \n",
       "4  2006       66      Do schools kill creativity?  Sir Ken Robinson   \n",
       "\n",
       "                            occupations    event published_date  duration  \\\n",
       "0                      climate advocate  TED2006     2006-06-27     16.28   \n",
       "1   global health expert data visionary  TED2006     2006-06-27     19.83   \n",
       "2                  technology columnist  TED2006     2006-06-27     21.43   \n",
       "3    activist for environmental justice  TED2006     2006-06-27     18.60   \n",
       "4                       author educator  TED2006     2006-06-27     19.40   \n",
       "\n",
       "                                              topics  \\\n",
       "0  alternative energy cars climate change culture...   \n",
       "1  Africa Asia Google demo economics global issue...   \n",
       "2  computers entertainment interface design media...   \n",
       "3  MacArthur grant activism business cities envir...   \n",
       "4  children creativity culture dance education pa...   \n",
       "\n",
       "                                         description     views  comments  \\\n",
       "0  With the same humor and humanity he exuded in ...   3523392       272   \n",
       "1  You've never seen data presented like this. Wi...  14501685       628   \n",
       "2  New York Times columnist David Pogue takes aim...   1920832       124   \n",
       "3  In an emotionally charged talk, MacArthur-winn...   2664069       219   \n",
       "4  Sir Ken Robinson makes an entertaining and pro...  65051954      4931   \n",
       "\n",
       "                                          transcript  \n",
       "0  Thank you so much, Chris. And it's truly a gre...  \n",
       "1  About 10 years ago, I took on the task to teac...  \n",
       "2  (Music: \"The Sound of Silence,\" Simon & Garfun...  \n",
       "3  If you're here today — and I'm very happy that...  \n",
       "4  Good morning. How are you? (Audience) Good. It...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display first five rows of the cleaned dataframe.\n",
    "tt_df_sub.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361c8f84",
   "metadata": {},
   "source": [
    "#### Advanced Text Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24a0d5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# More Pre-processing.\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Convert text to lowercase.\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove text in square brackets & parenthesis.\n",
    "    text = re.sub(r'\\[.*?\\]', '', text)\n",
    "    text = re.sub(r'\\(.*?\\)', '', text)\n",
    "\n",
    "    # Remove punctuation.\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "    # Tokenize.\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove stopwords.\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "    # Lemmatize.\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply preprocessing to the dataframe columns.\n",
    "tt_df_sub['description'] = tt_df_sub['description'].apply(preprocess_text)\n",
    "tt_df_sub['transcript'] = tt_df_sub['transcript'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7c9b7a",
   "metadata": {},
   "source": [
    "### Feature Engineering "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c47186b",
   "metadata": {},
   "source": [
    "#### Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b813760d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to determine the word count of description and transcript columns.\n",
    "def compute_word_counts(df, column_name):\n",
    "    return df[column_name].apply(lambda x: len(x.split()))\n",
    "\n",
    "def add_word_counts(df):\n",
    "    df['description_word_count'] = compute_word_counts(df, 'description')\n",
    "    df['transcript_word_count'] = compute_word_counts(df, 'transcript')\n",
    "    return df\n",
    "\n",
    "tt_df_sub = add_word_counts(tt_df_sub)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703f4dd4",
   "metadata": {},
   "source": [
    "#### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df30c597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute tf-idf.\n",
    "def compute_tfidf(df, column_name, max_features=5000):\n",
    "    vectorizer = TfidfVectorizer(max_df=0.50, max_features=max_features, stop_words='english')\n",
    "    tfidf_matrix = vectorizer.fit_transform(df[column_name])\n",
    "    return vectorizer, tfidf_matrix\n",
    "\n",
    "description_vectorizer, description_tfidf_matrix = compute_tfidf(tt_df_sub, 'description')\n",
    "transcript_vectorizer, transcript_tfidf_matrix = compute_tfidf(tt_df_sub, 'transcript')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2971b970",
   "metadata": {},
   "source": [
    "#### N-Grams "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e3c2f480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute n-grams.\n",
    "def compute_tfidf_with_ngrams(df, column_name, max_features=5000, ngram_range=(1,2)):\n",
    "    vectorizer = TfidfVectorizer(max_df=0.80, max_features=max_features, stop_words='english', ngram_range=ngram_range)\n",
    "    tfidf_matrix = vectorizer.fit_transform(df[column_name])\n",
    "    return vectorizer, tfidf_matrix\n",
    "\n",
    "vectorizer, tfidf_matrix = compute_tfidf_with_ngrams(tt_df_sub, 'transcript')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248f5734",
   "metadata": {},
   "source": [
    "#### Sentiment Scores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5d8f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute sentiment scores.\n",
    "def compute_sentiment(text):\n",
    "    return TextBlob(text).sentiment.polarity\n",
    "\n",
    "def add_sentiment_scores(df):\n",
    "    df['description_sentiment'] = df['description'].apply(compute_sentiment)\n",
    "    df['transcript_sentiment'] = df['transcript'].apply(compute_sentiment)\n",
    "    return df\n",
    "\n",
    "tt_df_sub = add_sentiment_scores(tt_df_sub)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e9b57f",
   "metadata": {},
   "source": [
    "#### Named Entity Recognition "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac08f5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to perform Named Entity Recognition.\n",
    "\n",
    "# Load the large English model.\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "def extract_named_entities(text):\n",
    "    # Defining the entity labels we are interested in\n",
    "    relevant_entities = ['PERSON', 'ORG', 'GPE', 'DATE', 'EVENT', 'PRODUCT', 'WORK_OF_ART', 'LAW']\n",
    "    doc = nlp(text)\n",
    "    return [ent.text for ent in doc.ents if ent.label_ in relevant_entities]\n",
    "\n",
    "def clean_entities(entities):\n",
    "    return [ent for ent in entities if len(ent) > 2 and ent.lower() not in STOP_WORDS]\n",
    "\n",
    "def add_named_entities(df, column_name):\n",
    "    new_column = f\"{column_name}_named_entities\"\n",
    "    tqdm.pandas(desc=f\"Processing {column_name}\")\n",
    "    df[new_column] = df[column_name].progress_apply(extract_named_entities)\n",
    "    df[new_column] = df[new_column].apply(clean_entities)\n",
    "    return df\n",
    "\n",
    "tt_df_sub = add_named_entities(tt_df_sub, 'description')\n",
    "tt_df_sub = add_named_entities(tt_df_sub, 'transcript')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9ffd80",
   "metadata": {},
   "source": [
    "#### Keyword Extraction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8c20bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform keyword extraction.\n",
    "def extract_keywords(text):\n",
    "    kw_extractor = yake.KeywordExtractor()\n",
    "    keywords = kw_extractor.extract_keywords(text)\n",
    "    return [kw[0] for kw in keywords]\n",
    "\n",
    "# Apply the keyword extraction function to the 'transcript' column with tqdm progress.\n",
    "tt_df_sub['transcript_keywords'] = tqdm(tt_df_sub['transcript'].apply(extract_keywords), desc='Extracting Keywords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1951b61e",
   "metadata": {},
   "source": [
    "#### Document Embedding using BERT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23221c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained BERT model and tokenizer.\n",
    "#model_name = \"bert-base-uncased\"\n",
    "#tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "#model = BertModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9e9d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform BERT embedding.\n",
    "#def get_bert_embedding(text):\n",
    "    # Tokenize input text\n",
    "    #inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "    \n",
    "    # Get BERT embeddings.\n",
    "    #with torch.no_grad():\n",
    "        #outputs = model(**inputs)\n",
    "    \n",
    "    # Use the embedding of the [CLS] token as representation.\n",
    "    #embedding = outputs.last_hidden_state[:, 0, :].numpy()\n",
    "\n",
    "    #return embedding\n",
    "\n",
    "# Apply the function and see progress with tqdm.\n",
    "#tt_df_sub['bert_embedding'] = [get_bert_embedding(x) for x in tqdm(tt_df_sub['transcript'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179ed147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert DataFrame to CSV and Json for verification.\n",
    "tt_df_sub.to_csv('tt_df_sub.csv', index=False)\n",
    "#tt_df_sub.to_json('tt_df_sub.json', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1067fdfd",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d5a69c",
   "metadata": {},
   "source": [
    "#### Descriptive Statistics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd83759",
   "metadata": {},
   "outputs": [],
   "source": [
    "tt_df_sub.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa5e20b",
   "metadata": {},
   "source": [
    "#### Word Count Distribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68606ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram to plot the distribution of description and transcript word count.\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(tt_df_sub['description_word_count'], kde=True, bins=50)\n",
    "plt.title('Distribution of Description Word Counts')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(tt_df_sub['transcript_word_count'], kde=True, bins=50)\n",
    "plt.title('Distribution of Transcript Word Counts')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64c319f",
   "metadata": {},
   "source": [
    "#### Sentiment Distribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6be640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram to plot the distribution of transcript sentiment.\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(tt_df_sub['transcript_sentiment'], kde=True, bins=50)\n",
    "plt.title('Distribution of Transcript Sentiment')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53321d1",
   "metadata": {},
   "source": [
    "#### Word Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b86f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word cloud for transcript.\n",
    "def generate_wordcloud(text):\n",
    "    wordcloud = WordCloud(background_color='white', width=800, height=800).generate(text)\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "generate_wordcloud(' '.join(tt_df_sub['transcript'].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0176bd",
   "metadata": {},
   "source": [
    "### Model Building "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48322d44",
   "metadata": {},
   "source": [
    "#### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "86585293",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "becec230fdc743708707ae4bf38acbd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Making bigrams:   0%|          | 0/4005 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8722c54af0304c3fbbe01624e040c197",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Making trigrams:   0%|          | 0/4005 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "830a6b8aaeb9457ab3c17726ef20ed05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating corpus:   0%|          | 0/4005 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-20 14:16:38,064 : INFO : using asymmetric alpha [0.15263604, 0.121313125, 0.10065699, 0.08601167, 0.07508676, 0.06662436, 0.05987621, 0.05436933, 0.04979008, 0.045922287, 0.04261209, 0.039747022, 0.037242956, 0.035035703, 0.03307544]\n",
      "2023-11-20 14:16:38,064 : INFO : using serial LDA version on this node\n",
      "2023-11-20 14:16:38,107 : INFO : running online LDA training, 15 topics, 30 passes over the supplied corpus of 4005 documents, updating every 6000 documents, evaluating every ~4005 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "2023-11-20 14:16:38,112 : INFO : training LDA model using 3 processes\n",
      "2023-11-20 14:16:41,238 : INFO : PROGRESS: pass 0, dispatched chunk #0 = documents up to #2000/4005, outstanding queue size 1\n",
      "2023-11-20 14:16:41,465 : INFO : PROGRESS: pass 0, dispatched chunk #1 = documents up to #4000/4005, outstanding queue size 2\n",
      "2023-11-20 14:16:41,466 : INFO : PROGRESS: pass 0, dispatched chunk #2 = documents up to #4005/4005, outstanding queue size 3\n",
      "2023-11-20 14:16:43,707 : INFO : topic #14 (0.033): 0.003*\"human\" + 0.003*\"light\" + 0.003*\"child\" + 0.003*\"system\" + 0.003*\"kid\" + 0.003*\"problem\" + 0.003*\"brain\" + 0.002*\"data\" + 0.002*\"ive\" + 0.002*\"technology\"\n",
      "2023-11-20 14:16:43,710 : INFO : topic #13 (0.035): 0.003*\"human\" + 0.003*\"system\" + 0.003*\"child\" + 0.003*\"together\" + 0.003*\"country\" + 0.003*\"great\" + 0.002*\"city\" + 0.002*\"weve\" + 0.002*\"talk\" + 0.002*\"able\"\n",
      "2023-11-20 14:16:43,712 : INFO : topic #2 (0.101): 0.003*\"brain\" + 0.003*\"woman\" + 0.003*\"city\" + 0.003*\"problem\" + 0.003*\"thought\" + 0.002*\"used\" + 0.002*\"question\" + 0.002*\"important\" + 0.002*\"country\" + 0.002*\"example\"\n",
      "2023-11-20 14:16:43,714 : INFO : topic #1 (0.121): 0.003*\"water\" + 0.003*\"weve\" + 0.003*\"show\" + 0.003*\"human\" + 0.003*\"system\" + 0.002*\"problem\" + 0.002*\"love\" + 0.002*\"body\" + 0.002*\"brain\" + 0.002*\"end\"\n",
      "2023-11-20 14:16:43,716 : INFO : topic #0 (0.153): 0.003*\"problem\" + 0.003*\"question\" + 0.002*\"great\" + 0.002*\"might\" + 0.002*\"body\" + 0.002*\"human\" + 0.002*\"city\" + 0.002*\"found\" + 0.002*\"woman\" + 0.002*\"country\"\n",
      "2023-11-20 14:16:43,718 : INFO : topic diff=1.152552, rho=1.000000\n",
      "2023-11-20 14:16:43,763 : INFO : -8.603 per-word bound, 388.9 perplexity estimate based on a held-out corpus of 5 documents with 2034 words\n",
      "2023-11-20 14:16:43,766 : INFO : PROGRESS: pass 1, dispatched chunk #0 = documents up to #2000/4005, outstanding queue size 1\n",
      "2023-11-20 14:16:44,103 : INFO : PROGRESS: pass 1, dispatched chunk #1 = documents up to #4000/4005, outstanding queue size 2\n",
      "2023-11-20 14:16:44,115 : INFO : PROGRESS: pass 1, dispatched chunk #2 = documents up to #4005/4005, outstanding queue size 3\n",
      "2023-11-20 14:16:46,187 : INFO : topic #14 (0.033): 0.006*\"light\" + 0.003*\"space\" + 0.003*\"human\" + 0.003*\"system\" + 0.003*\"data\" + 0.003*\"child\" + 0.003*\"planet\" + 0.003*\"technology\" + 0.003*\"brain\" + 0.003*\"kid\"\n",
      "2023-11-20 14:16:46,187 : INFO : topic #13 (0.035): 0.004*\"system\" + 0.003*\"human\" + 0.003*\"city\" + 0.003*\"together\" + 0.003*\"weve\" + 0.003*\"cell\" + 0.002*\"cancer\" + 0.002*\"able\" + 0.002*\"using\" + 0.002*\"information\"\n",
      "2023-11-20 14:16:46,187 : INFO : topic #2 (0.101): 0.007*\"brain\" + 0.004*\"woman\" + 0.003*\"city\" + 0.003*\"problem\" + 0.003*\"thought\" + 0.002*\"drug\" + 0.002*\"example\" + 0.002*\"cell\" + 0.002*\"important\" + 0.002*\"used\"\n",
      "2023-11-20 14:16:46,187 : INFO : topic #1 (0.121): 0.005*\"water\" + 0.003*\"show\" + 0.003*\"body\" + 0.003*\"weve\" + 0.003*\"human\" + 0.003*\"system\" + 0.002*\"love\" + 0.002*\"ive\" + 0.002*\"animal\" + 0.002*\"end\"\n",
      "2023-11-20 14:16:46,187 : INFO : topic #0 (0.153): 0.003*\"problem\" + 0.003*\"might\" + 0.002*\"question\" + 0.002*\"brain\" + 0.002*\"great\" + 0.002*\"body\" + 0.002*\"water\" + 0.002*\"city\" + 0.002*\"found\" + 0.002*\"human\"\n",
      "2023-11-20 14:16:46,203 : INFO : topic diff=0.211420, rho=0.499844\n",
      "2023-11-20 14:16:46,240 : INFO : -8.483 per-word bound, 357.8 perplexity estimate based on a held-out corpus of 5 documents with 2034 words\n",
      "2023-11-20 14:16:46,254 : INFO : PROGRESS: pass 2, dispatched chunk #0 = documents up to #2000/4005, outstanding queue size 1\n",
      "2023-11-20 14:16:46,524 : INFO : PROGRESS: pass 2, dispatched chunk #1 = documents up to #4000/4005, outstanding queue size 2\n",
      "2023-11-20 14:16:46,720 : INFO : PROGRESS: pass 2, dispatched chunk #2 = documents up to #4005/4005, outstanding queue size 3\n",
      "2023-11-20 14:16:51,373 : INFO : topic #14 (0.033): 0.008*\"light\" + 0.004*\"planet\" + 0.004*\"space\" + 0.004*\"earth\" + 0.003*\"star\" + 0.003*\"data\" + 0.003*\"human\" + 0.003*\"technology\" + 0.003*\"system\" + 0.003*\"able\"\n",
      "2023-11-20 14:16:51,375 : INFO : topic #13 (0.035): 0.004*\"system\" + 0.004*\"cancer\" + 0.004*\"cell\" + 0.003*\"human\" + 0.003*\"city\" + 0.003*\"together\" + 0.003*\"weve\" + 0.003*\"design\" + 0.003*\"using\" + 0.003*\"computer\"\n",
      "2023-11-20 14:16:51,378 : INFO : topic #2 (0.101): 0.012*\"brain\" + 0.004*\"woman\" + 0.004*\"drug\" + 0.003*\"patient\" + 0.003*\"body\" + 0.003*\"cell\" + 0.003*\"problem\" + 0.002*\"example\" + 0.002*\"thought\" + 0.002*\"human\"\n",
      "2023-11-20 14:16:51,380 : INFO : topic #1 (0.121): 0.006*\"water\" + 0.003*\"show\" + 0.003*\"body\" + 0.003*\"animal\" + 0.003*\"weve\" + 0.003*\"ive\" + 0.003*\"human\" + 0.003*\"ocean\" + 0.002*\"hand\" + 0.002*\"thought\"\n",
      "2023-11-20 14:16:51,382 : INFO : topic #0 (0.153): 0.003*\"problem\" + 0.003*\"might\" + 0.003*\"brain\" + 0.003*\"water\" + 0.002*\"body\" + 0.002*\"question\" + 0.002*\"great\" + 0.002*\"human\" + 0.002*\"found\" + 0.002*\"city\"\n",
      "2023-11-20 14:16:51,386 : INFO : topic diff=0.243093, rho=0.447102\n",
      "2023-11-20 14:16:51,433 : INFO : -8.395 per-word bound, 336.7 perplexity estimate based on a held-out corpus of 5 documents with 2034 words\n",
      "2023-11-20 14:16:51,435 : INFO : PROGRESS: pass 3, dispatched chunk #0 = documents up to #2000/4005, outstanding queue size 1\n",
      "2023-11-20 14:16:51,781 : INFO : PROGRESS: pass 3, dispatched chunk #1 = documents up to #4000/4005, outstanding queue size 2\n",
      "2023-11-20 14:16:52,069 : INFO : PROGRESS: pass 3, dispatched chunk #2 = documents up to #4005/4005, outstanding queue size 3\n",
      "2023-11-20 14:16:58,563 : INFO : topic #14 (0.033): 0.010*\"light\" + 0.006*\"planet\" + 0.005*\"earth\" + 0.005*\"space\" + 0.004*\"star\" + 0.004*\"robot\" + 0.003*\"technology\" + 0.003*\"data\" + 0.003*\"human\" + 0.003*\"system\"\n",
      "2023-11-20 14:16:58,578 : INFO : topic #13 (0.035): 0.005*\"cell\" + 0.005*\"cancer\" + 0.004*\"system\" + 0.004*\"human\" + 0.003*\"dna\" + 0.003*\"weve\" + 0.003*\"virus\" + 0.003*\"together\" + 0.003*\"material\" + 0.003*\"design\"\n",
      "2023-11-20 14:16:58,580 : INFO : topic #2 (0.101): 0.015*\"brain\" + 0.005*\"patient\" + 0.005*\"drug\" + 0.004*\"woman\" + 0.004*\"body\" + 0.003*\"disease\" + 0.003*\"cell\" + 0.003*\"human\" + 0.003*\"health\" + 0.003*\"problem\"\n",
      "2023-11-20 14:16:58,582 : INFO : topic #1 (0.121): 0.006*\"water\" + 0.003*\"animal\" + 0.003*\"show\" + 0.003*\"body\" + 0.003*\"ocean\" + 0.003*\"ive\" + 0.003*\"weve\" + 0.003*\"human\" + 0.002*\"hand\" + 0.002*\"thought\"\n",
      "2023-11-20 14:16:58,584 : INFO : topic #0 (0.153): 0.003*\"problem\" + 0.003*\"might\" + 0.003*\"water\" + 0.002*\"body\" + 0.002*\"human\" + 0.002*\"brain\" + 0.002*\"sleep\" + 0.002*\"question\" + 0.002*\"person\" + 0.002*\"found\"\n",
      "2023-11-20 14:16:58,587 : INFO : topic diff=0.238183, rho=0.408163\n",
      "2023-11-20 14:16:58,670 : INFO : -8.342 per-word bound, 324.4 perplexity estimate based on a held-out corpus of 5 documents with 2034 words\n",
      "2023-11-20 14:16:58,673 : INFO : PROGRESS: pass 4, dispatched chunk #0 = documents up to #2000/4005, outstanding queue size 1\n",
      "2023-11-20 14:16:59,053 : INFO : PROGRESS: pass 4, dispatched chunk #1 = documents up to #4000/4005, outstanding queue size 2\n",
      "2023-11-20 14:16:59,066 : INFO : PROGRESS: pass 4, dispatched chunk #2 = documents up to #4005/4005, outstanding queue size 3\n",
      "2023-11-20 14:17:04,337 : INFO : topic #14 (0.033): 0.011*\"light\" + 0.007*\"planet\" + 0.006*\"earth\" + 0.006*\"space\" + 0.005*\"star\" + 0.005*\"robot\" + 0.004*\"technology\" + 0.003*\"data\" + 0.003*\"human\" + 0.003*\"able\"\n",
      "2023-11-20 14:17:04,337 : INFO : topic #13 (0.035): 0.006*\"cell\" + 0.005*\"cancer\" + 0.004*\"system\" + 0.004*\"dna\" + 0.004*\"human\" + 0.004*\"material\" + 0.004*\"bacteria\" + 0.004*\"virus\" + 0.003*\"weve\" + 0.003*\"gene\"\n",
      "2023-11-20 14:17:04,337 : INFO : topic #2 (0.101): 0.017*\"brain\" + 0.006*\"patient\" + 0.005*\"drug\" + 0.004*\"body\" + 0.004*\"woman\" + 0.004*\"disease\" + 0.004*\"cell\" + 0.003*\"health\" + 0.003*\"human\" + 0.003*\"doctor\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-20 14:17:04,353 : INFO : topic #1 (0.121): 0.007*\"water\" + 0.004*\"animal\" + 0.004*\"ocean\" + 0.003*\"show\" + 0.003*\"body\" + 0.003*\"ive\" + 0.003*\"human\" + 0.003*\"hand\" + 0.003*\"weve\" + 0.002*\"sort\"\n",
      "2023-11-20 14:17:04,355 : INFO : topic #0 (0.153): 0.003*\"sleep\" + 0.003*\"problem\" + 0.003*\"might\" + 0.003*\"human\" + 0.003*\"memory\" + 0.003*\"person\" + 0.002*\"water\" + 0.002*\"body\" + 0.002*\"study\" + 0.002*\"found\"\n",
      "2023-11-20 14:17:04,356 : INFO : topic diff=0.240023, rho=0.377897\n",
      "2023-11-20 14:17:04,396 : INFO : -8.305 per-word bound, 316.3 perplexity estimate based on a held-out corpus of 5 documents with 2034 words\n",
      "2023-11-20 14:17:04,403 : INFO : PROGRESS: pass 5, dispatched chunk #0 = documents up to #2000/4005, outstanding queue size 1\n",
      "2023-11-20 14:17:04,638 : INFO : PROGRESS: pass 5, dispatched chunk #1 = documents up to #4000/4005, outstanding queue size 2\n",
      "2023-11-20 14:17:04,641 : INFO : PROGRESS: pass 5, dispatched chunk #2 = documents up to #4005/4005, outstanding queue size 3\n",
      "2023-11-20 14:17:09,932 : INFO : topic #14 (0.033): 0.012*\"light\" + 0.007*\"planet\" + 0.007*\"earth\" + 0.006*\"space\" + 0.006*\"robot\" + 0.005*\"star\" + 0.004*\"technology\" + 0.003*\"image\" + 0.003*\"data\" + 0.003*\"human\"\n",
      "2023-11-20 14:17:09,932 : INFO : topic #13 (0.035): 0.007*\"cell\" + 0.005*\"cancer\" + 0.004*\"dna\" + 0.004*\"system\" + 0.004*\"material\" + 0.004*\"bacteria\" + 0.004*\"human\" + 0.004*\"virus\" + 0.004*\"gene\" + 0.004*\"weve\"\n",
      "2023-11-20 14:17:09,932 : INFO : topic #2 (0.101): 0.017*\"brain\" + 0.007*\"patient\" + 0.006*\"drug\" + 0.005*\"body\" + 0.005*\"disease\" + 0.004*\"woman\" + 0.004*\"health\" + 0.004*\"cell\" + 0.004*\"doctor\" + 0.003*\"study\"\n",
      "2023-11-20 14:17:09,932 : INFO : topic #1 (0.121): 0.007*\"water\" + 0.004*\"animal\" + 0.004*\"ocean\" + 0.003*\"body\" + 0.003*\"show\" + 0.003*\"ive\" + 0.003*\"foot\" + 0.003*\"hand\" + 0.003*\"sort\" + 0.003*\"human\"\n",
      "2023-11-20 14:17:09,938 : INFO : topic #0 (0.153): 0.004*\"sleep\" + 0.003*\"human\" + 0.003*\"memory\" + 0.003*\"compassion\" + 0.003*\"problem\" + 0.003*\"might\" + 0.003*\"person\" + 0.002*\"study\" + 0.002*\"water\" + 0.002*\"body\"\n",
      "2023-11-20 14:17:09,940 : INFO : topic diff=0.248674, rho=0.353498\n",
      "2023-11-20 14:17:09,984 : INFO : -8.279 per-word bound, 310.5 perplexity estimate based on a held-out corpus of 5 documents with 2034 words\n",
      "2023-11-20 14:17:09,985 : INFO : PROGRESS: pass 6, dispatched chunk #0 = documents up to #2000/4005, outstanding queue size 1\n",
      "2023-11-20 14:17:10,226 : INFO : PROGRESS: pass 6, dispatched chunk #1 = documents up to #4000/4005, outstanding queue size 2\n",
      "2023-11-20 14:17:10,434 : INFO : PROGRESS: pass 6, dispatched chunk #2 = documents up to #4005/4005, outstanding queue size 3\n",
      "2023-11-20 14:17:15,681 : INFO : topic #14 (0.033): 0.012*\"light\" + 0.008*\"planet\" + 0.007*\"earth\" + 0.006*\"space\" + 0.006*\"robot\" + 0.005*\"star\" + 0.004*\"image\" + 0.004*\"technology\" + 0.004*\"sun\" + 0.003*\"able\"\n",
      "2023-11-20 14:17:15,681 : INFO : topic #13 (0.035): 0.008*\"cell\" + 0.006*\"cancer\" + 0.005*\"dna\" + 0.004*\"material\" + 0.004*\"bacteria\" + 0.004*\"system\" + 0.004*\"human\" + 0.004*\"virus\" + 0.004*\"gene\" + 0.004*\"plant\"\n",
      "2023-11-20 14:17:15,683 : INFO : topic #2 (0.101): 0.018*\"brain\" + 0.008*\"patient\" + 0.006*\"drug\" + 0.005*\"body\" + 0.005*\"disease\" + 0.004*\"health\" + 0.004*\"woman\" + 0.004*\"doctor\" + 0.004*\"cell\" + 0.004*\"study\"\n",
      "2023-11-20 14:17:15,685 : INFO : topic #1 (0.121): 0.008*\"water\" + 0.004*\"animal\" + 0.004*\"ocean\" + 0.003*\"body\" + 0.003*\"show\" + 0.003*\"ive\" + 0.003*\"foot\" + 0.003*\"sort\" + 0.003*\"hand\" + 0.003*\"big\"\n",
      "2023-11-20 14:17:15,686 : INFO : topic #0 (0.153): 0.004*\"sleep\" + 0.003*\"compassion\" + 0.003*\"human\" + 0.003*\"memory\" + 0.003*\"person\" + 0.003*\"might\" + 0.003*\"god\" + 0.003*\"problem\" + 0.002*\"study\" + 0.002*\"body\"\n",
      "2023-11-20 14:17:15,692 : INFO : topic diff=0.260491, rho=0.333287\n",
      "2023-11-20 14:17:15,738 : INFO : -8.259 per-word bound, 306.2 perplexity estimate based on a held-out corpus of 5 documents with 2034 words\n",
      "2023-11-20 14:17:15,741 : INFO : PROGRESS: pass 7, dispatched chunk #0 = documents up to #2000/4005, outstanding queue size 1\n",
      "2023-11-20 14:17:15,950 : INFO : PROGRESS: pass 7, dispatched chunk #1 = documents up to #4000/4005, outstanding queue size 2\n",
      "2023-11-20 14:17:15,951 : INFO : PROGRESS: pass 7, dispatched chunk #2 = documents up to #4005/4005, outstanding queue size 3\n",
      "2023-11-20 14:17:21,289 : INFO : topic #14 (0.033): 0.013*\"light\" + 0.008*\"planet\" + 0.008*\"earth\" + 0.007*\"robot\" + 0.007*\"space\" + 0.005*\"star\" + 0.004*\"image\" + 0.004*\"technology\" + 0.004*\"sun\" + 0.004*\"object\"\n",
      "2023-11-20 14:17:21,298 : INFO : topic #13 (0.035): 0.009*\"cell\" + 0.006*\"cancer\" + 0.005*\"dna\" + 0.005*\"bacteria\" + 0.005*\"material\" + 0.004*\"system\" + 0.004*\"gene\" + 0.004*\"human\" + 0.004*\"virus\" + 0.004*\"plant\"\n",
      "2023-11-20 14:17:21,301 : INFO : topic #2 (0.101): 0.018*\"brain\" + 0.008*\"patient\" + 0.006*\"drug\" + 0.006*\"body\" + 0.005*\"disease\" + 0.005*\"health\" + 0.004*\"doctor\" + 0.004*\"cell\" + 0.004*\"woman\" + 0.004*\"study\"\n",
      "2023-11-20 14:17:21,305 : INFO : topic #1 (0.121): 0.008*\"water\" + 0.005*\"animal\" + 0.004*\"ocean\" + 0.003*\"body\" + 0.003*\"show\" + 0.003*\"ive\" + 0.003*\"foot\" + 0.003*\"big\" + 0.003*\"sort\" + 0.003*\"hand\"\n",
      "2023-11-20 14:17:21,308 : INFO : topic #0 (0.153): 0.005*\"sleep\" + 0.004*\"compassion\" + 0.003*\"human\" + 0.003*\"god\" + 0.003*\"memory\" + 0.003*\"person\" + 0.003*\"might\" + 0.002*\"problem\" + 0.002*\"study\" + 0.002*\"may\"\n",
      "2023-11-20 14:17:21,310 : INFO : topic diff=0.272009, rho=0.316188\n",
      "2023-11-20 14:17:21,376 : INFO : -8.245 per-word bound, 303.5 perplexity estimate based on a held-out corpus of 5 documents with 2034 words\n",
      "2023-11-20 14:17:21,376 : INFO : PROGRESS: pass 8, dispatched chunk #0 = documents up to #2000/4005, outstanding queue size 1\n",
      "2023-11-20 14:17:21,376 : INFO : PROGRESS: pass 8, dispatched chunk #1 = documents up to #4000/4005, outstanding queue size 2\n",
      "2023-11-20 14:17:21,648 : INFO : PROGRESS: pass 8, dispatched chunk #2 = documents up to #4005/4005, outstanding queue size 3\n",
      "2023-11-20 14:17:26,756 : INFO : topic #14 (0.033): 0.013*\"light\" + 0.009*\"planet\" + 0.008*\"earth\" + 0.008*\"robot\" + 0.007*\"space\" + 0.005*\"star\" + 0.004*\"image\" + 0.004*\"sun\" + 0.004*\"object\" + 0.004*\"technology\"\n",
      "2023-11-20 14:17:26,758 : INFO : topic #13 (0.035): 0.010*\"cell\" + 0.006*\"cancer\" + 0.006*\"dna\" + 0.005*\"bacteria\" + 0.005*\"material\" + 0.004*\"gene\" + 0.004*\"plant\" + 0.004*\"system\" + 0.004*\"human\" + 0.004*\"virus\"\n",
      "2023-11-20 14:17:26,761 : INFO : topic #2 (0.101): 0.019*\"brain\" + 0.009*\"patient\" + 0.006*\"drug\" + 0.006*\"body\" + 0.006*\"disease\" + 0.005*\"health\" + 0.005*\"doctor\" + 0.004*\"cell\" + 0.004*\"study\" + 0.004*\"woman\"\n",
      "2023-11-20 14:17:26,763 : INFO : topic #1 (0.121): 0.009*\"water\" + 0.005*\"animal\" + 0.005*\"ocean\" + 0.003*\"body\" + 0.003*\"foot\" + 0.003*\"show\" + 0.003*\"ive\" + 0.003*\"big\" + 0.003*\"sort\" + 0.003*\"hand\"\n",
      "2023-11-20 14:17:26,765 : INFO : topic #0 (0.153): 0.005*\"sleep\" + 0.004*\"compassion\" + 0.004*\"god\" + 0.004*\"human\" + 0.003*\"memory\" + 0.003*\"person\" + 0.003*\"might\" + 0.003*\"death\" + 0.002*\"may\" + 0.002*\"study\"\n",
      "2023-11-20 14:17:26,767 : INFO : topic diff=0.280873, rho=0.301477\n",
      "2023-11-20 14:17:26,817 : INFO : -8.236 per-word bound, 301.6 perplexity estimate based on a held-out corpus of 5 documents with 2034 words\n",
      "2023-11-20 14:17:26,823 : INFO : PROGRESS: pass 9, dispatched chunk #0 = documents up to #2000/4005, outstanding queue size 1\n",
      "2023-11-20 14:17:27,234 : INFO : PROGRESS: pass 9, dispatched chunk #1 = documents up to #4000/4005, outstanding queue size 2\n",
      "2023-11-20 14:17:27,737 : INFO : PROGRESS: pass 9, dispatched chunk #2 = documents up to #4005/4005, outstanding queue size 3\n",
      "2023-11-20 14:17:35,324 : INFO : topic #14 (0.033): 0.013*\"light\" + 0.009*\"planet\" + 0.008*\"earth\" + 0.008*\"robot\" + 0.007*\"space\" + 0.006*\"star\" + 0.004*\"image\" + 0.004*\"object\" + 0.004*\"sun\" + 0.004*\"technology\"\n",
      "2023-11-20 14:17:35,324 : INFO : topic #13 (0.035): 0.011*\"cell\" + 0.006*\"cancer\" + 0.006*\"dna\" + 0.005*\"bacteria\" + 0.005*\"material\" + 0.005*\"gene\" + 0.004*\"plant\" + 0.004*\"human\" + 0.004*\"virus\" + 0.004*\"system\"\n",
      "2023-11-20 14:17:35,340 : INFO : topic #2 (0.101): 0.019*\"brain\" + 0.009*\"patient\" + 0.006*\"drug\" + 0.006*\"body\" + 0.006*\"disease\" + 0.005*\"health\" + 0.005*\"doctor\" + 0.004*\"cell\" + 0.004*\"study\" + 0.004*\"treatment\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-20 14:17:35,343 : INFO : topic #1 (0.121): 0.009*\"water\" + 0.005*\"animal\" + 0.005*\"ocean\" + 0.003*\"foot\" + 0.003*\"body\" + 0.003*\"ive\" + 0.003*\"show\" + 0.003*\"big\" + 0.003*\"sort\" + 0.003*\"hand\"\n",
      "2023-11-20 14:17:35,350 : INFO : topic #0 (0.153): 0.005*\"sleep\" + 0.004*\"god\" + 0.004*\"compassion\" + 0.004*\"human\" + 0.003*\"memory\" + 0.003*\"death\" + 0.003*\"person\" + 0.003*\"might\" + 0.002*\"may\" + 0.002*\"study\"\n",
      "2023-11-20 14:17:35,353 : INFO : topic diff=0.285718, rho=0.288645\n",
      "2023-11-20 14:17:35,400 : INFO : -8.229 per-word bound, 300.0 perplexity estimate based on a held-out corpus of 5 documents with 2034 words\n",
      "2023-11-20 14:17:35,401 : INFO : PROGRESS: pass 10, dispatched chunk #0 = documents up to #2000/4005, outstanding queue size 1\n",
      "2023-11-20 14:17:35,402 : INFO : PROGRESS: pass 10, dispatched chunk #1 = documents up to #4000/4005, outstanding queue size 2\n",
      "2023-11-20 14:17:35,711 : INFO : PROGRESS: pass 10, dispatched chunk #2 = documents up to #4005/4005, outstanding queue size 3\n",
      "2023-11-20 14:17:42,079 : INFO : topic #14 (0.033): 0.013*\"light\" + 0.009*\"planet\" + 0.009*\"earth\" + 0.008*\"robot\" + 0.007*\"space\" + 0.006*\"star\" + 0.004*\"image\" + 0.004*\"object\" + 0.004*\"sun\" + 0.004*\"mar\"\n",
      "2023-11-20 14:17:42,082 : INFO : topic #13 (0.035): 0.011*\"cell\" + 0.006*\"cancer\" + 0.006*\"dna\" + 0.005*\"bacteria\" + 0.005*\"material\" + 0.005*\"gene\" + 0.005*\"plant\" + 0.004*\"human\" + 0.004*\"virus\" + 0.004*\"protein\"\n",
      "2023-11-20 14:17:42,084 : INFO : topic #2 (0.101): 0.019*\"brain\" + 0.010*\"patient\" + 0.006*\"body\" + 0.006*\"drug\" + 0.006*\"disease\" + 0.006*\"health\" + 0.005*\"doctor\" + 0.004*\"cell\" + 0.004*\"study\" + 0.004*\"treatment\"\n",
      "2023-11-20 14:17:42,086 : INFO : topic #1 (0.121): 0.010*\"water\" + 0.005*\"animal\" + 0.005*\"ocean\" + 0.003*\"foot\" + 0.003*\"body\" + 0.003*\"ive\" + 0.003*\"show\" + 0.003*\"big\" + 0.003*\"fish\" + 0.003*\"sort\"\n",
      "2023-11-20 14:17:42,088 : INFO : topic #0 (0.153): 0.005*\"sleep\" + 0.005*\"god\" + 0.004*\"compassion\" + 0.004*\"human\" + 0.003*\"death\" + 0.003*\"memory\" + 0.003*\"person\" + 0.003*\"might\" + 0.002*\"may\" + 0.002*\"others\"\n",
      "2023-11-20 14:17:42,089 : INFO : topic diff=0.286166, rho=0.277323\n",
      "2023-11-20 14:17:42,128 : INFO : -8.223 per-word bound, 298.9 perplexity estimate based on a held-out corpus of 5 documents with 2034 words\n",
      "2023-11-20 14:17:42,133 : INFO : PROGRESS: pass 11, dispatched chunk #0 = documents up to #2000/4005, outstanding queue size 1\n",
      "2023-11-20 14:17:42,353 : INFO : PROGRESS: pass 11, dispatched chunk #1 = documents up to #4000/4005, outstanding queue size 2\n",
      "2023-11-20 14:17:42,361 : INFO : PROGRESS: pass 11, dispatched chunk #2 = documents up to #4005/4005, outstanding queue size 3\n",
      "2023-11-20 14:17:47,326 : INFO : topic #14 (0.033): 0.014*\"light\" + 0.009*\"planet\" + 0.009*\"earth\" + 0.009*\"robot\" + 0.008*\"space\" + 0.006*\"star\" + 0.005*\"image\" + 0.004*\"object\" + 0.004*\"sun\" + 0.004*\"mar\"\n",
      "2023-11-20 14:17:47,329 : INFO : topic #13 (0.035): 0.012*\"cell\" + 0.006*\"dna\" + 0.006*\"cancer\" + 0.005*\"bacteria\" + 0.005*\"material\" + 0.005*\"gene\" + 0.005*\"plant\" + 0.004*\"human\" + 0.004*\"virus\" + 0.004*\"protein\"\n",
      "2023-11-20 14:17:47,332 : INFO : topic #2 (0.101): 0.019*\"brain\" + 0.010*\"patient\" + 0.007*\"body\" + 0.006*\"disease\" + 0.006*\"drug\" + 0.006*\"health\" + 0.005*\"doctor\" + 0.004*\"cell\" + 0.004*\"study\" + 0.004*\"treatment\"\n",
      "2023-11-20 14:17:47,334 : INFO : topic #1 (0.121): 0.010*\"water\" + 0.006*\"animal\" + 0.005*\"ocean\" + 0.003*\"foot\" + 0.003*\"body\" + 0.003*\"ive\" + 0.003*\"show\" + 0.003*\"big\" + 0.003*\"fish\" + 0.003*\"sort\"\n",
      "2023-11-20 14:17:47,336 : INFO : topic #0 (0.153): 0.005*\"god\" + 0.005*\"sleep\" + 0.004*\"compassion\" + 0.004*\"human\" + 0.003*\"death\" + 0.003*\"memory\" + 0.003*\"person\" + 0.003*\"might\" + 0.002*\"others\" + 0.002*\"age\"\n",
      "2023-11-20 14:17:47,339 : INFO : topic diff=0.282592, rho=0.267237\n",
      "2023-11-20 14:17:47,387 : INFO : -8.219 per-word bound, 298.1 perplexity estimate based on a held-out corpus of 5 documents with 2034 words\n",
      "2023-11-20 14:17:47,389 : INFO : PROGRESS: pass 12, dispatched chunk #0 = documents up to #2000/4005, outstanding queue size 1\n",
      "2023-11-20 14:17:47,621 : INFO : PROGRESS: pass 12, dispatched chunk #1 = documents up to #4000/4005, outstanding queue size 2\n",
      "2023-11-20 14:17:47,621 : INFO : PROGRESS: pass 12, dispatched chunk #2 = documents up to #4005/4005, outstanding queue size 3\n",
      "2023-11-20 14:17:53,872 : INFO : topic #14 (0.033): 0.014*\"light\" + 0.009*\"planet\" + 0.009*\"earth\" + 0.009*\"robot\" + 0.008*\"space\" + 0.006*\"star\" + 0.005*\"image\" + 0.004*\"object\" + 0.004*\"sun\" + 0.004*\"energy\"\n",
      "2023-11-20 14:17:53,887 : INFO : topic #13 (0.035): 0.013*\"cell\" + 0.006*\"dna\" + 0.006*\"cancer\" + 0.006*\"bacteria\" + 0.005*\"material\" + 0.005*\"gene\" + 0.005*\"plant\" + 0.004*\"human\" + 0.004*\"virus\" + 0.004*\"protein\"\n",
      "2023-11-20 14:17:53,896 : INFO : topic #2 (0.101): 0.019*\"brain\" + 0.010*\"patient\" + 0.007*\"body\" + 0.007*\"disease\" + 0.006*\"drug\" + 0.006*\"health\" + 0.005*\"doctor\" + 0.004*\"study\" + 0.004*\"cell\" + 0.004*\"treatment\"\n",
      "2023-11-20 14:17:53,899 : INFO : topic #1 (0.121): 0.010*\"water\" + 0.006*\"animal\" + 0.006*\"ocean\" + 0.003*\"foot\" + 0.003*\"body\" + 0.003*\"ive\" + 0.003*\"big\" + 0.003*\"show\" + 0.003*\"fish\" + 0.003*\"long\"\n",
      "2023-11-20 14:17:53,899 : INFO : topic #0 (0.153): 0.005*\"god\" + 0.005*\"sleep\" + 0.004*\"compassion\" + 0.004*\"human\" + 0.004*\"death\" + 0.003*\"memory\" + 0.003*\"person\" + 0.003*\"others\" + 0.003*\"might\" + 0.003*\"age\"\n",
      "2023-11-20 14:17:53,899 : INFO : topic diff=0.275693, rho=0.258177\n",
      "2023-11-20 14:17:53,950 : INFO : -8.216 per-word bound, 297.4 perplexity estimate based on a held-out corpus of 5 documents with 2034 words\n",
      "2023-11-20 14:17:53,954 : INFO : PROGRESS: pass 13, dispatched chunk #0 = documents up to #2000/4005, outstanding queue size 1\n",
      "2023-11-20 14:17:54,193 : INFO : PROGRESS: pass 13, dispatched chunk #1 = documents up to #4000/4005, outstanding queue size 2\n",
      "2023-11-20 14:17:54,193 : INFO : PROGRESS: pass 13, dispatched chunk #2 = documents up to #4005/4005, outstanding queue size 3\n",
      "2023-11-20 14:17:59,262 : INFO : topic #14 (0.033): 0.014*\"light\" + 0.009*\"planet\" + 0.009*\"robot\" + 0.009*\"earth\" + 0.008*\"space\" + 0.006*\"star\" + 0.005*\"image\" + 0.005*\"object\" + 0.004*\"sun\" + 0.004*\"energy\"\n",
      "2023-11-20 14:17:59,262 : INFO : topic #13 (0.035): 0.014*\"cell\" + 0.007*\"dna\" + 0.006*\"cancer\" + 0.006*\"bacteria\" + 0.005*\"material\" + 0.005*\"gene\" + 0.005*\"plant\" + 0.005*\"human\" + 0.005*\"virus\" + 0.004*\"molecule\"\n",
      "2023-11-20 14:17:59,272 : INFO : topic #2 (0.101): 0.019*\"brain\" + 0.010*\"patient\" + 0.007*\"disease\" + 0.007*\"body\" + 0.007*\"drug\" + 0.006*\"health\" + 0.005*\"doctor\" + 0.005*\"study\" + 0.004*\"cell\" + 0.004*\"treatment\"\n",
      "2023-11-20 14:17:59,282 : INFO : topic #1 (0.121): 0.010*\"water\" + 0.006*\"animal\" + 0.006*\"ocean\" + 0.004*\"foot\" + 0.003*\"body\" + 0.003*\"ive\" + 0.003*\"fish\" + 0.003*\"big\" + 0.003*\"show\" + 0.003*\"long\"\n",
      "2023-11-20 14:17:59,282 : INFO : topic #0 (0.153): 0.006*\"god\" + 0.005*\"sleep\" + 0.004*\"human\" + 0.004*\"compassion\" + 0.004*\"death\" + 0.003*\"person\" + 0.003*\"memory\" + 0.003*\"others\" + 0.003*\"age\" + 0.003*\"great\"\n",
      "2023-11-20 14:17:59,282 : INFO : topic diff=0.266307, rho=0.249980\n",
      "2023-11-20 14:17:59,341 : INFO : -8.215 per-word bound, 297.1 perplexity estimate based on a held-out corpus of 5 documents with 2034 words\n",
      "2023-11-20 14:17:59,344 : INFO : PROGRESS: pass 14, dispatched chunk #0 = documents up to #2000/4005, outstanding queue size 1\n",
      "2023-11-20 14:17:59,777 : INFO : PROGRESS: pass 14, dispatched chunk #1 = documents up to #4000/4005, outstanding queue size 2\n",
      "2023-11-20 14:17:59,793 : INFO : PROGRESS: pass 14, dispatched chunk #2 = documents up to #4005/4005, outstanding queue size 3\n",
      "2023-11-20 14:18:06,381 : INFO : topic #14 (0.033): 0.014*\"light\" + 0.010*\"planet\" + 0.010*\"robot\" + 0.009*\"earth\" + 0.008*\"space\" + 0.006*\"star\" + 0.005*\"object\" + 0.005*\"image\" + 0.004*\"sun\" + 0.004*\"universe\"\n",
      "2023-11-20 14:18:06,381 : INFO : topic #13 (0.035): 0.014*\"cell\" + 0.007*\"dna\" + 0.006*\"cancer\" + 0.006*\"bacteria\" + 0.006*\"material\" + 0.005*\"gene\" + 0.005*\"plant\" + 0.005*\"human\" + 0.005*\"molecule\" + 0.005*\"virus\"\n",
      "2023-11-20 14:18:06,387 : INFO : topic #2 (0.101): 0.019*\"brain\" + 0.010*\"patient\" + 0.007*\"disease\" + 0.007*\"body\" + 0.007*\"drug\" + 0.006*\"health\" + 0.005*\"doctor\" + 0.005*\"study\" + 0.004*\"treatment\" + 0.004*\"cell\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-20 14:18:06,389 : INFO : topic #1 (0.121): 0.011*\"water\" + 0.006*\"animal\" + 0.006*\"ocean\" + 0.004*\"foot\" + 0.003*\"fish\" + 0.003*\"ive\" + 0.003*\"body\" + 0.003*\"big\" + 0.003*\"show\" + 0.003*\"long\"\n",
      "2023-11-20 14:18:06,390 : INFO : topic #0 (0.153): 0.006*\"god\" + 0.005*\"sleep\" + 0.005*\"human\" + 0.004*\"compassion\" + 0.004*\"death\" + 0.003*\"others\" + 0.003*\"person\" + 0.003*\"age\" + 0.003*\"great\" + 0.003*\"memory\"\n",
      "2023-11-20 14:18:06,393 : INFO : topic diff=0.255231, rho=0.242518\n",
      "2023-11-20 14:18:06,438 : INFO : -8.212 per-word bound, 296.6 perplexity estimate based on a held-out corpus of 5 documents with 2034 words\n",
      "2023-11-20 14:18:06,441 : INFO : PROGRESS: pass 15, dispatched chunk #0 = documents up to #2000/4005, outstanding queue size 1\n",
      "2023-11-20 14:18:06,663 : INFO : PROGRESS: pass 15, dispatched chunk #1 = documents up to #4000/4005, outstanding queue size 2\n",
      "2023-11-20 14:18:06,668 : INFO : PROGRESS: pass 15, dispatched chunk #2 = documents up to #4005/4005, outstanding queue size 3\n",
      "2023-11-20 14:18:12,086 : INFO : topic #14 (0.033): 0.014*\"light\" + 0.010*\"robot\" + 0.010*\"planet\" + 0.010*\"earth\" + 0.008*\"space\" + 0.006*\"star\" + 0.005*\"object\" + 0.005*\"image\" + 0.004*\"sun\" + 0.004*\"universe\"\n",
      "2023-11-20 14:18:12,086 : INFO : topic #13 (0.035): 0.015*\"cell\" + 0.007*\"dna\" + 0.006*\"bacteria\" + 0.006*\"cancer\" + 0.006*\"material\" + 0.005*\"gene\" + 0.005*\"plant\" + 0.005*\"molecule\" + 0.005*\"human\" + 0.005*\"protein\"\n",
      "2023-11-20 14:18:12,095 : INFO : topic #2 (0.101): 0.020*\"brain\" + 0.011*\"patient\" + 0.007*\"disease\" + 0.007*\"body\" + 0.007*\"drug\" + 0.006*\"health\" + 0.006*\"doctor\" + 0.005*\"study\" + 0.004*\"treatment\" + 0.004*\"medical\"\n",
      "2023-11-20 14:18:12,097 : INFO : topic #1 (0.121): 0.011*\"water\" + 0.006*\"animal\" + 0.006*\"ocean\" + 0.004*\"foot\" + 0.003*\"fish\" + 0.003*\"ive\" + 0.003*\"body\" + 0.003*\"big\" + 0.003*\"long\" + 0.003*\"sea\"\n",
      "2023-11-20 14:18:12,101 : INFO : topic #0 (0.153): 0.006*\"god\" + 0.005*\"human\" + 0.005*\"sleep\" + 0.004*\"compassion\" + 0.004*\"death\" + 0.003*\"others\" + 0.003*\"great\" + 0.003*\"age\" + 0.003*\"person\" + 0.002*\"memory\"\n",
      "2023-11-20 14:18:12,103 : INFO : topic diff=0.243163, rho=0.235686\n",
      "2023-11-20 14:18:12,149 : INFO : -8.211 per-word bound, 296.2 perplexity estimate based on a held-out corpus of 5 documents with 2034 words\n",
      "2023-11-20 14:18:12,152 : INFO : PROGRESS: pass 16, dispatched chunk #0 = documents up to #2000/4005, outstanding queue size 1\n",
      "2023-11-20 14:18:12,471 : INFO : PROGRESS: pass 16, dispatched chunk #1 = documents up to #4000/4005, outstanding queue size 2\n",
      "2023-11-20 14:18:12,484 : INFO : PROGRESS: pass 16, dispatched chunk #2 = documents up to #4005/4005, outstanding queue size 3\n",
      "2023-11-20 14:18:17,702 : INFO : topic #14 (0.033): 0.014*\"light\" + 0.010*\"robot\" + 0.010*\"planet\" + 0.010*\"earth\" + 0.008*\"space\" + 0.006*\"star\" + 0.005*\"object\" + 0.005*\"image\" + 0.005*\"sun\" + 0.004*\"universe\"\n",
      "2023-11-20 14:18:17,702 : INFO : topic #13 (0.035): 0.015*\"cell\" + 0.007*\"dna\" + 0.006*\"bacteria\" + 0.006*\"cancer\" + 0.006*\"gene\" + 0.006*\"material\" + 0.005*\"plant\" + 0.005*\"molecule\" + 0.005*\"human\" + 0.005*\"protein\"\n",
      "2023-11-20 14:18:17,709 : INFO : topic #2 (0.101): 0.020*\"brain\" + 0.011*\"patient\" + 0.007*\"disease\" + 0.007*\"body\" + 0.007*\"drug\" + 0.006*\"health\" + 0.006*\"doctor\" + 0.005*\"study\" + 0.004*\"treatment\" + 0.004*\"medical\"\n",
      "2023-11-20 14:18:17,712 : INFO : topic #1 (0.121): 0.011*\"water\" + 0.006*\"animal\" + 0.006*\"ocean\" + 0.004*\"foot\" + 0.003*\"fish\" + 0.003*\"big\" + 0.003*\"ive\" + 0.003*\"body\" + 0.003*\"sea\" + 0.003*\"long\"\n",
      "2023-11-20 14:18:17,717 : INFO : topic #0 (0.153): 0.006*\"god\" + 0.005*\"human\" + 0.005*\"sleep\" + 0.004*\"compassion\" + 0.004*\"death\" + 0.003*\"others\" + 0.003*\"great\" + 0.003*\"age\" + 0.003*\"person\" + 0.002*\"may\"\n",
      "2023-11-20 14:18:17,717 : INFO : topic diff=0.230657, rho=0.229401\n",
      "2023-11-20 14:18:17,772 : INFO : -8.209 per-word bound, 295.9 perplexity estimate based on a held-out corpus of 5 documents with 2034 words\n",
      "2023-11-20 14:18:17,775 : INFO : PROGRESS: pass 17, dispatched chunk #0 = documents up to #2000/4005, outstanding queue size 1\n",
      "2023-11-20 14:18:17,987 : INFO : PROGRESS: pass 17, dispatched chunk #1 = documents up to #4000/4005, outstanding queue size 2\n",
      "2023-11-20 14:18:18,444 : INFO : PROGRESS: pass 17, dispatched chunk #2 = documents up to #4005/4005, outstanding queue size 3\n",
      "2023-11-20 14:18:23,313 : INFO : topic #14 (0.033): 0.015*\"light\" + 0.010*\"robot\" + 0.010*\"planet\" + 0.010*\"earth\" + 0.008*\"space\" + 0.006*\"star\" + 0.005*\"object\" + 0.005*\"image\" + 0.005*\"universe\" + 0.005*\"sun\"\n",
      "2023-11-20 14:18:23,331 : INFO : topic #13 (0.035): 0.016*\"cell\" + 0.007*\"dna\" + 0.006*\"bacteria\" + 0.006*\"cancer\" + 0.006*\"gene\" + 0.006*\"material\" + 0.005*\"plant\" + 0.005*\"molecule\" + 0.005*\"human\" + 0.005*\"protein\"\n",
      "2023-11-20 14:18:23,334 : INFO : topic #2 (0.101): 0.020*\"brain\" + 0.011*\"patient\" + 0.007*\"disease\" + 0.007*\"body\" + 0.007*\"drug\" + 0.007*\"health\" + 0.006*\"doctor\" + 0.005*\"study\" + 0.004*\"treatment\" + 0.004*\"medical\"\n",
      "2023-11-20 14:18:23,343 : INFO : topic #1 (0.121): 0.011*\"water\" + 0.007*\"animal\" + 0.006*\"ocean\" + 0.004*\"foot\" + 0.003*\"fish\" + 0.003*\"big\" + 0.003*\"ive\" + 0.003*\"body\" + 0.003*\"sea\" + 0.003*\"long\"\n",
      "2023-11-20 14:18:23,345 : INFO : topic #0 (0.153): 0.007*\"god\" + 0.005*\"human\" + 0.004*\"sleep\" + 0.004*\"death\" + 0.004*\"compassion\" + 0.003*\"others\" + 0.003*\"great\" + 0.003*\"age\" + 0.003*\"person\" + 0.003*\"history\"\n",
      "2023-11-20 14:18:23,348 : INFO : topic diff=0.218133, rho=0.223593\n",
      "2023-11-20 14:18:23,394 : INFO : -8.209 per-word bound, 295.8 perplexity estimate based on a held-out corpus of 5 documents with 2034 words\n",
      "2023-11-20 14:18:23,396 : INFO : PROGRESS: pass 18, dispatched chunk #0 = documents up to #2000/4005, outstanding queue size 1\n",
      "2023-11-20 14:18:23,668 : INFO : PROGRESS: pass 18, dispatched chunk #1 = documents up to #4000/4005, outstanding queue size 2\n",
      "2023-11-20 14:18:23,679 : INFO : PROGRESS: pass 18, dispatched chunk #2 = documents up to #4005/4005, outstanding queue size 3\n",
      "2023-11-20 14:18:28,734 : INFO : topic #14 (0.033): 0.015*\"light\" + 0.010*\"robot\" + 0.010*\"earth\" + 0.010*\"planet\" + 0.008*\"space\" + 0.006*\"star\" + 0.005*\"object\" + 0.005*\"image\" + 0.005*\"universe\" + 0.005*\"sun\"\n",
      "2023-11-20 14:18:28,734 : INFO : topic #13 (0.035): 0.016*\"cell\" + 0.007*\"dna\" + 0.006*\"bacteria\" + 0.006*\"gene\" + 0.006*\"cancer\" + 0.006*\"material\" + 0.005*\"plant\" + 0.005*\"molecule\" + 0.005*\"human\" + 0.005*\"protein\"\n",
      "2023-11-20 14:18:28,748 : INFO : topic #2 (0.101): 0.020*\"brain\" + 0.011*\"patient\" + 0.007*\"disease\" + 0.007*\"body\" + 0.007*\"drug\" + 0.007*\"health\" + 0.006*\"doctor\" + 0.005*\"study\" + 0.005*\"treatment\" + 0.004*\"medical\"\n",
      "2023-11-20 14:18:28,750 : INFO : topic #1 (0.121): 0.012*\"water\" + 0.007*\"animal\" + 0.006*\"ocean\" + 0.004*\"foot\" + 0.004*\"fish\" + 0.003*\"big\" + 0.003*\"sea\" + 0.003*\"ive\" + 0.003*\"body\" + 0.003*\"specie\"\n",
      "2023-11-20 14:18:28,754 : INFO : topic #0 (0.153): 0.007*\"god\" + 0.005*\"human\" + 0.004*\"death\" + 0.004*\"sleep\" + 0.004*\"compassion\" + 0.003*\"others\" + 0.003*\"great\" + 0.003*\"age\" + 0.003*\"history\" + 0.002*\"person\"\n",
      "2023-11-20 14:18:28,758 : INFO : topic diff=0.205853, rho=0.218205\n",
      "2023-11-20 14:18:28,822 : INFO : -8.206 per-word bound, 295.4 perplexity estimate based on a held-out corpus of 5 documents with 2034 words\n",
      "2023-11-20 14:18:28,827 : INFO : PROGRESS: pass 19, dispatched chunk #0 = documents up to #2000/4005, outstanding queue size 1\n",
      "2023-11-20 14:18:28,829 : INFO : PROGRESS: pass 19, dispatched chunk #1 = documents up to #4000/4005, outstanding queue size 2\n",
      "2023-11-20 14:18:28,830 : INFO : PROGRESS: pass 19, dispatched chunk #2 = documents up to #4005/4005, outstanding queue size 3\n",
      "2023-11-20 14:18:34,138 : INFO : topic #14 (0.033): 0.015*\"light\" + 0.010*\"robot\" + 0.010*\"earth\" + 0.010*\"planet\" + 0.009*\"space\" + 0.006*\"star\" + 0.005*\"object\" + 0.005*\"universe\" + 0.005*\"image\" + 0.005*\"sun\"\n",
      "2023-11-20 14:18:34,138 : INFO : topic #13 (0.035): 0.017*\"cell\" + 0.007*\"dna\" + 0.006*\"bacteria\" + 0.006*\"gene\" + 0.006*\"material\" + 0.006*\"cancer\" + 0.005*\"plant\" + 0.005*\"molecule\" + 0.005*\"human\" + 0.005*\"body\"\n",
      "2023-11-20 14:18:34,151 : INFO : topic #2 (0.101): 0.020*\"brain\" + 0.011*\"patient\" + 0.007*\"disease\" + 0.007*\"body\" + 0.007*\"health\" + 0.007*\"drug\" + 0.006*\"doctor\" + 0.005*\"study\" + 0.005*\"treatment\" + 0.004*\"medical\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-20 14:18:34,154 : INFO : topic #1 (0.121): 0.012*\"water\" + 0.007*\"animal\" + 0.007*\"ocean\" + 0.004*\"foot\" + 0.004*\"fish\" + 0.003*\"sea\" + 0.003*\"big\" + 0.003*\"ive\" + 0.003*\"specie\" + 0.003*\"body\"\n",
      "2023-11-20 14:18:34,154 : INFO : topic #0 (0.153): 0.007*\"god\" + 0.005*\"human\" + 0.004*\"death\" + 0.004*\"sleep\" + 0.004*\"compassion\" + 0.003*\"others\" + 0.003*\"great\" + 0.003*\"age\" + 0.003*\"history\" + 0.002*\"may\"\n",
      "2023-11-20 14:18:34,154 : INFO : topic diff=0.194044, rho=0.213189\n",
      "2023-11-20 14:18:34,206 : INFO : -8.205 per-word bound, 295.0 perplexity estimate based on a held-out corpus of 5 documents with 2034 words\n",
      "2023-11-20 14:18:34,208 : INFO : PROGRESS: pass 20, dispatched chunk #0 = documents up to #2000/4005, outstanding queue size 1\n",
      "2023-11-20 14:18:34,407 : INFO : PROGRESS: pass 20, dispatched chunk #1 = documents up to #4000/4005, outstanding queue size 2\n",
      "2023-11-20 14:18:34,419 : INFO : PROGRESS: pass 20, dispatched chunk #2 = documents up to #4005/4005, outstanding queue size 3\n",
      "2023-11-20 14:18:39,662 : INFO : topic #14 (0.033): 0.015*\"light\" + 0.010*\"robot\" + 0.010*\"earth\" + 0.010*\"planet\" + 0.009*\"space\" + 0.006*\"star\" + 0.005*\"object\" + 0.005*\"universe\" + 0.005*\"image\" + 0.005*\"energy\"\n",
      "2023-11-20 14:18:39,662 : INFO : topic #13 (0.035): 0.017*\"cell\" + 0.007*\"dna\" + 0.006*\"bacteria\" + 0.006*\"gene\" + 0.006*\"material\" + 0.006*\"cancer\" + 0.005*\"plant\" + 0.005*\"molecule\" + 0.005*\"body\" + 0.005*\"human\"\n",
      "2023-11-20 14:18:39,668 : INFO : topic #2 (0.101): 0.020*\"brain\" + 0.011*\"patient\" + 0.008*\"disease\" + 0.007*\"body\" + 0.007*\"health\" + 0.007*\"drug\" + 0.006*\"doctor\" + 0.005*\"study\" + 0.005*\"treatment\" + 0.005*\"medical\"\n",
      "2023-11-20 14:18:39,670 : INFO : topic #1 (0.121): 0.012*\"water\" + 0.007*\"animal\" + 0.007*\"ocean\" + 0.004*\"foot\" + 0.004*\"fish\" + 0.003*\"sea\" + 0.003*\"big\" + 0.003*\"specie\" + 0.003*\"ive\" + 0.003*\"long\"\n",
      "2023-11-20 14:18:39,671 : INFO : topic #0 (0.153): 0.007*\"god\" + 0.005*\"human\" + 0.004*\"death\" + 0.004*\"compassion\" + 0.004*\"sleep\" + 0.003*\"others\" + 0.003*\"great\" + 0.003*\"history\" + 0.003*\"age\" + 0.002*\"may\"\n",
      "2023-11-20 14:18:39,671 : INFO : topic diff=0.182811, rho=0.208503\n",
      "2023-11-20 14:18:39,746 : INFO : -8.202 per-word bound, 294.6 perplexity estimate based on a held-out corpus of 5 documents with 2034 words\n",
      "2023-11-20 14:18:39,765 : INFO : PROGRESS: pass 21, dispatched chunk #0 = documents up to #2000/4005, outstanding queue size 1\n",
      "2023-11-20 14:18:40,139 : INFO : PROGRESS: pass 21, dispatched chunk #1 = documents up to #4000/4005, outstanding queue size 2\n",
      "2023-11-20 14:18:40,350 : INFO : PROGRESS: pass 21, dispatched chunk #2 = documents up to #4005/4005, outstanding queue size 3\n",
      "2023-11-20 14:18:46,152 : INFO : topic #14 (0.033): 0.015*\"light\" + 0.010*\"robot\" + 0.010*\"earth\" + 0.010*\"planet\" + 0.009*\"space\" + 0.006*\"star\" + 0.005*\"universe\" + 0.005*\"object\" + 0.005*\"image\" + 0.005*\"energy\"\n",
      "2023-11-20 14:18:46,152 : INFO : topic #13 (0.035): 0.017*\"cell\" + 0.007*\"dna\" + 0.006*\"bacteria\" + 0.006*\"gene\" + 0.006*\"material\" + 0.006*\"cancer\" + 0.005*\"plant\" + 0.005*\"molecule\" + 0.005*\"body\" + 0.005*\"human\"\n",
      "2023-11-20 14:18:46,156 : INFO : topic #2 (0.101): 0.020*\"brain\" + 0.011*\"patient\" + 0.008*\"disease\" + 0.007*\"body\" + 0.007*\"health\" + 0.007*\"drug\" + 0.006*\"doctor\" + 0.005*\"study\" + 0.005*\"treatment\" + 0.005*\"medical\"\n",
      "2023-11-20 14:18:46,156 : INFO : topic #1 (0.121): 0.012*\"water\" + 0.007*\"animal\" + 0.007*\"ocean\" + 0.004*\"foot\" + 0.004*\"fish\" + 0.003*\"sea\" + 0.003*\"specie\" + 0.003*\"big\" + 0.003*\"long\" + 0.003*\"ive\"\n",
      "2023-11-20 14:18:46,156 : INFO : topic #0 (0.153): 0.007*\"god\" + 0.005*\"human\" + 0.005*\"death\" + 0.004*\"compassion\" + 0.004*\"sleep\" + 0.003*\"others\" + 0.003*\"great\" + 0.003*\"history\" + 0.003*\"age\" + 0.002*\"old\"\n",
      "2023-11-20 14:18:46,156 : INFO : topic diff=0.172239, rho=0.204114\n",
      "2023-11-20 14:18:46,215 : INFO : -8.201 per-word bound, 294.2 perplexity estimate based on a held-out corpus of 5 documents with 2034 words\n",
      "2023-11-20 14:18:46,218 : INFO : PROGRESS: pass 22, dispatched chunk #0 = documents up to #2000/4005, outstanding queue size 1\n",
      "2023-11-20 14:18:46,538 : INFO : PROGRESS: pass 22, dispatched chunk #1 = documents up to #4000/4005, outstanding queue size 2\n",
      "2023-11-20 14:18:46,811 : INFO : PROGRESS: pass 22, dispatched chunk #2 = documents up to #4005/4005, outstanding queue size 3\n",
      "2023-11-20 14:18:51,233 : INFO : topic #14 (0.033): 0.015*\"light\" + 0.011*\"robot\" + 0.010*\"earth\" + 0.010*\"planet\" + 0.009*\"space\" + 0.006*\"star\" + 0.005*\"universe\" + 0.005*\"object\" + 0.005*\"energy\" + 0.005*\"image\"\n",
      "2023-11-20 14:18:51,250 : INFO : topic #13 (0.035): 0.018*\"cell\" + 0.008*\"dna\" + 0.006*\"bacteria\" + 0.006*\"gene\" + 0.006*\"material\" + 0.005*\"cancer\" + 0.005*\"plant\" + 0.005*\"molecule\" + 0.005*\"body\" + 0.005*\"human\"\n",
      "2023-11-20 14:18:51,253 : INFO : topic #2 (0.101): 0.020*\"brain\" + 0.011*\"patient\" + 0.008*\"disease\" + 0.007*\"body\" + 0.007*\"health\" + 0.007*\"drug\" + 0.006*\"doctor\" + 0.005*\"study\" + 0.005*\"treatment\" + 0.005*\"medical\"\n",
      "2023-11-20 14:18:51,257 : INFO : topic #1 (0.121): 0.012*\"water\" + 0.007*\"animal\" + 0.007*\"ocean\" + 0.004*\"fish\" + 0.004*\"foot\" + 0.003*\"sea\" + 0.003*\"specie\" + 0.003*\"big\" + 0.003*\"long\" + 0.003*\"ive\"\n",
      "2023-11-20 14:18:51,260 : INFO : topic #0 (0.153): 0.007*\"god\" + 0.005*\"human\" + 0.005*\"death\" + 0.004*\"compassion\" + 0.004*\"sleep\" + 0.003*\"others\" + 0.003*\"great\" + 0.003*\"history\" + 0.003*\"age\" + 0.003*\"culture\"\n",
      "2023-11-20 14:18:51,262 : INFO : topic diff=0.162334, rho=0.199990\n",
      "2023-11-20 14:18:51,306 : INFO : -8.200 per-word bound, 294.0 perplexity estimate based on a held-out corpus of 5 documents with 2034 words\n",
      "2023-11-20 14:18:51,308 : INFO : PROGRESS: pass 23, dispatched chunk #0 = documents up to #2000/4005, outstanding queue size 1\n",
      "2023-11-20 14:18:51,494 : INFO : PROGRESS: pass 23, dispatched chunk #1 = documents up to #4000/4005, outstanding queue size 2\n",
      "2023-11-20 14:18:51,507 : INFO : PROGRESS: pass 23, dispatched chunk #2 = documents up to #4005/4005, outstanding queue size 3\n",
      "2023-11-20 14:18:56,469 : INFO : topic #14 (0.033): 0.015*\"light\" + 0.011*\"robot\" + 0.010*\"earth\" + 0.010*\"planet\" + 0.009*\"space\" + 0.006*\"star\" + 0.005*\"universe\" + 0.005*\"object\" + 0.005*\"energy\" + 0.005*\"image\"\n",
      "2023-11-20 14:18:56,472 : INFO : topic #13 (0.035): 0.018*\"cell\" + 0.008*\"dna\" + 0.006*\"gene\" + 0.006*\"bacteria\" + 0.006*\"material\" + 0.005*\"plant\" + 0.005*\"cancer\" + 0.005*\"molecule\" + 0.005*\"body\" + 0.005*\"human\"\n",
      "2023-11-20 14:18:56,474 : INFO : topic #2 (0.101): 0.020*\"brain\" + 0.011*\"patient\" + 0.008*\"disease\" + 0.007*\"body\" + 0.007*\"health\" + 0.007*\"drug\" + 0.006*\"doctor\" + 0.005*\"study\" + 0.005*\"treatment\" + 0.005*\"medical\"\n",
      "2023-11-20 14:18:56,476 : INFO : topic #1 (0.121): 0.012*\"water\" + 0.007*\"animal\" + 0.007*\"ocean\" + 0.004*\"fish\" + 0.004*\"foot\" + 0.003*\"sea\" + 0.003*\"specie\" + 0.003*\"big\" + 0.003*\"long\" + 0.003*\"ive\"\n",
      "2023-11-20 14:18:56,478 : INFO : topic #0 (0.153): 0.007*\"god\" + 0.005*\"human\" + 0.005*\"death\" + 0.004*\"compassion\" + 0.004*\"sleep\" + 0.003*\"great\" + 0.003*\"others\" + 0.003*\"history\" + 0.003*\"age\" + 0.003*\"culture\"\n",
      "2023-11-20 14:18:56,483 : INFO : topic diff=0.153121, rho=0.196107\n",
      "2023-11-20 14:18:56,528 : INFO : -8.198 per-word bound, 293.7 perplexity estimate based on a held-out corpus of 5 documents with 2034 words\n",
      "2023-11-20 14:18:56,532 : INFO : PROGRESS: pass 24, dispatched chunk #0 = documents up to #2000/4005, outstanding queue size 1\n",
      "2023-11-20 14:18:56,758 : INFO : PROGRESS: pass 24, dispatched chunk #1 = documents up to #4000/4005, outstanding queue size 2\n",
      "2023-11-20 14:18:56,761 : INFO : PROGRESS: pass 24, dispatched chunk #2 = documents up to #4005/4005, outstanding queue size 3\n",
      "2023-11-20 14:19:02,146 : INFO : topic #14 (0.033): 0.015*\"light\" + 0.011*\"robot\" + 0.010*\"earth\" + 0.010*\"planet\" + 0.009*\"space\" + 0.006*\"star\" + 0.005*\"universe\" + 0.005*\"object\" + 0.005*\"energy\" + 0.005*\"image\"\n",
      "2023-11-20 14:19:02,157 : INFO : topic #13 (0.035): 0.018*\"cell\" + 0.008*\"dna\" + 0.006*\"gene\" + 0.006*\"bacteria\" + 0.006*\"material\" + 0.006*\"plant\" + 0.005*\"molecule\" + 0.005*\"body\" + 0.005*\"cancer\" + 0.005*\"human\"\n",
      "2023-11-20 14:19:02,160 : INFO : topic #2 (0.101): 0.020*\"brain\" + 0.011*\"patient\" + 0.008*\"disease\" + 0.007*\"body\" + 0.007*\"health\" + 0.007*\"drug\" + 0.006*\"doctor\" + 0.005*\"study\" + 0.005*\"treatment\" + 0.005*\"medical\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-20 14:19:02,160 : INFO : topic #1 (0.121): 0.012*\"water\" + 0.008*\"animal\" + 0.007*\"ocean\" + 0.004*\"fish\" + 0.004*\"foot\" + 0.004*\"sea\" + 0.003*\"specie\" + 0.003*\"big\" + 0.003*\"long\" + 0.003*\"earth\"\n",
      "2023-11-20 14:19:02,160 : INFO : topic #0 (0.153): 0.007*\"god\" + 0.005*\"human\" + 0.005*\"death\" + 0.004*\"compassion\" + 0.004*\"sleep\" + 0.003*\"history\" + 0.003*\"great\" + 0.003*\"others\" + 0.003*\"age\" + 0.003*\"culture\"\n",
      "2023-11-20 14:19:02,160 : INFO : topic diff=0.144567, rho=0.192441\n",
      "2023-11-20 14:19:02,215 : INFO : -8.197 per-word bound, 293.4 perplexity estimate based on a held-out corpus of 5 documents with 2034 words\n",
      "2023-11-20 14:19:02,218 : INFO : PROGRESS: pass 25, dispatched chunk #0 = documents up to #2000/4005, outstanding queue size 1\n",
      "2023-11-20 14:19:02,511 : INFO : PROGRESS: pass 25, dispatched chunk #1 = documents up to #4000/4005, outstanding queue size 2\n",
      "2023-11-20 14:19:02,900 : INFO : PROGRESS: pass 25, dispatched chunk #2 = documents up to #4005/4005, outstanding queue size 3\n",
      "2023-11-20 14:19:07,689 : INFO : topic #14 (0.033): 0.015*\"light\" + 0.011*\"robot\" + 0.010*\"earth\" + 0.010*\"planet\" + 0.009*\"space\" + 0.006*\"star\" + 0.005*\"universe\" + 0.005*\"object\" + 0.005*\"energy\" + 0.005*\"sun\"\n",
      "2023-11-20 14:19:07,693 : INFO : topic #13 (0.035): 0.019*\"cell\" + 0.008*\"dna\" + 0.006*\"gene\" + 0.006*\"bacteria\" + 0.006*\"material\" + 0.006*\"plant\" + 0.006*\"body\" + 0.006*\"molecule\" + 0.005*\"cancer\" + 0.005*\"human\"\n",
      "2023-11-20 14:19:07,694 : INFO : topic #2 (0.101): 0.020*\"brain\" + 0.012*\"patient\" + 0.008*\"disease\" + 0.007*\"health\" + 0.007*\"body\" + 0.007*\"drug\" + 0.006*\"doctor\" + 0.005*\"study\" + 0.005*\"treatment\" + 0.005*\"medical\"\n",
      "2023-11-20 14:19:07,697 : INFO : topic #1 (0.121): 0.012*\"water\" + 0.008*\"animal\" + 0.007*\"ocean\" + 0.004*\"fish\" + 0.004*\"foot\" + 0.004*\"sea\" + 0.003*\"specie\" + 0.003*\"big\" + 0.003*\"long\" + 0.003*\"earth\"\n",
      "2023-11-20 14:19:07,700 : INFO : topic #0 (0.153): 0.007*\"god\" + 0.005*\"human\" + 0.005*\"death\" + 0.004*\"compassion\" + 0.004*\"sleep\" + 0.003*\"history\" + 0.003*\"great\" + 0.003*\"others\" + 0.003*\"age\" + 0.003*\"culture\"\n",
      "2023-11-20 14:19:07,700 : INFO : topic diff=0.136636, rho=0.188974\n",
      "2023-11-20 14:19:07,746 : INFO : -8.195 per-word bound, 293.1 perplexity estimate based on a held-out corpus of 5 documents with 2034 words\n",
      "2023-11-20 14:19:07,747 : INFO : PROGRESS: pass 26, dispatched chunk #0 = documents up to #2000/4005, outstanding queue size 1\n",
      "2023-11-20 14:19:07,944 : INFO : PROGRESS: pass 26, dispatched chunk #1 = documents up to #4000/4005, outstanding queue size 2\n",
      "2023-11-20 14:19:07,944 : INFO : PROGRESS: pass 26, dispatched chunk #2 = documents up to #4005/4005, outstanding queue size 3\n",
      "2023-11-20 14:19:13,107 : INFO : topic #14 (0.033): 0.015*\"light\" + 0.011*\"robot\" + 0.011*\"earth\" + 0.010*\"planet\" + 0.009*\"space\" + 0.006*\"star\" + 0.006*\"universe\" + 0.005*\"object\" + 0.005*\"energy\" + 0.005*\"sun\"\n",
      "2023-11-20 14:19:13,107 : INFO : topic #13 (0.035): 0.019*\"cell\" + 0.008*\"dna\" + 0.007*\"gene\" + 0.006*\"bacteria\" + 0.006*\"material\" + 0.006*\"body\" + 0.006*\"molecule\" + 0.006*\"plant\" + 0.005*\"cancer\" + 0.005*\"human\"\n",
      "2023-11-20 14:19:13,117 : INFO : topic #2 (0.101): 0.020*\"brain\" + 0.012*\"patient\" + 0.008*\"disease\" + 0.007*\"health\" + 0.007*\"body\" + 0.007*\"drug\" + 0.006*\"doctor\" + 0.005*\"study\" + 0.005*\"treatment\" + 0.005*\"medical\"\n",
      "2023-11-20 14:19:13,119 : INFO : topic #1 (0.121): 0.012*\"water\" + 0.008*\"animal\" + 0.007*\"ocean\" + 0.004*\"fish\" + 0.004*\"foot\" + 0.004*\"sea\" + 0.004*\"specie\" + 0.003*\"big\" + 0.003*\"long\" + 0.003*\"earth\"\n",
      "2023-11-20 14:19:13,122 : INFO : topic #0 (0.153): 0.007*\"god\" + 0.006*\"human\" + 0.005*\"death\" + 0.004*\"compassion\" + 0.003*\"sleep\" + 0.003*\"history\" + 0.003*\"great\" + 0.003*\"others\" + 0.003*\"culture\" + 0.003*\"age\"\n",
      "2023-11-20 14:19:13,126 : INFO : topic diff=0.129298, rho=0.185687\n",
      "2023-11-20 14:19:13,172 : INFO : -8.194 per-word bound, 292.8 perplexity estimate based on a held-out corpus of 5 documents with 2034 words\n",
      "2023-11-20 14:19:13,174 : INFO : PROGRESS: pass 27, dispatched chunk #0 = documents up to #2000/4005, outstanding queue size 1\n",
      "2023-11-20 14:19:13,517 : INFO : PROGRESS: pass 27, dispatched chunk #1 = documents up to #4000/4005, outstanding queue size 2\n",
      "2023-11-20 14:19:13,519 : INFO : PROGRESS: pass 27, dispatched chunk #2 = documents up to #4005/4005, outstanding queue size 3\n",
      "2023-11-20 14:19:18,212 : INFO : topic #14 (0.033): 0.015*\"light\" + 0.011*\"robot\" + 0.011*\"earth\" + 0.010*\"planet\" + 0.009*\"space\" + 0.006*\"star\" + 0.006*\"universe\" + 0.005*\"object\" + 0.005*\"energy\" + 0.005*\"sun\"\n",
      "2023-11-20 14:19:18,220 : INFO : topic #13 (0.035): 0.019*\"cell\" + 0.008*\"dna\" + 0.007*\"gene\" + 0.006*\"bacteria\" + 0.006*\"body\" + 0.006*\"material\" + 0.006*\"molecule\" + 0.006*\"plant\" + 0.005*\"human\" + 0.005*\"cancer\"\n",
      "2023-11-20 14:19:18,222 : INFO : topic #2 (0.101): 0.020*\"brain\" + 0.012*\"patient\" + 0.008*\"disease\" + 0.007*\"health\" + 0.007*\"body\" + 0.007*\"drug\" + 0.006*\"doctor\" + 0.005*\"study\" + 0.005*\"treatment\" + 0.005*\"medical\"\n",
      "2023-11-20 14:19:18,225 : INFO : topic #1 (0.121): 0.012*\"water\" + 0.008*\"animal\" + 0.007*\"ocean\" + 0.004*\"fish\" + 0.004*\"foot\" + 0.004*\"sea\" + 0.004*\"specie\" + 0.003*\"big\" + 0.003*\"long\" + 0.003*\"earth\"\n",
      "2023-11-20 14:19:18,229 : INFO : topic #0 (0.153): 0.008*\"god\" + 0.006*\"human\" + 0.005*\"death\" + 0.004*\"compassion\" + 0.003*\"history\" + 0.003*\"sleep\" + 0.003*\"great\" + 0.003*\"others\" + 0.003*\"culture\" + 0.003*\"age\"\n",
      "2023-11-20 14:19:18,232 : INFO : topic diff=0.122514, rho=0.182567\n",
      "2023-11-20 14:19:18,280 : INFO : -8.193 per-word bound, 292.5 perplexity estimate based on a held-out corpus of 5 documents with 2034 words\n",
      "2023-11-20 14:19:18,283 : INFO : PROGRESS: pass 28, dispatched chunk #0 = documents up to #2000/4005, outstanding queue size 1\n",
      "2023-11-20 14:19:18,527 : INFO : PROGRESS: pass 28, dispatched chunk #1 = documents up to #4000/4005, outstanding queue size 2\n",
      "2023-11-20 14:19:18,722 : INFO : PROGRESS: pass 28, dispatched chunk #2 = documents up to #4005/4005, outstanding queue size 3\n",
      "2023-11-20 14:19:23,169 : INFO : topic #14 (0.033): 0.015*\"light\" + 0.011*\"robot\" + 0.011*\"earth\" + 0.010*\"planet\" + 0.009*\"space\" + 0.006*\"star\" + 0.006*\"universe\" + 0.005*\"object\" + 0.005*\"energy\" + 0.005*\"sun\"\n",
      "2023-11-20 14:19:23,169 : INFO : topic #13 (0.035): 0.019*\"cell\" + 0.008*\"dna\" + 0.007*\"gene\" + 0.006*\"bacteria\" + 0.006*\"body\" + 0.006*\"material\" + 0.006*\"molecule\" + 0.006*\"plant\" + 0.005*\"human\" + 0.005*\"cancer\"\n",
      "2023-11-20 14:19:23,185 : INFO : topic #2 (0.101): 0.020*\"brain\" + 0.012*\"patient\" + 0.008*\"disease\" + 0.007*\"health\" + 0.007*\"body\" + 0.007*\"drug\" + 0.006*\"doctor\" + 0.005*\"study\" + 0.005*\"treatment\" + 0.005*\"medical\"\n",
      "2023-11-20 14:19:23,188 : INFO : topic #1 (0.121): 0.013*\"water\" + 0.008*\"animal\" + 0.007*\"ocean\" + 0.005*\"fish\" + 0.004*\"foot\" + 0.004*\"sea\" + 0.004*\"specie\" + 0.003*\"big\" + 0.003*\"long\" + 0.003*\"earth\"\n",
      "2023-11-20 14:19:23,192 : INFO : topic #0 (0.153): 0.008*\"god\" + 0.006*\"human\" + 0.005*\"death\" + 0.004*\"compassion\" + 0.003*\"history\" + 0.003*\"great\" + 0.003*\"sleep\" + 0.003*\"others\" + 0.003*\"culture\" + 0.003*\"story\"\n",
      "2023-11-20 14:19:23,195 : INFO : topic diff=0.116228, rho=0.179598\n",
      "2023-11-20 14:19:23,240 : INFO : -8.191 per-word bound, 292.3 perplexity estimate based on a held-out corpus of 5 documents with 2034 words\n",
      "2023-11-20 14:19:23,245 : INFO : PROGRESS: pass 29, dispatched chunk #0 = documents up to #2000/4005, outstanding queue size 1\n",
      "2023-11-20 14:19:23,450 : INFO : PROGRESS: pass 29, dispatched chunk #1 = documents up to #4000/4005, outstanding queue size 2\n",
      "2023-11-20 14:19:23,450 : INFO : PROGRESS: pass 29, dispatched chunk #2 = documents up to #4005/4005, outstanding queue size 3\n",
      "2023-11-20 14:19:28,336 : INFO : topic #14 (0.033): 0.015*\"light\" + 0.011*\"robot\" + 0.011*\"earth\" + 0.010*\"planet\" + 0.009*\"space\" + 0.006*\"star\" + 0.006*\"universe\" + 0.005*\"energy\" + 0.005*\"object\" + 0.005*\"sun\"\n",
      "2023-11-20 14:19:28,336 : INFO : topic #13 (0.035): 0.019*\"cell\" + 0.008*\"dna\" + 0.007*\"gene\" + 0.006*\"bacteria\" + 0.006*\"body\" + 0.006*\"molecule\" + 0.006*\"material\" + 0.006*\"plant\" + 0.005*\"human\" + 0.005*\"virus\"\n",
      "2023-11-20 14:19:28,349 : INFO : topic #2 (0.101): 0.020*\"brain\" + 0.012*\"patient\" + 0.008*\"disease\" + 0.008*\"health\" + 0.007*\"body\" + 0.007*\"drug\" + 0.006*\"doctor\" + 0.005*\"study\" + 0.005*\"treatment\" + 0.005*\"medical\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-20 14:19:28,352 : INFO : topic #1 (0.121): 0.013*\"water\" + 0.008*\"animal\" + 0.007*\"ocean\" + 0.005*\"fish\" + 0.004*\"foot\" + 0.004*\"sea\" + 0.004*\"specie\" + 0.003*\"big\" + 0.003*\"long\" + 0.003*\"earth\"\n",
      "2023-11-20 14:19:28,353 : INFO : topic #0 (0.153): 0.008*\"god\" + 0.006*\"human\" + 0.005*\"death\" + 0.004*\"compassion\" + 0.004*\"history\" + 0.003*\"great\" + 0.003*\"others\" + 0.003*\"culture\" + 0.003*\"sleep\" + 0.003*\"story\"\n",
      "2023-11-20 14:19:28,353 : INFO : topic diff=0.110415, rho=0.176770\n",
      "2023-11-20 14:19:28,405 : INFO : -8.190 per-word bound, 292.0 perplexity estimate based on a held-out corpus of 5 documents with 2034 words\n",
      "2023-11-20 14:19:28,430 : INFO : LdaMulticore lifecycle event {'msg': 'trained LdaMulticore<num_terms=9529, num_topics=15, decay=0.5, chunksize=2000> in 170.33s', 'datetime': '2023-11-20T14:19:28.430389', 'gensim': '4.2.0', 'python': '3.9.13 (main, Aug 25 2022, 23:51:50) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'created'}\n",
      "2023-11-20 14:19:28,432 : INFO : topic #0 (0.153): 0.008*\"god\" + 0.006*\"human\" + 0.005*\"death\" + 0.004*\"compassion\" + 0.004*\"history\"\n",
      "2023-11-20 14:19:28,435 : INFO : topic #1 (0.121): 0.013*\"water\" + 0.008*\"animal\" + 0.007*\"ocean\" + 0.005*\"fish\" + 0.004*\"foot\"\n",
      "2023-11-20 14:19:28,438 : INFO : topic #2 (0.101): 0.020*\"brain\" + 0.012*\"patient\" + 0.008*\"disease\" + 0.008*\"health\" + 0.007*\"body\"\n",
      "2023-11-20 14:19:28,441 : INFO : topic #3 (0.086): 0.009*\"word\" + 0.008*\"feel\" + 0.008*\"language\" + 0.005*\"love\" + 0.005*\"person\"\n",
      "2023-11-20 14:19:28,442 : INFO : topic #4 (0.075): 0.012*\"human\" + 0.009*\"computer\" + 0.008*\"data\" + 0.008*\"technology\" + 0.007*\"machine\"\n",
      "2023-11-20 14:19:28,442 : INFO : topic #5 (0.067): 0.006*\"government\" + 0.005*\"question\" + 0.004*\"information\" + 0.004*\"group\" + 0.004*\"system\"\n",
      "2023-11-20 14:19:28,442 : INFO : topic #6 (0.060): 0.017*\"woman\" + 0.009*\"child\" + 0.008*\"family\" + 0.006*\"girl\" + 0.006*\"men\"\n",
      "2023-11-20 14:19:28,442 : INFO : topic #7 (0.054): 0.007*\"kid\" + 0.006*\"company\" + 0.005*\"data\" + 0.005*\"money\" + 0.005*\"guy\"\n",
      "2023-11-20 14:19:28,442 : INFO : topic #8 (0.050): 0.029*\"city\" + 0.015*\"building\" + 0.009*\"design\" + 0.008*\"space\" + 0.008*\"car\"\n",
      "2023-11-20 14:19:28,455 : INFO : topic #9 (0.046): 0.021*\"country\" + 0.007*\"africa\" + 0.006*\"global\" + 0.005*\"government\" + 0.005*\"china\"\n",
      "2023-11-20 14:19:28,457 : INFO : topic #10 (0.043): 0.012*\"food\" + 0.008*\"energy\" + 0.006*\"problem\" + 0.005*\"company\" + 0.005*\"system\"\n",
      "2023-11-20 14:19:28,459 : INFO : topic #11 (0.040): 0.013*\"story\" + 0.008*\"art\" + 0.007*\"book\" + 0.005*\"image\" + 0.005*\"started\"\n",
      "2023-11-20 14:19:28,459 : INFO : topic #12 (0.037): 0.026*\"music\" + 0.020*\"sound\" + 0.019*\"student\" + 0.014*\"school\" + 0.013*\"teacher\"\n",
      "2023-11-20 14:19:28,459 : INFO : topic #13 (0.035): 0.019*\"cell\" + 0.008*\"dna\" + 0.007*\"gene\" + 0.006*\"bacteria\" + 0.006*\"body\"\n",
      "2023-11-20 14:19:28,459 : INFO : topic #14 (0.033): 0.015*\"light\" + 0.011*\"robot\" + 0.011*\"earth\" + 0.010*\"planet\" + 0.009*\"space\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.008*\"god\" + 0.006*\"human\" + 0.005*\"death\" + 0.004*\"compassion\" + 0.004*\"history\"')\n",
      "(1, '0.013*\"water\" + 0.008*\"animal\" + 0.007*\"ocean\" + 0.005*\"fish\" + 0.004*\"foot\"')\n",
      "(2, '0.020*\"brain\" + 0.012*\"patient\" + 0.008*\"disease\" + 0.008*\"health\" + 0.007*\"body\"')\n",
      "(3, '0.009*\"word\" + 0.008*\"feel\" + 0.008*\"language\" + 0.005*\"love\" + 0.005*\"person\"')\n",
      "(4, '0.012*\"human\" + 0.009*\"computer\" + 0.008*\"data\" + 0.008*\"technology\" + 0.007*\"machine\"')\n",
      "(5, '0.006*\"government\" + 0.005*\"question\" + 0.004*\"information\" + 0.004*\"group\" + 0.004*\"system\"')\n",
      "(6, '0.017*\"woman\" + 0.009*\"child\" + 0.008*\"family\" + 0.006*\"girl\" + 0.006*\"men\"')\n",
      "(7, '0.007*\"kid\" + 0.006*\"company\" + 0.005*\"data\" + 0.005*\"money\" + 0.005*\"guy\"')\n",
      "(8, '0.029*\"city\" + 0.015*\"building\" + 0.009*\"design\" + 0.008*\"space\" + 0.008*\"car\"')\n",
      "(9, '0.021*\"country\" + 0.007*\"africa\" + 0.006*\"global\" + 0.005*\"government\" + 0.005*\"china\"')\n",
      "(10, '0.012*\"food\" + 0.008*\"energy\" + 0.006*\"problem\" + 0.005*\"company\" + 0.005*\"system\"')\n",
      "(11, '0.013*\"story\" + 0.008*\"art\" + 0.007*\"book\" + 0.005*\"image\" + 0.005*\"started\"')\n",
      "(12, '0.026*\"music\" + 0.020*\"sound\" + 0.019*\"student\" + 0.014*\"school\" + 0.013*\"teacher\"')\n",
      "(13, '0.019*\"cell\" + 0.008*\"dna\" + 0.007*\"gene\" + 0.006*\"bacteria\" + 0.006*\"body\"')\n",
      "(14, '0.015*\"light\" + 0.011*\"robot\" + 0.011*\"earth\" + 0.010*\"planet\" + 0.009*\"space\"')\n"
     ]
    }
   ],
   "source": [
    "# Split each transcript into words.\n",
    "texts = tt_df_sub['transcript'].apply(lambda x: x.split()).tolist()\n",
    "\n",
    "# Build the bigram and trigram models.\n",
    "bigram = Phrases(texts, min_count=5, threshold=100)  \n",
    "trigram = Phrases(bigram[texts], threshold=100)\n",
    "\n",
    "# Construct bigram and trigram.\n",
    "bigram_mod = Phraser(bigram)\n",
    "trigram_mod = Phraser(trigram)\n",
    "\n",
    "# Define functions for creating bigrams and trigrams with tqdm progress.\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in tqdm(texts, desc='Making bigrams')]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in tqdm(texts, desc='Making trigrams')]\n",
    "\n",
    "# Form bigrams and trigrams with tqdm progress.\n",
    "texts_with_bigrams = make_bigrams(texts)\n",
    "texts_with_trigrams = make_trigrams(texts_with_bigrams)\n",
    "\n",
    "# Recreate the dictionary to include bigrams and trigrams.\n",
    "dictionary = corpora.Dictionary(texts_with_trigrams)\n",
    "\n",
    "# Filter out extremes to remove tokens that appear too frequently or infrequently.\n",
    "dictionary.filter_extremes(no_below=20, no_above=0.5)\n",
    "\n",
    "# Convert the documents to a bag-of-words representation with tqdm progress.\n",
    "corpus = [dictionary.doc2bow(text) for text in tqdm(texts_with_trigrams, desc='Creating corpus')]\n",
    "\n",
    "# Set up logging for progress tracking.\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "# Train LDA model with progress updates and additional hyperparameters.\n",
    "NUM_TOPICS = 15\n",
    "ldamodel = gensim.models.LdaMulticore(\n",
    "    corpus,\n",
    "    num_topics=NUM_TOPICS,\n",
    "    id2word=dictionary,\n",
    "    passes=30,\n",
    "    workers=3,\n",
    "    alpha='asymmetric',  \n",
    "    eta='auto',        \n",
    "    random_state=100,   \n",
    "    per_word_topics=True\n",
    ")\n",
    "\n",
    "# Print topics.\n",
    "topics = ldamodel.print_topics(num_words=5)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6c8d81a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-20 14:19:46,506 : INFO : using ParallelWordOccurrenceAccumulator<processes=11, batch_size=64> to estimate probabilities from sliding windows\n",
      "2023-11-20 14:19:57,737 : INFO : 1 batches submitted to accumulate stats from 64 documents (83903 virtual)\n",
      "2023-11-20 14:19:57,747 : INFO : 2 batches submitted to accumulate stats from 128 documents (146504 virtual)\n",
      "2023-11-20 14:19:57,764 : INFO : 3 batches submitted to accumulate stats from 192 documents (213358 virtual)\n",
      "2023-11-20 14:19:57,787 : INFO : 4 batches submitted to accumulate stats from 256 documents (278533 virtual)\n",
      "2023-11-20 14:19:57,814 : INFO : 5 batches submitted to accumulate stats from 320 documents (350213 virtual)\n",
      "2023-11-20 14:19:57,833 : INFO : 6 batches submitted to accumulate stats from 384 documents (429970 virtual)\n",
      "2023-11-20 14:19:57,976 : INFO : 7 batches submitted to accumulate stats from 448 documents (487391 virtual)\n",
      "2023-11-20 14:19:57,986 : INFO : 8 batches submitted to accumulate stats from 512 documents (537110 virtual)\n",
      "2023-11-20 14:19:58,003 : INFO : 9 batches submitted to accumulate stats from 576 documents (590330 virtual)\n",
      "2023-11-20 14:19:58,023 : INFO : 10 batches submitted to accumulate stats from 640 documents (652040 virtual)\n",
      "2023-11-20 14:19:58,045 : INFO : 11 batches submitted to accumulate stats from 704 documents (718354 virtual)\n",
      "2023-11-20 14:19:58,055 : INFO : 12 batches submitted to accumulate stats from 768 documents (777434 virtual)\n",
      "2023-11-20 14:19:58,079 : INFO : 13 batches submitted to accumulate stats from 832 documents (836928 virtual)\n",
      "2023-11-20 14:19:58,100 : INFO : 14 batches submitted to accumulate stats from 896 documents (889521 virtual)\n",
      "2023-11-20 14:19:58,113 : INFO : 15 batches submitted to accumulate stats from 960 documents (939059 virtual)\n",
      "2023-11-20 14:19:58,136 : INFO : 16 batches submitted to accumulate stats from 1024 documents (993788 virtual)\n",
      "2023-11-20 14:19:58,151 : INFO : 17 batches submitted to accumulate stats from 1088 documents (1049887 virtual)\n",
      "2023-11-20 14:19:58,164 : INFO : 18 batches submitted to accumulate stats from 1152 documents (1105437 virtual)\n",
      "2023-11-20 14:19:58,179 : INFO : 19 batches submitted to accumulate stats from 1216 documents (1156443 virtual)\n",
      "2023-11-20 14:19:58,186 : INFO : 20 batches submitted to accumulate stats from 1280 documents (1211250 virtual)\n",
      "2023-11-20 14:19:58,196 : INFO : 21 batches submitted to accumulate stats from 1344 documents (1261852 virtual)\n",
      "2023-11-20 14:19:58,219 : INFO : 22 batches submitted to accumulate stats from 1408 documents (1315094 virtual)\n",
      "2023-11-20 14:19:59,744 : INFO : 23 batches submitted to accumulate stats from 1472 documents (1363304 virtual)\n",
      "2023-11-20 14:19:59,908 : INFO : 24 batches submitted to accumulate stats from 1536 documents (1416397 virtual)\n",
      "2023-11-20 14:19:59,933 : INFO : 25 batches submitted to accumulate stats from 1600 documents (1469796 virtual)\n",
      "2023-11-20 14:20:00,144 : INFO : 26 batches submitted to accumulate stats from 1664 documents (1523855 virtual)\n",
      "2023-11-20 14:20:00,189 : INFO : 27 batches submitted to accumulate stats from 1728 documents (1575758 virtual)\n",
      "2023-11-20 14:20:00,389 : INFO : 28 batches submitted to accumulate stats from 1792 documents (1624903 virtual)\n",
      "2023-11-20 14:20:00,633 : INFO : 29 batches submitted to accumulate stats from 1856 documents (1672064 virtual)\n",
      "2023-11-20 14:20:00,853 : INFO : 30 batches submitted to accumulate stats from 1920 documents (1725092 virtual)\n",
      "2023-11-20 14:20:00,989 : INFO : 31 batches submitted to accumulate stats from 1984 documents (1776054 virtual)\n",
      "2023-11-20 14:20:01,089 : INFO : 32 batches submitted to accumulate stats from 2048 documents (1831873 virtual)\n",
      "2023-11-20 14:20:01,353 : INFO : 33 batches submitted to accumulate stats from 2112 documents (1885953 virtual)\n",
      "2023-11-20 14:20:01,663 : INFO : 34 batches submitted to accumulate stats from 2176 documents (1936889 virtual)\n",
      "2023-11-20 14:20:02,026 : INFO : 35 batches submitted to accumulate stats from 2240 documents (1988562 virtual)\n",
      "2023-11-20 14:20:02,071 : INFO : 36 batches submitted to accumulate stats from 2304 documents (2040561 virtual)\n",
      "2023-11-20 14:20:02,315 : INFO : 37 batches submitted to accumulate stats from 2368 documents (2098422 virtual)\n",
      "2023-11-20 14:20:03,376 : INFO : 38 batches submitted to accumulate stats from 2432 documents (2152205 virtual)\n",
      "2023-11-20 14:20:03,886 : INFO : 39 batches submitted to accumulate stats from 2496 documents (2192477 virtual)\n",
      "2023-11-20 14:20:03,957 : INFO : 40 batches submitted to accumulate stats from 2560 documents (2235115 virtual)\n",
      "2023-11-20 14:20:05,208 : INFO : 41 batches submitted to accumulate stats from 2624 documents (2286165 virtual)\n",
      "2023-11-20 14:20:05,516 : INFO : 42 batches submitted to accumulate stats from 2688 documents (2329430 virtual)\n",
      "2023-11-20 14:20:07,885 : INFO : 43 batches submitted to accumulate stats from 2752 documents (2371394 virtual)\n",
      "2023-11-20 14:20:08,063 : INFO : 44 batches submitted to accumulate stats from 2816 documents (2415000 virtual)\n",
      "2023-11-20 14:20:08,925 : INFO : 45 batches submitted to accumulate stats from 2880 documents (2455649 virtual)\n",
      "2023-11-20 14:20:09,255 : INFO : 46 batches submitted to accumulate stats from 2944 documents (2490191 virtual)\n",
      "2023-11-20 14:20:09,637 : INFO : 47 batches submitted to accumulate stats from 3008 documents (2524895 virtual)\n",
      "2023-11-20 14:20:12,665 : INFO : 48 batches submitted to accumulate stats from 3072 documents (2563338 virtual)\n",
      "2023-11-20 14:20:13,303 : INFO : 49 batches submitted to accumulate stats from 3136 documents (2598758 virtual)\n",
      "2023-11-20 14:20:13,359 : INFO : 50 batches submitted to accumulate stats from 3200 documents (2627212 virtual)\n",
      "2023-11-20 14:20:13,878 : INFO : 51 batches submitted to accumulate stats from 3264 documents (2650634 virtual)\n",
      "2023-11-20 14:20:15,803 : INFO : 52 batches submitted to accumulate stats from 3328 documents (2676264 virtual)\n",
      "2023-11-20 14:20:16,184 : INFO : 53 batches submitted to accumulate stats from 3392 documents (2697860 virtual)\n",
      "2023-11-20 14:20:16,533 : INFO : 54 batches submitted to accumulate stats from 3456 documents (2726605 virtual)\n",
      "2023-11-20 14:20:18,260 : INFO : 55 batches submitted to accumulate stats from 3520 documents (2763017 virtual)\n",
      "2023-11-20 14:20:18,313 : INFO : 56 batches submitted to accumulate stats from 3584 documents (2802573 virtual)\n",
      "2023-11-20 14:20:18,874 : INFO : 57 batches submitted to accumulate stats from 3648 documents (2841749 virtual)\n",
      "2023-11-20 14:20:20,553 : INFO : 58 batches submitted to accumulate stats from 3712 documents (2881240 virtual)\n",
      "2023-11-20 14:20:20,593 : INFO : 59 batches submitted to accumulate stats from 3776 documents (2924180 virtual)\n",
      "2023-11-20 14:20:20,787 : INFO : 60 batches submitted to accumulate stats from 3840 documents (2955835 virtual)\n",
      "2023-11-20 14:20:22,459 : INFO : 61 batches submitted to accumulate stats from 3904 documents (2981564 virtual)\n",
      "2023-11-20 14:20:22,669 : INFO : 62 batches submitted to accumulate stats from 3968 documents (3025916 virtual)\n",
      "2023-11-20 14:20:22,984 : INFO : 63 batches submitted to accumulate stats from 4032 documents (3057930 virtual)\n",
      "2023-11-20 14:20:31,766 : INFO : 11 accumulators retrieved from output queue\n",
      "2023-11-20 14:20:31,815 : INFO : accumulated word occurrence stats for 2843946 virtual documents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Coherence Score:  0.4145271214519435\n"
     ]
    }
   ],
   "source": [
    "# Coherence Score.\n",
    "coherence_model_lda = CoherenceModel(model=ldamodel, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b95c59",
   "metadata": {},
   "source": [
    "#### LSA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b635aa7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1:\n",
      "woman, story, child, country, brain, city, school, problem, kid, technology\n",
      "\n",
      "\n",
      "Topic 2:\n",
      "woman, girl, child, men, school, family, kid, story, country, mother\n",
      "\n",
      "\n",
      "Topic 3:\n",
      "brain, cell, cancer, woman, patient, body, disease, neuron, blood, drug\n",
      "\n",
      "\n",
      "Topic 4:\n",
      "cell, cancer, health, patient, country, disease, drug, woman, percent, africa\n",
      "\n",
      "\n",
      "Topic 5:\n",
      "woman, water, ocean, earth, planet, men, animal, girl, sea, fish\n",
      "\n",
      "\n",
      "Topic 6:\n",
      "woman, universe, data, galaxy, star, men, planet, earth, information, theory\n",
      "\n",
      "\n",
      "Topic 7:\n",
      "brain, neuron, ocean, child, food, water, fish, language, climate, animal\n",
      "\n",
      "\n",
      "Topic 8:\n",
      "city, brain, woman, building, neuron, design, architecture, men, space, urban\n",
      "\n",
      "\n",
      "Topic 9:\n",
      "robot, woman, water, ocean, fish, machine, animal, girl, ai, computer\n",
      "\n",
      "\n",
      "Topic 10:\n",
      "school, child, kid, brain, universe, student, teacher, education, city, woman\n",
      "\n",
      "\n",
      "Topic 11:\n",
      "cell, language, school, dna, student, music, teacher, africa, education, english\n",
      "\n",
      "\n",
      "Topic 12:\n",
      "robot, war, ai, country, universe, child, africa, refugee, democracy, machine\n",
      "\n",
      "\n",
      "Topic 13:\n",
      "music, sound, robot, musical, song, play, musician, africa, instrument, patient\n",
      "\n",
      "\n",
      "Topic 14:\n",
      "data, language, patient, ocean, fish, health, information, image, computer, sea\n",
      "\n",
      "\n",
      "Topic 15:\n",
      "brain, light, image, country, cell, ocean, government, africa, computer, camera\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# Define the number of topics\n",
    "NUM_TOPICS = 15\n",
    "\n",
    "# Step 1: Apply LSA\n",
    "lsa_model = TruncatedSVD(n_components=NUM_TOPICS, random_state=42)\n",
    "lsa_model.fit(transcript_tfidf_matrix)\n",
    "\n",
    "# Step 2: Extract and display topics\n",
    "feature_names = transcript_vectorizer.get_feature_names_out()\n",
    "for i, topic in enumerate(lsa_model.components_):\n",
    "    top_features_ind = topic.argsort()[:-10 - 1:-1]\n",
    "    top_features = [feature_names[index] for index in top_features_ind]\n",
    "    weights = topic[top_features_ind]\n",
    "\n",
    "    print(f\"Topic {i+1}:\")\n",
    "    print(\", \".join(top_features))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "02efa0a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-20 14:34:32,632 : INFO : adding document #0 to Dictionary<0 unique tokens: []>\n",
      "2023-11-20 14:34:35,255 : INFO : built Dictionary<76564 unique tokens: ['20', '238', '28second', '30second', '31']...> from 4005 documents (total 3416317 corpus positions)\n",
      "2023-11-20 14:34:35,255 : INFO : Dictionary lifecycle event {'msg': \"built Dictionary<76564 unique tokens: ['20', '238', '28second', '30second', '31']...> from 4005 documents (total 3416317 corpus positions)\", 'datetime': '2023-11-20T14:34:35.255151', 'gensim': '4.2.0', 'python': '3.9.13 (main, Aug 25 2022, 23:51:50) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'created'}\n",
      "2023-11-20 14:34:37,007 : INFO : using ParallelWordOccurrenceAccumulator<processes=11, batch_size=64> to estimate probabilities from sliding windows\n",
      "2023-11-20 14:34:49,030 : INFO : 1 batches submitted to accumulate stats from 64 documents (81929 virtual)\n",
      "2023-11-20 14:34:49,088 : INFO : 2 batches submitted to accumulate stats from 128 documents (143059 virtual)\n",
      "2023-11-20 14:34:49,102 : INFO : 3 batches submitted to accumulate stats from 192 documents (208449 virtual)\n",
      "2023-11-20 14:34:49,117 : INFO : 4 batches submitted to accumulate stats from 256 documents (271913 virtual)\n",
      "2023-11-20 14:34:49,180 : INFO : 5 batches submitted to accumulate stats from 320 documents (341980 virtual)\n",
      "2023-11-20 14:34:49,262 : INFO : 6 batches submitted to accumulate stats from 384 documents (419911 virtual)\n",
      "2023-11-20 14:34:49,280 : INFO : 7 batches submitted to accumulate stats from 448 documents (476046 virtual)\n",
      "2023-11-20 14:34:49,320 : INFO : 8 batches submitted to accumulate stats from 512 documents (524489 virtual)\n",
      "2023-11-20 14:34:49,337 : INFO : 9 batches submitted to accumulate stats from 576 documents (576357 virtual)\n",
      "2023-11-20 14:34:49,347 : INFO : 10 batches submitted to accumulate stats from 640 documents (636579 virtual)\n",
      "2023-11-20 14:34:49,357 : INFO : 11 batches submitted to accumulate stats from 704 documents (701150 virtual)\n",
      "2023-11-20 14:34:49,461 : INFO : 12 batches submitted to accumulate stats from 768 documents (758657 virtual)\n",
      "2023-11-20 14:34:49,477 : INFO : 13 batches submitted to accumulate stats from 832 documents (816792 virtual)\n",
      "2023-11-20 14:34:49,497 : INFO : 14 batches submitted to accumulate stats from 896 documents (868049 virtual)\n",
      "2023-11-20 14:34:49,497 : INFO : 15 batches submitted to accumulate stats from 960 documents (916272 virtual)\n",
      "2023-11-20 14:34:49,513 : INFO : 16 batches submitted to accumulate stats from 1024 documents (969640 virtual)\n",
      "2023-11-20 14:34:49,536 : INFO : 17 batches submitted to accumulate stats from 1088 documents (1024384 virtual)\n",
      "2023-11-20 14:34:49,567 : INFO : 18 batches submitted to accumulate stats from 1152 documents (1078511 virtual)\n",
      "2023-11-20 14:34:49,578 : INFO : 19 batches submitted to accumulate stats from 1216 documents (1128230 virtual)\n",
      "2023-11-20 14:34:49,590 : INFO : 20 batches submitted to accumulate stats from 1280 documents (1181625 virtual)\n",
      "2023-11-20 14:34:49,603 : INFO : 21 batches submitted to accumulate stats from 1344 documents (1230958 virtual)\n",
      "2023-11-20 14:34:49,614 : INFO : 22 batches submitted to accumulate stats from 1408 documents (1282869 virtual)\n",
      "2023-11-20 14:34:50,213 : INFO : 23 batches submitted to accumulate stats from 1472 documents (1329810 virtual)\n",
      "2023-11-20 14:34:50,213 : INFO : 24 batches submitted to accumulate stats from 1536 documents (1381660 virtual)\n",
      "2023-11-20 14:34:50,263 : INFO : 25 batches submitted to accumulate stats from 1600 documents (1433693 virtual)\n",
      "2023-11-20 14:34:50,280 : INFO : 26 batches submitted to accumulate stats from 1664 documents (1486333 virtual)\n",
      "2023-11-20 14:34:50,346 : INFO : 27 batches submitted to accumulate stats from 1728 documents (1536892 virtual)\n",
      "2023-11-20 14:34:50,442 : INFO : 28 batches submitted to accumulate stats from 1792 documents (1584744 virtual)\n",
      "2023-11-20 14:34:50,576 : INFO : 29 batches submitted to accumulate stats from 1856 documents (1630733 virtual)\n",
      "2023-11-20 14:34:50,863 : INFO : 30 batches submitted to accumulate stats from 1920 documents (1682527 virtual)\n",
      "2023-11-20 14:34:50,929 : INFO : 31 batches submitted to accumulate stats from 1984 documents (1732238 virtual)\n",
      "2023-11-20 14:34:50,997 : INFO : 32 batches submitted to accumulate stats from 2048 documents (1786621 virtual)\n",
      "2023-11-20 14:34:51,030 : INFO : 33 batches submitted to accumulate stats from 2112 documents (1839033 virtual)\n",
      "2023-11-20 14:34:51,079 : INFO : 34 batches submitted to accumulate stats from 2176 documents (1888540 virtual)\n",
      "2023-11-20 14:34:51,129 : INFO : 35 batches submitted to accumulate stats from 2240 documents (1938883 virtual)\n",
      "2023-11-20 14:34:51,362 : INFO : 36 batches submitted to accumulate stats from 2304 documents (1989658 virtual)\n",
      "2023-11-20 14:34:51,587 : INFO : 37 batches submitted to accumulate stats from 2368 documents (2046170 virtual)\n",
      "2023-11-20 14:34:51,676 : INFO : 38 batches submitted to accumulate stats from 2432 documents (2098509 virtual)\n",
      "2023-11-20 14:34:51,699 : INFO : 39 batches submitted to accumulate stats from 2496 documents (2137741 virtual)\n",
      "2023-11-20 14:34:51,718 : INFO : 40 batches submitted to accumulate stats from 2560 documents (2179344 virtual)\n",
      "2023-11-20 14:34:51,844 : INFO : 41 batches submitted to accumulate stats from 2624 documents (2229274 virtual)\n",
      "2023-11-20 14:34:52,084 : INFO : 42 batches submitted to accumulate stats from 2688 documents (2271487 virtual)\n",
      "2023-11-20 14:34:52,177 : INFO : 43 batches submitted to accumulate stats from 2752 documents (2312196 virtual)\n",
      "2023-11-20 14:34:52,236 : INFO : 44 batches submitted to accumulate stats from 2816 documents (2354482 virtual)\n",
      "2023-11-20 14:34:52,609 : INFO : 45 batches submitted to accumulate stats from 2880 documents (2394143 virtual)\n",
      "2023-11-20 14:34:52,898 : INFO : 46 batches submitted to accumulate stats from 2944 documents (2427660 virtual)\n",
      "2023-11-20 14:34:53,525 : INFO : 47 batches submitted to accumulate stats from 3008 documents (2461356 virtual)\n",
      "2023-11-20 14:34:54,017 : INFO : 48 batches submitted to accumulate stats from 3072 documents (2498754 virtual)\n",
      "2023-11-20 14:34:54,424 : INFO : 49 batches submitted to accumulate stats from 3136 documents (2533155 virtual)\n",
      "2023-11-20 14:34:54,762 : INFO : 50 batches submitted to accumulate stats from 3200 documents (2560853 virtual)\n",
      "2023-11-20 14:34:55,014 : INFO : 51 batches submitted to accumulate stats from 3264 documents (2583589 virtual)\n",
      "2023-11-20 14:34:55,947 : INFO : 52 batches submitted to accumulate stats from 3328 documents (2608525 virtual)\n",
      "2023-11-20 14:34:55,947 : INFO : 53 batches submitted to accumulate stats from 3392 documents (2629483 virtual)\n",
      "2023-11-20 14:34:56,059 : INFO : 54 batches submitted to accumulate stats from 3456 documents (2657414 virtual)\n",
      "2023-11-20 14:34:57,086 : INFO : 55 batches submitted to accumulate stats from 3520 documents (2692879 virtual)\n",
      "2023-11-20 14:34:57,096 : INFO : 56 batches submitted to accumulate stats from 3584 documents (2731400 virtual)\n",
      "2023-11-20 14:34:57,556 : INFO : 57 batches submitted to accumulate stats from 3648 documents (2769579 virtual)\n",
      "2023-11-20 14:34:58,131 : INFO : 58 batches submitted to accumulate stats from 3712 documents (2807932 virtual)\n",
      "2023-11-20 14:34:58,257 : INFO : 59 batches submitted to accumulate stats from 3776 documents (2849689 virtual)\n",
      "2023-11-20 14:34:58,607 : INFO : 60 batches submitted to accumulate stats from 3840 documents (2880361 virtual)\n",
      "2023-11-20 14:34:59,113 : INFO : 61 batches submitted to accumulate stats from 3904 documents (2905428 virtual)\n",
      "2023-11-20 14:34:59,206 : INFO : 62 batches submitted to accumulate stats from 3968 documents (2948508 virtual)\n",
      "2023-11-20 14:34:59,637 : INFO : 63 batches submitted to accumulate stats from 4032 documents (2979772 virtual)\n",
      "2023-11-20 14:35:05,374 : INFO : 11 accumulators retrieved from output queue\n",
      "2023-11-20 14:35:05,386 : INFO : accumulated word occurrence stats for 2771199 virtual documents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence Score: 0.4046219173348973\n"
     ]
    }
   ],
   "source": [
    "# Prepare the list of top words for each topic from LSA.\n",
    "top_n = 10  # Number of top words to consider for each topic.\n",
    "topics = [[feature_names[i] for i in topic.argsort()[:-top_n - 1:-1]] for topic in lsa_model.components_]\n",
    "\n",
    "# Prepare the Gensim dictionary and corpus.\n",
    "dictionary = Dictionary(texts_with_trigrams)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts_with_trigrams]\n",
    "\n",
    "# Create the Coherence Model.\n",
    "coherence_model = CoherenceModel(topics=topics, texts=texts_with_trigrams, dictionary=dictionary, coherence='c_v')\n",
    "\n",
    "# Compute the Coherence Score.\n",
    "coherence_score = coherence_model.get_coherence()\n",
    "print(f\"Coherence Score: {coherence_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b828e5de",
   "metadata": {},
   "source": [
    "#### Non-Negative Matrix Factorization (NMF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0dc4a76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 15  \n",
    "\n",
    "# Create an NMF model.\n",
    "nmf_model = NMF(\n",
    "    n_components=num_topics,        # Number of topics.\n",
    "    init='nndsvd',                 # Initialization method .\n",
    "    random_state=42,               # Random seed for reproducibility.\n",
    "    alpha=0.1,                   # Regularization parameters.\n",
    "    l1_ratio=0.5,                  # Sparsity parameter. \n",
    "    max_iter=200,                  # Maximum number of iterations.\n",
    "    verbose=1                      # Verbosity level (1 for progress updates).\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5fad2b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashly\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1422: FutureWarning: `alpha` was deprecated in version 1.0 and will be removed in 1.2. Use `alpha_W` and `alpha_H` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "violation: 1.0\n",
      "violation: 0.3169809045245732\n",
      "violation: 0.22661446577718866\n",
      "violation: 0.16499193648219967\n",
      "violation: 0.11732147148271226\n",
      "violation: 0.09059244295447151\n",
      "violation: 0.06972224470310531\n",
      "violation: 0.05530169672659834\n",
      "violation: 0.0466509717633364\n",
      "violation: 0.0388682695211769\n",
      "violation: 0.03222279074174161\n",
      "violation: 0.02706648860005983\n",
      "violation: 0.023221417683279927\n",
      "violation: 0.020301419964456852\n",
      "violation: 0.017879498761617568\n",
      "violation: 0.015990983131277607\n",
      "violation: 0.014411395882686166\n",
      "violation: 0.013035246821910004\n",
      "violation: 0.011761933200442453\n",
      "violation: 0.010657887009088438\n",
      "violation: 0.009742087633558575\n",
      "violation: 0.009005365611653333\n",
      "violation: 0.008403996067707711\n",
      "violation: 0.00788386901112701\n",
      "violation: 0.007374620032212143\n",
      "violation: 0.006855574624390159\n",
      "violation: 0.006310407768147964\n",
      "violation: 0.00568176354771571\n",
      "violation: 0.005043680007553677\n",
      "violation: 0.00445555244477551\n",
      "violation: 0.0039258783454808235\n",
      "violation: 0.003453206658418631\n",
      "violation: 0.0030458052706761507\n",
      "violation: 0.0027041807239283388\n",
      "violation: 0.0024119145972380397\n",
      "violation: 0.0021609516121918176\n",
      "violation: 0.0019461648221403508\n",
      "violation: 0.0017586273085507332\n",
      "violation: 0.001590676910051701\n",
      "violation: 0.0014447528227641592\n",
      "violation: 0.001316229393874315\n",
      "violation: 0.0012033455204453647\n",
      "violation: 0.0011038922948548027\n",
      "violation: 0.001013958758125313\n",
      "violation: 0.0009330933153757278\n",
      "violation: 0.0008595906778077708\n",
      "violation: 0.00079349346785239\n",
      "violation: 0.0007338113160565584\n",
      "violation: 0.0006789687298326605\n",
      "violation: 0.0006298040555171847\n",
      "violation: 0.0005856100342897924\n",
      "violation: 0.0005452895477207389\n",
      "violation: 0.0005092316910650562\n",
      "violation: 0.0004767906987646882\n",
      "violation: 0.0004475107237262647\n",
      "violation: 0.0004208427408146712\n",
      "violation: 0.00039643286319769567\n",
      "violation: 0.00037398379677880595\n",
      "violation: 0.00035339753961799214\n",
      "violation: 0.00033470569418327475\n",
      "violation: 0.0003176302524523699\n",
      "violation: 0.00030172836202009307\n",
      "violation: 0.00028685986652315833\n",
      "violation: 0.0002729976984259479\n",
      "violation: 0.0002600330072531237\n",
      "violation: 0.0002479321183551925\n",
      "violation: 0.00023666018207826127\n",
      "violation: 0.0002261942851385457\n",
      "violation: 0.0002163645404372676\n",
      "violation: 0.00020715052792616906\n",
      "violation: 0.0001985798901111725\n",
      "violation: 0.0001905679499866742\n",
      "violation: 0.0001828713051347396\n",
      "violation: 0.00017555894248023267\n",
      "violation: 0.00016858456955568375\n",
      "violation: 0.00016193317309196455\n",
      "violation: 0.00015556656547506843\n",
      "violation: 0.00014948823053045318\n",
      "violation: 0.0001436947498258889\n",
      "violation: 0.00013818616635458345\n",
      "violation: 0.00013289081004440762\n",
      "violation: 0.00012777048652993006\n",
      "violation: 0.00012284012459670845\n",
      "violation: 0.00011810657850485971\n",
      "violation: 0.00011353389714981643\n",
      "violation: 0.00010911733097001613\n",
      "violation: 0.00010487118393149718\n",
      "violation: 0.00010077423525313962\n",
      "violation: 9.682130715258383e-05\n",
      "Converged at iteration 90\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NMF(alpha=0.1, init='nndsvd', l1_ratio=0.5, n_components=15, random_state=42,\n",
       "    verbose=1)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model to the 'transcript' TF-IDF matrix.\n",
    "nmf_model.fit(transcript_tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4eba9f76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #1: story, love, feel, ive, thought, talk, started, went, question, book\n",
      "Topic #2: water, plant, energy, climate, food, earth, carbon, planet, forest, tree\n",
      "Topic #3: patient, cancer, health, doctor, disease, drug, care, treatment, medical, hospital\n",
      "Topic #4: country, africa, government, percent, dollar, economy, china, money, company, business\n",
      "Topic #5: ocean, fish, sea, coral, reef, animal, shark, marine, fishing, specie\n",
      "Topic #6: woman, men, girl, sex, gender, female, male, sexual, mother, boy\n",
      "Topic #7: brain, neuron, memory, sleep, signal, activity, disorder, body, cortex, behavior\n",
      "Topic #8: city, building, space, design, architecture, urban, street, park, community, neighborhood\n",
      "Topic #9: school, kid, child, student, teacher, education, girl, parent, classroom, learning\n",
      "Topic #10: universe, galaxy, star, planet, light, earth, space, telescope, particle, sun\n",
      "Topic #11: cell, cancer, dna, body, gene, tissue, stem, protein, blood, immune\n",
      "Topic #12: robot, ai, robotics, machine, robotic, leg, build, autonomous, task, intelligence\n",
      "Topic #13: music, sound, musical, musician, song, play, instrument, orchestra, composer, hear\n",
      "Topic #14: data, computer, technology, information, machine, digital, algorithm, device, internet, ai\n",
      "Topic #15: language, english, word, speaker, speak, arabic, sentence, learning, dictionary, sound\n"
     ]
    }
   ],
   "source": [
    "# Get the topics and the topic-word distributions.\n",
    "nmftopics = nmf_model.components_\n",
    "\n",
    "n_top_words = 10  \n",
    "feature_names = transcript_vectorizer.get_feature_names_out()\n",
    "for topic_idx, topic in enumerate(nmftopics):\n",
    "    top_words_idx = topic.argsort()[:-n_top_words - 1:-1]\n",
    "    top_words = [feature_names[i] for i in top_words_idx]\n",
    "    print(f\"Topic #{topic_idx + 1}: {', '.join(top_words)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1a5ed273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "violation: 1.0\n",
      "violation: 0.152577051651251\n",
      "violation: 0.008614368778717537\n",
      "violation: 0.0009801830371258743\n",
      "violation: 0.00011631146678459456\n",
      "violation: 1.031063061798322e-05\n",
      "Converged at iteration 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashly\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1422: FutureWarning: `alpha` was deprecated in version 1.0 and will be removed in 1.2. Use `alpha_W` and `alpha_H` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Get document-topic matrix.\n",
    "document_topic_matrix = nmf_model.transform(transcript_tfidf_matrix)\n",
    "\n",
    "# Assign each document to the most probable topic.\n",
    "dominant_topic_for_each_doc = document_topic_matrix.argmax(axis=1) + 1  # +1 to start topics from 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c75f92db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-20 14:36:28,917 : INFO : adding document #0 to Dictionary<0 unique tokens: []>\n",
      "2023-11-20 14:36:31,395 : INFO : built Dictionary<73015 unique tokens: ['20', '238', '28second', '30second', '31']...> from 4005 documents (total 3494475 corpus positions)\n",
      "2023-11-20 14:36:31,410 : INFO : Dictionary lifecycle event {'msg': \"built Dictionary<73015 unique tokens: ['20', '238', '28second', '30second', '31']...> from 4005 documents (total 3494475 corpus positions)\", 'datetime': '2023-11-20T14:36:31.410912', 'gensim': '4.2.0', 'python': '3.9.13 (main, Aug 25 2022, 23:51:50) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'created'}\n",
      "2023-11-20 14:36:31,795 : INFO : using ParallelWordOccurrenceAccumulator<processes=11, batch_size=64> to estimate probabilities from sliding windows\n",
      "2023-11-20 14:36:43,082 : INFO : 1 batches submitted to accumulate stats from 64 documents (83903 virtual)\n",
      "2023-11-20 14:36:43,093 : INFO : 2 batches submitted to accumulate stats from 128 documents (146504 virtual)\n",
      "2023-11-20 14:36:43,191 : INFO : 3 batches submitted to accumulate stats from 192 documents (213358 virtual)\n",
      "2023-11-20 14:36:43,287 : INFO : 4 batches submitted to accumulate stats from 256 documents (278533 virtual)\n",
      "2023-11-20 14:36:43,362 : INFO : 5 batches submitted to accumulate stats from 320 documents (350213 virtual)\n",
      "2023-11-20 14:36:43,614 : INFO : 6 batches submitted to accumulate stats from 384 documents (429970 virtual)\n",
      "2023-11-20 14:36:43,640 : INFO : 7 batches submitted to accumulate stats from 448 documents (487391 virtual)\n",
      "2023-11-20 14:36:43,656 : INFO : 8 batches submitted to accumulate stats from 512 documents (537110 virtual)\n",
      "2023-11-20 14:36:43,676 : INFO : 9 batches submitted to accumulate stats from 576 documents (590330 virtual)\n",
      "2023-11-20 14:36:43,688 : INFO : 10 batches submitted to accumulate stats from 640 documents (652040 virtual)\n",
      "2023-11-20 14:36:43,726 : INFO : 11 batches submitted to accumulate stats from 704 documents (718354 virtual)\n",
      "2023-11-20 14:36:43,740 : INFO : 12 batches submitted to accumulate stats from 768 documents (777434 virtual)\n",
      "2023-11-20 14:36:43,756 : INFO : 13 batches submitted to accumulate stats from 832 documents (836928 virtual)\n",
      "2023-11-20 14:36:43,769 : INFO : 14 batches submitted to accumulate stats from 896 documents (889521 virtual)\n",
      "2023-11-20 14:36:43,789 : INFO : 15 batches submitted to accumulate stats from 960 documents (939059 virtual)\n",
      "2023-11-20 14:36:43,803 : INFO : 16 batches submitted to accumulate stats from 1024 documents (993788 virtual)\n",
      "2023-11-20 14:36:43,823 : INFO : 17 batches submitted to accumulate stats from 1088 documents (1049887 virtual)\n",
      "2023-11-20 14:36:43,838 : INFO : 18 batches submitted to accumulate stats from 1152 documents (1105437 virtual)\n",
      "2023-11-20 14:36:43,841 : INFO : 19 batches submitted to accumulate stats from 1216 documents (1156443 virtual)\n",
      "2023-11-20 14:36:43,858 : INFO : 20 batches submitted to accumulate stats from 1280 documents (1211250 virtual)\n",
      "2023-11-20 14:36:43,877 : INFO : 21 batches submitted to accumulate stats from 1344 documents (1261852 virtual)\n",
      "2023-11-20 14:36:43,892 : INFO : 22 batches submitted to accumulate stats from 1408 documents (1315094 virtual)\n",
      "2023-11-20 14:36:44,409 : INFO : 23 batches submitted to accumulate stats from 1472 documents (1363304 virtual)\n",
      "2023-11-20 14:36:44,425 : INFO : 24 batches submitted to accumulate stats from 1536 documents (1416397 virtual)\n",
      "2023-11-20 14:36:44,691 : INFO : 25 batches submitted to accumulate stats from 1600 documents (1469796 virtual)\n",
      "2023-11-20 14:36:44,707 : INFO : 26 batches submitted to accumulate stats from 1664 documents (1523855 virtual)\n",
      "2023-11-20 14:36:44,879 : INFO : 27 batches submitted to accumulate stats from 1728 documents (1575758 virtual)\n",
      "2023-11-20 14:36:44,969 : INFO : 28 batches submitted to accumulate stats from 1792 documents (1624903 virtual)\n",
      "2023-11-20 14:36:45,302 : INFO : 29 batches submitted to accumulate stats from 1856 documents (1672064 virtual)\n",
      "2023-11-20 14:36:45,401 : INFO : 30 batches submitted to accumulate stats from 1920 documents (1725092 virtual)\n",
      "2023-11-20 14:36:45,416 : INFO : 31 batches submitted to accumulate stats from 1984 documents (1776054 virtual)\n",
      "2023-11-20 14:36:45,484 : INFO : 32 batches submitted to accumulate stats from 2048 documents (1831873 virtual)\n",
      "2023-11-20 14:36:45,510 : INFO : 33 batches submitted to accumulate stats from 2112 documents (1885953 virtual)\n",
      "2023-11-20 14:36:45,710 : INFO : 34 batches submitted to accumulate stats from 2176 documents (1936889 virtual)\n",
      "2023-11-20 14:36:45,779 : INFO : 35 batches submitted to accumulate stats from 2240 documents (1988562 virtual)\n",
      "2023-11-20 14:36:45,914 : INFO : 36 batches submitted to accumulate stats from 2304 documents (2040561 virtual)\n",
      "2023-11-20 14:36:46,174 : INFO : 37 batches submitted to accumulate stats from 2368 documents (2098422 virtual)\n",
      "2023-11-20 14:36:46,208 : INFO : 38 batches submitted to accumulate stats from 2432 documents (2152205 virtual)\n",
      "2023-11-20 14:36:46,332 : INFO : 39 batches submitted to accumulate stats from 2496 documents (2192477 virtual)\n",
      "2023-11-20 14:36:46,511 : INFO : 40 batches submitted to accumulate stats from 2560 documents (2235115 virtual)\n",
      "2023-11-20 14:36:46,836 : INFO : 41 batches submitted to accumulate stats from 2624 documents (2286165 virtual)\n",
      "2023-11-20 14:36:46,867 : INFO : 42 batches submitted to accumulate stats from 2688 documents (2329430 virtual)\n",
      "2023-11-20 14:36:46,981 : INFO : 43 batches submitted to accumulate stats from 2752 documents (2371394 virtual)\n",
      "2023-11-20 14:36:47,038 : INFO : 44 batches submitted to accumulate stats from 2816 documents (2415000 virtual)\n",
      "2023-11-20 14:36:47,871 : INFO : 45 batches submitted to accumulate stats from 2880 documents (2455649 virtual)\n",
      "2023-11-20 14:36:47,932 : INFO : 46 batches submitted to accumulate stats from 2944 documents (2490191 virtual)\n",
      "2023-11-20 14:36:48,771 : INFO : 47 batches submitted to accumulate stats from 3008 documents (2524895 virtual)\n",
      "2023-11-20 14:36:49,864 : INFO : 48 batches submitted to accumulate stats from 3072 documents (2563338 virtual)\n",
      "2023-11-20 14:36:50,341 : INFO : 49 batches submitted to accumulate stats from 3136 documents (2598758 virtual)\n",
      "2023-11-20 14:36:50,777 : INFO : 50 batches submitted to accumulate stats from 3200 documents (2627212 virtual)\n",
      "2023-11-20 14:36:51,793 : INFO : 51 batches submitted to accumulate stats from 3264 documents (2650634 virtual)\n",
      "2023-11-20 14:36:52,633 : INFO : 52 batches submitted to accumulate stats from 3328 documents (2676264 virtual)\n",
      "2023-11-20 14:36:52,646 : INFO : 53 batches submitted to accumulate stats from 3392 documents (2697860 virtual)\n",
      "2023-11-20 14:36:53,331 : INFO : 54 batches submitted to accumulate stats from 3456 documents (2726605 virtual)\n",
      "2023-11-20 14:36:53,930 : INFO : 55 batches submitted to accumulate stats from 3520 documents (2763017 virtual)\n",
      "2023-11-20 14:36:54,420 : INFO : 56 batches submitted to accumulate stats from 3584 documents (2802573 virtual)\n",
      "2023-11-20 14:36:54,792 : INFO : 57 batches submitted to accumulate stats from 3648 documents (2841749 virtual)\n",
      "2023-11-20 14:36:55,127 : INFO : 58 batches submitted to accumulate stats from 3712 documents (2881240 virtual)\n",
      "2023-11-20 14:36:55,572 : INFO : 59 batches submitted to accumulate stats from 3776 documents (2924180 virtual)\n",
      "2023-11-20 14:36:56,006 : INFO : 60 batches submitted to accumulate stats from 3840 documents (2955835 virtual)\n",
      "2023-11-20 14:36:56,344 : INFO : 61 batches submitted to accumulate stats from 3904 documents (2981564 virtual)\n",
      "2023-11-20 14:36:56,724 : INFO : 62 batches submitted to accumulate stats from 3968 documents (3025916 virtual)\n",
      "2023-11-20 14:36:57,452 : INFO : 63 batches submitted to accumulate stats from 4032 documents (3057930 virtual)\n",
      "2023-11-20 14:37:04,404 : INFO : 11 accumulators retrieved from output queue\n",
      "2023-11-20 14:37:04,430 : INFO : accumulated word occurrence stats for 2843946 virtual documents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence Score: 0.6848976765913517\n"
     ]
    }
   ],
   "source": [
    "# Create a Gensim Dictionary.\n",
    "transcript_dict = Dictionary(tt_df_sub['transcript'].apply(str.split))\n",
    "\n",
    "# Convert the NMF topics into a format suitable for coherence calculation.\n",
    "nmf_topics = [[feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]] for topic in nmftopics]\n",
    "\n",
    "# Calculate the coherence score using the 'c_v' measure.\n",
    "coherence_model = CoherenceModel(topics=nmf_topics, texts=tt_df_sub['transcript'].apply(str.split), dictionary=transcript_dict, coherence='c_v')\n",
    "coherence_score = coherence_model.get_coherence()\n",
    "\n",
    "# Print the coherence score.\n",
    "print(f\"Coherence Score: {coherence_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba196929",
   "metadata": {},
   "source": [
    "#### CTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b63c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data for CTM.\n",
    "tp = TopicModelDataPreparation(\"bert-base-nli-mean-tokens\")\n",
    "\n",
    "training_dataset = tp.fit(text_for_contextual=tt_df['transcript'].tolist(), \n",
    "                          text_for_bow=tt_df_sub['transcript'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6516acbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the size of the vocabulary and contextual embeddings.\n",
    "bow_size = len(tp.vocab)  \n",
    "contextual_size = 768  \n",
    "\n",
    "num_topics = 10  \n",
    "num_epochs = 10  \n",
    "\n",
    "# Initialize the CombinedTM model.\n",
    "ctm = CombinedTM(bow_size=bow_size, contextual_size=contextual_size, n_components=num_topics, num_epochs=num_epochs)\n",
    "\n",
    "# Train the model.\n",
    "ctm.fit(training_dataset)\n",
    "\n",
    "# Extract and display the topics.\n",
    "topics = ctm.get_topic_lists(10)\n",
    "for topic_idx, topic in enumerate(topics):\n",
    "    print(f\"Topic {topic_idx + 1}: {', '.join(topic)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7ba53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the CTM topics to a format suitable for coherence calculation.\n",
    "ctm_topics = ctm.get_topic_lists(10)\n",
    "\n",
    "# Create a dictionary and corpus for the topics.\n",
    "dictionary = corpora.Dictionary(ctm_topics)\n",
    "corpus = [dictionary.doc2bow(topic) for topic in ctm_topics]\n",
    "\n",
    "# Calculate the coherence score using UMass coherence (you can also use C_v coherence).\n",
    "coherence_model = CoherenceModel(model=None, corpus=corpus, dictionary=dictionary, coherence='u_mass', topics=ctm_topics)\n",
    "coherence_score = coherence_model.get_coherence()\n",
    "\n",
    "print(f\"Coherence Score: {coherence_score}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
